{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"cfds_logo.png\">\n",
    "\n",
    "###  Lab 08 - \"Supervised Deep Learning - Long Short-Term Memory Neural Networks\"\n",
    "\n",
    "Chartered Financial Data Scientist (CFDS), Autumn Term 2019/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lab you learned about how to utilize an **unsupervised** (deep) learning technique namely **Convolutional Neural Networks (CNNs)** to classify tiny images of objects contained in the CIFAR-10 dataset.\n",
    "\n",
    "In this seventh lab, we will learn how to apply another type of deep learning technique referred to as **Long-Short-Term-Memory (LSTM)** neural networks. Unlike standard feedforward neural networks, LSTMs encompass feedback connections that make it a \"general-purpose computer\". LSTMs are designed to process not only a single data point (such as images), but also entire sequences of data, e.g., such as speech, video, or financial time series.\n",
    "\n",
    "\n",
    "We will again use the functionality of the **'PyTorch'** library to implement and train an LSTM based neural network. The network will be trained on the historic daily (in-sample) returns of an exemplary financial stock. Once the network is trained, we will use the learned model to predict future (out-of-sample) returns. Finally, we will convert the predictions into tradable signals and the backtest the signals accordingly. \n",
    "\n",
    "The figure below illustrates a high-level eye view on the machine learning process we aim to establish in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px\" src=\"process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, pls. don't hesitate to ask all your questions either during the lab or send us an email via marco.schreyer@fds.ai or damian.borth@fds.ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After today's lab, you should be able to:\n",
    "\n",
    "> 1. Understand the basic concepts, intuitions and major building blocks of **Long-Short Term Memory (LSTM) Neural Networks**.\n",
    "> 2. Know how to **implement and to train an LSTM** to learn a model of financial time-series data.\n",
    "> 3. Understand how to apply such a learned model to **predict future data points of a time-series**.\n",
    "> 4. Know how to **interpret the model's prediction results** and backtest the predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the Jupyter Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. We will mostly use the PyTorch, Numpy, Sklearn, Matplotlib, Seaborn, BT and a few utility libraries throughout the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python data science libraries\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import financial data science libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl as ql # library to retreive financial data\n",
    "import bt as bt # library to backtest trading signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python machine / deep learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the sklearn pre-processing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn libraries\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python plotting libraries and set general plotting parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.rcParams['figure.dpi']= 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable notebook matplotlib inline plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress potential warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create notebook folder structure to store the data as well as the trained neural network models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data'): os.makedirs('./data')  # create data directory\n",
    "if not os.path.exists('./models'): os.makedirs('./models')  # create trained models directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed value to obtain reproducable results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13b95d710>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the lab notebook we will download and access historic daily stock market data ranging from **01/01/2000** to **31/12/2017** of the **\"International Business Machines\" (IBM)** corporation (ticker symbol: \"IBM\"). To start the data download, let's initialize the \"quandl\" financial data download API and set an API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ql.ApiConfig.api_key = '<enter you own quandl api code here>'\n",
    "ql.ApiConfig.api_key = 'xn6g-K_ebmMgSJRTCSUk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's specify the start and end date of the stock market data download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.datetime(2000, 1, 1)\n",
    "end_date = dt.datetime(2017, 12, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the daily \"International Business Machines\" (IBM) stock market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = ql.get('WIKI/IBM', start_date=start_date, end_date=end_date, collapse='daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect top 10 records of the retreived IBM stock market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ex-Dividend</th>\n",
       "      <th>Split Ratio</th>\n",
       "      <th>Adj. Open</th>\n",
       "      <th>Adj. High</th>\n",
       "      <th>Adj. Low</th>\n",
       "      <th>Adj. Close</th>\n",
       "      <th>Adj. Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>112.44</td>\n",
       "      <td>116.00</td>\n",
       "      <td>111.87</td>\n",
       "      <td>116.00</td>\n",
       "      <td>10347700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.542051</td>\n",
       "      <td>85.155442</td>\n",
       "      <td>82.123614</td>\n",
       "      <td>85.155442</td>\n",
       "      <td>10347700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>114.00</td>\n",
       "      <td>114.50</td>\n",
       "      <td>110.87</td>\n",
       "      <td>112.06</td>\n",
       "      <td>8227800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.687245</td>\n",
       "      <td>84.054294</td>\n",
       "      <td>81.389516</td>\n",
       "      <td>82.263093</td>\n",
       "      <td>8227800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>112.94</td>\n",
       "      <td>119.75</td>\n",
       "      <td>112.12</td>\n",
       "      <td>116.00</td>\n",
       "      <td>12733200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.909100</td>\n",
       "      <td>87.908312</td>\n",
       "      <td>82.307139</td>\n",
       "      <td>85.155442</td>\n",
       "      <td>12733200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>118.00</td>\n",
       "      <td>118.94</td>\n",
       "      <td>113.50</td>\n",
       "      <td>114.00</td>\n",
       "      <td>7971900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.623639</td>\n",
       "      <td>87.313692</td>\n",
       "      <td>83.320195</td>\n",
       "      <td>83.687245</td>\n",
       "      <td>7971900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>117.25</td>\n",
       "      <td>117.94</td>\n",
       "      <td>110.62</td>\n",
       "      <td>113.50</td>\n",
       "      <td>11856700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.073065</td>\n",
       "      <td>86.579593</td>\n",
       "      <td>81.205991</td>\n",
       "      <td>83.320195</td>\n",
       "      <td>11856700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-10</th>\n",
       "      <td>117.25</td>\n",
       "      <td>119.37</td>\n",
       "      <td>115.37</td>\n",
       "      <td>118.00</td>\n",
       "      <td>8540500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.073065</td>\n",
       "      <td>87.629354</td>\n",
       "      <td>84.692960</td>\n",
       "      <td>86.623639</td>\n",
       "      <td>8540500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-11</th>\n",
       "      <td>117.87</td>\n",
       "      <td>121.12</td>\n",
       "      <td>116.62</td>\n",
       "      <td>119.00</td>\n",
       "      <td>7873300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.528206</td>\n",
       "      <td>88.914027</td>\n",
       "      <td>85.610583</td>\n",
       "      <td>87.357738</td>\n",
       "      <td>7873300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-12</th>\n",
       "      <td>119.62</td>\n",
       "      <td>122.00</td>\n",
       "      <td>118.25</td>\n",
       "      <td>119.50</td>\n",
       "      <td>6803800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.812879</td>\n",
       "      <td>89.560034</td>\n",
       "      <td>86.807164</td>\n",
       "      <td>87.724787</td>\n",
       "      <td>6803800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-13</th>\n",
       "      <td>119.94</td>\n",
       "      <td>121.00</td>\n",
       "      <td>115.75</td>\n",
       "      <td>118.25</td>\n",
       "      <td>8489700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.047790</td>\n",
       "      <td>88.825935</td>\n",
       "      <td>84.971917</td>\n",
       "      <td>86.807164</td>\n",
       "      <td>8489700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>120.94</td>\n",
       "      <td>123.31</td>\n",
       "      <td>117.50</td>\n",
       "      <td>119.62</td>\n",
       "      <td>10956600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.781889</td>\n",
       "      <td>90.521703</td>\n",
       "      <td>86.256590</td>\n",
       "      <td>87.812879</td>\n",
       "      <td>10956600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open    High     Low   Close      Volume  Ex-Dividend  \\\n",
       "Date                                                                  \n",
       "2000-01-03  112.44  116.00  111.87  116.00  10347700.0          0.0   \n",
       "2000-01-04  114.00  114.50  110.87  112.06   8227800.0          0.0   \n",
       "2000-01-05  112.94  119.75  112.12  116.00  12733200.0          0.0   \n",
       "2000-01-06  118.00  118.94  113.50  114.00   7971900.0          0.0   \n",
       "2000-01-07  117.25  117.94  110.62  113.50  11856700.0          0.0   \n",
       "2000-01-10  117.25  119.37  115.37  118.00   8540500.0          0.0   \n",
       "2000-01-11  117.87  121.12  116.62  119.00   7873300.0          0.0   \n",
       "2000-01-12  119.62  122.00  118.25  119.50   6803800.0          0.0   \n",
       "2000-01-13  119.94  121.00  115.75  118.25   8489700.0          0.0   \n",
       "2000-01-14  120.94  123.31  117.50  119.62  10956600.0          0.0   \n",
       "\n",
       "            Split Ratio  Adj. Open  Adj. High   Adj. Low  Adj. Close  \\\n",
       "Date                                                                   \n",
       "2000-01-03          1.0  82.542051  85.155442  82.123614   85.155442   \n",
       "2000-01-04          1.0  83.687245  84.054294  81.389516   82.263093   \n",
       "2000-01-05          1.0  82.909100  87.908312  82.307139   85.155442   \n",
       "2000-01-06          1.0  86.623639  87.313692  83.320195   83.687245   \n",
       "2000-01-07          1.0  86.073065  86.579593  81.205991   83.320195   \n",
       "2000-01-10          1.0  86.073065  87.629354  84.692960   86.623639   \n",
       "2000-01-11          1.0  86.528206  88.914027  85.610583   87.357738   \n",
       "2000-01-12          1.0  87.812879  89.560034  86.807164   87.724787   \n",
       "2000-01-13          1.0  88.047790  88.825935  84.971917   86.807164   \n",
       "2000-01-14          1.0  88.781889  90.521703  86.256590   87.812879   \n",
       "\n",
       "            Adj. Volume  \n",
       "Date                     \n",
       "2000-01-03   10347700.0  \n",
       "2000-01-04    8227800.0  \n",
       "2000-01-05   12733200.0  \n",
       "2000-01-06    7971900.0  \n",
       "2000-01-07   11856700.0  \n",
       "2000-01-10    8540500.0  \n",
       "2000-01-11    7873300.0  \n",
       "2000-01-12    6803800.0  \n",
       "2000-01-13    8489700.0  \n",
       "2000-01-14   10956600.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also evaluate the data quality of the download by creating a set of summary statistics of the retrieved data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ex-Dividend</th>\n",
       "      <th>Split Ratio</th>\n",
       "      <th>Adj. Open</th>\n",
       "      <th>Adj. High</th>\n",
       "      <th>Adj. Low</th>\n",
       "      <th>Adj. Close</th>\n",
       "      <th>Adj. Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4.527000e+03</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.0</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4.527000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>128.222709</td>\n",
       "      <td>129.396728</td>\n",
       "      <td>127.159447</td>\n",
       "      <td>128.298614</td>\n",
       "      <td>6.278760e+06</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>107.207865</td>\n",
       "      <td>108.166503</td>\n",
       "      <td>106.338592</td>\n",
       "      <td>107.269724</td>\n",
       "      <td>6.278760e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>40.122937</td>\n",
       "      <td>40.198267</td>\n",
       "      <td>40.057692</td>\n",
       "      <td>40.137144</td>\n",
       "      <td>3.314386e+06</td>\n",
       "      <td>0.094086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.589963</td>\n",
       "      <td>40.694568</td>\n",
       "      <td>40.490652</td>\n",
       "      <td>40.598655</td>\n",
       "      <td>3.314386e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>54.650000</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>54.010000</td>\n",
       "      <td>55.070000</td>\n",
       "      <td>1.027500e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.714758</td>\n",
       "      <td>42.242027</td>\n",
       "      <td>40.237952</td>\n",
       "      <td>41.027662</td>\n",
       "      <td>1.027500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>92.270000</td>\n",
       "      <td>93.180000</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>92.385000</td>\n",
       "      <td>4.023725e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.101463</td>\n",
       "      <td>70.880441</td>\n",
       "      <td>69.417812</td>\n",
       "      <td>70.119535</td>\n",
       "      <td>4.023725e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>119.310000</td>\n",
       "      <td>120.550000</td>\n",
       "      <td>117.850000</td>\n",
       "      <td>119.370000</td>\n",
       "      <td>5.502900e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.441676</td>\n",
       "      <td>93.664367</td>\n",
       "      <td>91.575772</td>\n",
       "      <td>92.473450</td>\n",
       "      <td>5.502900e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>161.935000</td>\n",
       "      <td>162.985000</td>\n",
       "      <td>160.845000</td>\n",
       "      <td>161.950000</td>\n",
       "      <td>7.644000e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>147.825960</td>\n",
       "      <td>148.985000</td>\n",
       "      <td>146.896456</td>\n",
       "      <td>147.799857</td>\n",
       "      <td>7.644000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>215.380000</td>\n",
       "      <td>215.900000</td>\n",
       "      <td>214.300000</td>\n",
       "      <td>215.800000</td>\n",
       "      <td>4.120730e+07</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>186.042617</td>\n",
       "      <td>186.491787</td>\n",
       "      <td>185.109726</td>\n",
       "      <td>186.405408</td>\n",
       "      <td>4.120730e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open         High          Low        Close        Volume  \\\n",
       "count  4527.000000  4527.000000  4527.000000  4527.000000  4.527000e+03   \n",
       "mean    128.222709   129.396728   127.159447   128.298614  6.278760e+06   \n",
       "std      40.122937    40.198267    40.057692    40.137144  3.314386e+06   \n",
       "min      54.650000    56.700000    54.010000    55.070000  1.027500e+06   \n",
       "25%      92.270000    93.180000    91.500000    92.385000  4.023725e+06   \n",
       "50%     119.310000   120.550000   117.850000   119.370000  5.502900e+06   \n",
       "75%     161.935000   162.985000   160.845000   161.950000  7.644000e+06   \n",
       "max     215.380000   215.900000   214.300000   215.800000  4.120730e+07   \n",
       "\n",
       "       Ex-Dividend  Split Ratio    Adj. Open    Adj. High     Adj. Low  \\\n",
       "count  4527.000000       4527.0  4527.000000  4527.000000  4527.000000   \n",
       "mean      0.009600          1.0   107.207865   108.166503   106.338592   \n",
       "std       0.094086          0.0    40.589963    40.694568    40.490652   \n",
       "min       0.000000          1.0    40.714758    42.242027    40.237952   \n",
       "25%       0.000000          1.0    70.101463    70.880441    69.417812   \n",
       "50%       0.000000          1.0    92.441676    93.664367    91.575772   \n",
       "75%       0.000000          1.0   147.825960   148.985000   146.896456   \n",
       "max       1.500000          1.0   186.042617   186.491787   185.109726   \n",
       "\n",
       "        Adj. Close   Adj. Volume  \n",
       "count  4527.000000  4.527000e+03  \n",
       "mean    107.269724  6.278760e+06  \n",
       "std      40.598655  3.314386e+06  \n",
       "min      41.027662  1.027500e+06  \n",
       "25%      70.119535  4.023725e+06  \n",
       "50%      92.473450  5.502900e+06  \n",
       "75%     147.799857  7.644000e+06  \n",
       "max     186.405408  4.120730e+07  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect the daily adjusted closing prices of the \"International Business Machines\" (IBM) stock market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'International Business Machines (IBM) - Daily Adjusted Historical Stock Closing Prices')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFVCAYAAAC9w02PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd5wb9Z3/8Zek7cX22l573bu/trExNmA6mB5CEtIbuUu51N8lIaSRu5AjvZAcB+EuDdILJJC7hHQIhGYbjDHGuH2xDe5tvd7ed6XfHzMjjdqutLvSrtfv5+PBw9JoNPPVSLPMZz7f7+cbiEQiiIiIiIiIyMkvONwNEBERERERkaGhAE9ERERERGSUUIAnIiIiIiIySijAExERERERGSUU4ImIiIiIiIwSCvBERERERERGiYLhboCInHyMMbOBe6215/axzvuBH1tru3Ow//cDPwZOA15jrf3iEGxzNik+kzGmC1gLBIAK4L+stb/Ictu3A7dZa/cNtp397Gc18A/gbdbae33LNwMbrbXvymJb7wIWWWs/k7D8XuCfrbVdQ9Fm33YvAlZaa+8wxhyx1tYYYz4PvB04hPP/qybg7dbaBmPMHmCHtfYVvm18HPhPa23AGHMNMNVa+8MM978a+A2wDee7LgRut9b+po/3fAZ4BFhCimPVx/s+DdwIzLHWdqR4/YNADfA94D+stf8vk+363p/xuefty1r7ed+y1cAHrbVv9S37OrAD2EQf55wx5mKgwVq7OcO2/q+19vWZrOuuP5vU5+k1wCdxvrsy4E5r7S+NMeOBV1hrf5XpPtztHbHW1vTx+gzgP4FJQCnwLPAxYGqq9vWzrxoG8D0nbONRnM/d6i7qAd5prT2UsF5Ozl8RGVmUwRORXPl3IJTLbVtrNw1FcNePE9ba1dbaS4DLgP80xgSy2YC19mO5Du58dgD+C/NlQPlQbdxa+9YcBHcB4PPAd1O8fJt7/C/ECS7e63ttmjFmou/5K4F6t51/Ad5ojBmTRVMe8X3XVwE3GWPOSLeytfbr1tr1WWzf8w7gXnzfU5rtHxngRX/Ozr0Mzrn34AQ5mW4v4+CuH98H3mCtvQy4EviSMWYScDrwmiHaBwDGmBDwe5ybCauttecA3cCA/hYN4ntO9M/W2kuttZcC/4sT8Cbua8jPXxEZeZTBE5FBce8cbwKWAmOANwFX4GQg7gVea4z5GnARzkXnbdba+9z3HQPGA/cAV+PcgZ4HfMNa+xNjzCXALTg3oypwsjkXedt2M2MftNa+1RhzPc4d9E5gJ/B+4Hqci/5MtpvJRc8YoN5aG3GzS0estd8zxiwCvmetXW2M+QpwKc7f199aa7/hftYP4lzQz8G56z8LuNFa+ze3PV8BeoHdwAfc9X6Mcyc+6LaxA/i1+7zE/eybEtr4vPO1mLHW2kacYOKXwEz3+/ow8HqcoO848Dr3e/mx26Yi4MPuts41xjwIVAPftdb+wM2cLcLJLnUCs4EpwLustRuNMW8CPu5+liettZ8xxlyAk+3oBtqAN1prm31tvhLYlsGFZxVOAOu5D+f39l1jzGL32C31vf5n4F3At/vZbhJrbYsx5vs4QeILOAHEDPezPmCtvdkY8xOc3zgQzZwtsNZ+yg0CNgFn+7N0bnZsN87x+wXwE3f5hcAdOAFqD/CUP1vlHXdrbYcvm/YnEn4PwJn0f+4l7SvT4+LP7hljfgzMx8lg3YGT/XwFsNIYs83db6pz8j1um28Bfulma88BbneXH3TXW0Xm52kDcIMx5n63HYuttZ3GmF8Cy93v5kHgRzjnZgT4qLX2eWPMvwAfco/RA9baW3yf96vAWODD1tqIu/hCYL+19mnf/m9y2znJ994rgS/jnLd17ucuJPk7ayD2PW8GHsMJTCPAdTiZ6/8BzgKO4PxteLW1dk+aYwHO39UW9/v6hnvcfgB8Cef8nQHcjXO+t+H8bSpx1ykF2nG+r1qczPZYnL+jn7XWPtjHfkVkBFAGT0SGwnpr7RXAQzjdA3+IcyHyVrfr1Bw3A3Mp8FljzDj3ffe47+sFxlprX4Vzt93r6nYa8A5r7WqcO9Jv8m/b27kxZgLwBeAydz8NOEESmW63j8823hjzqDHmcWAzzsVZX64nFog2pHi901p7DXADcKObvboLeL2bOTqIE5RcCazHCZZvwbnAWoVzoXgN8K+kz8z9Fni9u+1VOF1MMcYEgQnAFW7WoQA4G+cic4+19jyc43qOu51unMD7dTgX6on2WmuvBu4E3u92h/sCcLn7PUxzL3Jfi3OReAlOlq4qYTurcY5tKh93j/9m4NU4XSI99wBvdh9fjxPI+m12tz1QR4GJOBfDT7mfdRXO8UrlHpygKoQT6PwjRRfM9wJ3W2st0OkGNuAcl7e558PLGbYv6feQ4bmXyb4uc4/7o+4Nirf7XzTGVAIX49wseAXQa619Fvgr8GmcroLpzsl6a+2F1tqHfZv8PvAe93f5J2Ax2Z2nV+EEIPcAh4F/c3//X8HJzP4A+BZwh7X2Ypzz74dulu8zOOfrSqDYGFPhfsZvAQXW2n/1BXfgZChf8u/cWtthrW3zHZ8ATrDkndePATfT/zk8Bufvove34Bqcv10TrLWrgH/B+T2m8jP3+3oEmA58011eYq29yFr7c9+63wK+5p7zdwAr3GXfdo/3t4Cv49wYm4hz7r0NJQZETgo6UUVkKDzn/rsfJ3vgtww4071IBOcO9mz3sfWt52Wi9uPcSQbnAufbxpgWYBqwJs3+5wJbfVmhx3Eu+J4e5HbB7aIJ4Hb3W2uMeShhHX+XzetxLoxqgL+k2J7/WJXgZMemAL8xxoBz9/whnDv/N+FcMDfidLv7C7AAp3tYt7tOKr/CuYh/CXjCW2itDbtjCu9xP/t0nO/DeG211u4EbnfH4G10s5VHcC6e+/osF+Bkc6qBP7ufpRLnAvGrwGeBh3GO/dMJ25lI+izSbdba7wEYY96Dk/G6wrffgDse6gLgcwnvPYwT0EYZY+5221lrre0rYAAno3kAOAGcbYy5FCebUpxqZWttszHmMZyg+N0kdNkzxlThZJQnGWM+gpsZwjkek621L7qrrnHbmI73e+vv95Du3MtkX4/Y5DF4iZ/1YzhBzBicbKRfX+ekJVmNtXa7u+0fuvucQQbnqXtcZ1lrb8LpVjsN5ybHs4A/U7zYbQfW2k3u9ucCW6y17e46n3G3ORkni7YrxS73Am9IaMME4HzgBXfRRKDJWnvQ9/m/ihP89ncOJ/6NmA2sc9tda4zZkeI94HTRjHvNPQ9THW/j2+YD7rq3A/9ujLkJ5zfWba3d6may78H5/WSdDReR/FMGT0SGQiTFsjDO35gdOJmM1Thj2H6D00XNW6evbdwFvNs6xUEOEbuw9bbteRlYYozx7oZfAngXsNlstz/NOJmIIpxuV1Pc5SsBjDHFOFmGt+FkTN5ljJmVsI3E9hzHCSKuc4/RV3CyVNcBT1hrL8fpingTTjbqsLX2KpwLw6+maqS19iWczMBH8V14G2NOB15rrX0L8BGcYxgAtuNk8jDGzDXGeAUpUh27vj7LyzgXpVe6n+VOnMDtHcBP3LFBW3G6fvkdA8bRv/04x97vXpzun+sSsizgZAqP+RdYa9/rjpvqM7hzg/n34Rz7d+EUDrne3VdZH+Mw78LJ0k2yyYVG3gH80Fp7lXWKw5wDXGWMqQYOut1Mwf0uEnQAU9z9euMCV5P699DfudffvvpljJkCnGmtfR1wLXCrMabAt+++zslw4vaAQ8aYBe62bzLGvI7Mz9Ni4NduUAZOYH8Ep2uo/2/FdpxMHe7YyiM4x2ORe+5ijLnfDRCP4gTqpxljooV8XE8Bc4wxq9z3eGNIL/KtcxwY4x4n/+dfTf/ncOLveAtwnruvKmBhmuOQTqrj7T/nr3dvOOwAbnJ/Lx8A7jPOGN5Ka+21wDtxzmkRGeEU4IlIrjyBMwbqDzhjQZ7AuaMeSRh/1ZdfAE8YY9bgZIO84g3etgMA1trjON0Y/2GMeQrn7nmqgh39bTcVr4vmP3C6Oj6LU6ny18Ar3ezISrcdnTjZnqfcdR4E+iyuYq0N43QX+5MxZi3w/3Au6DYAX3S7W30Q58LqeeC97j6/CXytj03/Gpjhy9SAk41odT/3QzgXwlNxusfNdbNPPwNu66vNfXyWWve9jxljnsbpXvYiTlfTu40xD+MEGj9LeOujxLqFJvK6aP4dp8vfJxJevw+nC2iqyqbn4GQNM+V1S3wY53d7i9uV8mHgFW433e/ijCdL+Ztxx2XNJ7m7KDiB389967bhZJreh3NB/TN334k3BQBuxfnN/xm3kAzpfw/9nXv97SsTR4Aa9zf7EPAta20PTobu6ziZ3GzOyQ8AP3J/gyvc9md0nlprj+DczPijMWYdzvm30R0rthtY5mYbPwl8xPc9/ov7m/0Gzm92nfu+g+52IzhdIv/bzdB5+wvj3Mj5vNveZ3D+Ft3sWyeC873+r9v+K3DGv2VzDnv+BBx3j/UPccbMDbY68adwurE+Sqx78yeBW3x/Bzbj/NZXu8fsPuA/BrlfEcmDQCTS3w1aERGR3HHHBj4CXGWHsMKfMeavwJuttU1Dtc0M9hnE6Up49WD362a0fuiOGZNTlHGKOJ1hrb3XDTS34nRJ7RzmponICKUMnoiIDCs3I/IFnOzlkDDGXItTxTSfwd0cYCNORcTBBnfTccZS/m4o2iYntf3A29xM6F9xulEquBORtJTBExERERERGSWGvIqmMaYQZ56Z2TgDn7+MMwblTpxS6J04lZ6OGmO8cQc9wJettX8c6vaIiIiIiIicKnLRRfMdQJ219iKcuXH+G2eOlY/45rK5yRhTgzMo+gKcSlVf86pYiYiIiIiISPZyMQ/efcD97uMATnburdbaw759duBM9rnG7UfeaYzZhTPnzDM5aJOIiIiIiMioN+QBnrW2BcAYU4kT6N3sBXfGmPNxJnW9GCdr1+h7azPOpK99ikQikUAg0ymrRERERERERp20AVEuMngYY2YA/wd8x1r7K3fZW4DPAtdaa2uNMU0489p4KnEmEO5TIBCgtjbTKbRkKFRXV+qY55mOef7pmOefjnn+6Zjnn455/umY55+Oef5VV1emfS0XRVYm40zu+2Fr7cPusnfgFFNZba094a66HviKMaYEpxjLYpzJfUVERERERGQAcpHB+3egCvicMeZzQAhYCuwF/tcYA/CYtfYWY8y3gSdwir181lrbkYP2iIiIiIiInBJyMQbvBuCGDNe9C7hrqNsgIiIiIiJyKsrFNAkiIiIiIiIyDBTgiYiIiIiIjBIK8EREREREREYJBXgiIiIiIiKjhAI8ERERERGRUUIBnoiIiIiIyCihAE9ERERERGSUUIAnIiIiIiIySijAExERERERGSUU4ImIiIiIiIwSCvBERERERERGCQV4IiIiIiIio4QCPBERERERkVFCAZ6IiIiIiMgooQBPRERERERklFCAJyIiIiIiMkoowBMRERERERklFOCJiIiIiIiMEgrwRERERERERgkFeCIiIiIiIqOEAjwREREREZFRQgGeiIiIiIjIKKEAT0REREREZJRQgCciIiIiIjJKFORio8aYQuBHwGygGPgysA34CRABtgD/aq0NG2NuAa4FeoCPWWvX56JNIiIiIiIio12uMnjvAOqstRcBrwD+G7gNuNldFgCuM8asBC4BzgHeCvxPjtojIiIiIiIy6uUqwLsP+Jz7OICTnTsTeMxd9hfgCuBC4EFrbcRauw8oMMZU56hNIiIiIiIio1pOumhaa1sAjDGVwP3AzcC3rLURd5VmYCwwBqjzvdVbXtvX9qurK4e6ydIPHfP80zHPPx3z/NMxzz8d8/zTMc8/HfP80zEfOXIS4AEYY2YA/wd8x1r7K2PMrb6XK4EGoMl9nLi8T7W1zUPZVOlHdXWljnme6Zjnn455/umY55+Oef7pmOefjnn+6ZjnX18BdU66aBpjJgMPAjdZa3/kLn7OGLPafXwN8ASwBrjaGBM0xswEgtba47lok4iIiIiIyGiXqwzevwNVwOeMMd5YvBuAbxtjioDtwP3W2l5jzBPAOpxg819z1B4REREREZFRL1dj8G7ACegSXZJi3c8Dn89FO0RERERERE4lmuhcRERERERklFCAJyIiIiIiMkoowBMRERERERklFOCJiIiIiIiMEgrwRERERERERgkFeCIiIiIiIqOEAjwREREREZFRQgGeiIiIiIjIKKEAT0REREREZJRQgCciIiIiIjJKKMATEREREREZJRTgiYiIiIiIjBIK8EREREREREYJBXgiIiIiIiKjhAI8ERERERGRUUIBnoiIiIhkrfFgK91tPcPdDBFJUDDcDRARERGRk0t7fSeP3vocAK++7QKCocAwt0hEPMrgiYiIiEhWuttjmbsXfrub+r3Nw9gaEfFTgCciIiIi2fEl7PasOcLjtz1Pd4e6a4qMBArwRERERCQr4d5I0rJj2+uHoSUikkgBnoiIiIhkJdyTHOBt+Inl+K7GYWiNiPgpwBMRERGRrIR7wimXr7nzhTy3REQSKcATERERkaz0dPQOdxNEJA0FeCIiIiKSlU337sxova7WbtZ+Zwv1+1RlUyRfcjYPnjHmHOAb1trVxpgzgO8BPcCLwHuttWFjzPuAD7jLv2yt/WOu2iMiIiJyquto6qKovIBgaHD3+DubuwE4+z2LOPx8HQeerU1aZ/c/DrLldy8DUGsbuO6OCwe1TxHJTE4CPGPMp4F/AlrdRbcAX7TW/tkY80vgWmPMM8BHgbOAEuBJY8xD1trOXLRJRERE5FS16de72Lv2CADj51Ry0ceWD3hb3vi7wtIQU5dPpKisIGWA5wV3IpJfueqiuRt4ve/5c8B4Y0wAqAS6gVXAGmttp7W2EdgFnJ6j9oiIiIicksK94WhwB3Di5cF1l/QmOa824wCYuGAc533oNN/+IkQiyVU2Uy0TkaGXkwyetfa3xpjZvkU7gf8BbgYagUeBN7qPPc3A2Ey2X11dOSTtlMzpmOefjnn+6Zjnn455/umY599wH/PWEx1JywbTpsYep4NWZVVpdDvV1ZUceOoY+5+rpWpsGaHC5BxCWaiQigmlA95vNob7mJ+KdMxHjpyNwUtwB3CRtXarMeZfgf8E/oaTzfNUAg2ZbKy2VgN186m6ulLHPM90zPNPxzz/dMzzT8c8/0bCMW/Y35K0bDBtqj/ovLc3EInbTm/E6bp57HAjoaJQ0vt+d/Narv7iqgHvN1Mj4ZifanTM86+vgDpfVTRPAE3u40NAFbAeuMgYU2KMGQssBrbkqT0iIiIiI0YkHGHrAy9zZMuJId92Z1PXkG6vq9UpsFJYFp8nCAQCAEQisO/po0nv62js4ui2of98IhIvXwHee4F7jTGPAf8P+Hdr7RHg28ATwCPAZ621yX0IREREREa5I1tPsOvhgzx917Yh22YkEmHTvTt56gfx2yyfWDKo7XY0OgFjydii+Bec+I7mI2288NuXAJi2cmLcKk99f+g+n4iklrMumtbaPcC57uMngQtSrHMXcFeu2iAiIiJyMnjxwf1xz1tq29n6u5c57bo5VEwa2Li1xgOt7F2XnEkLFWV3f7+1roOj204w69waQoVBOtyMYMmYhADPjfD2rosVdEleR2Rki0Qi9HT20nSojc7mLqYun9j/m0aYfI3BExEREZEUulq7adjnjJMLBAP89bNP09nidIMsm1DCstfPHdB221IUVwEIpiiAks6+9Ud57pfOpOZ1u5o4+92L0mbw3B6aHNx4PLYsGMimySLD7qXHDrHl/2JTfLz6tvMHPW9kvp1crRUREREZZdpOxKYAjoQj0eAOYhOKD0RPZ2/c80XXzAQgVJD55d9Ljx2KPj60yQncYgFecfzKKWK58upSFl49I+P9iQy3bQ/siXve3jC0Y1jzQQGeiIiIyDA5tr2ex761Ke3riYVMstHTER/gTVpSBQFwi11mpOa08UnLvEIwhaXxlTK9Iiue8oklzDp3cjSwFDkZlE2IH6P61Pe2DlNLBk4BnoiIiMgwqXupqc/Xe7t6+3y9L14Gz+tKWT6xlEAwkNWE44mrttS2Rx8nBnSJGbzLP3smgWCAQCBAZU1ZbJvhkTXh+eb7d/Onm9bR25NF5CujVmKA13KsPc2aI5cCPBEREZFh0tXWE308/7JpSa+3N3QmLcuUl8E7652GV992AUVlBU6Al0WAlbjuw19+NuP3+sffXfzx5dHqneERFEiFeyO8/MRhejp6aTnalvHxbjvRwZN3bqbpcGuOWyj5FO6N0HighaLyAoIFzu+3+CQsFKQAT0RERGSY9HTEArzJKbpD1u9tGXDGy8vgFZQUEAw5F6uBQHYZtGyyfUkZPZ+C4hCVU5wsXm/PyMngvfTYwejjR2/dxIO3PBOd568vD31hA3W7muKKcXj2P3OMf3xjY0bbkZGl+Ugrnc3d1CydwLW3nk/FpNIhn0cyHxTgiYiIiAwTLwi7+kurUk5f0NvZG52WIBP71h9l0707iYQj0QxeQUlsrJzTRTPz9qUbr1daVZy8sJ+CmV5xFy+Dl03wmAvt9Z20Hk+uNOovctMfbxxi6/F2Dm6sBWDjL16k6VAbe9Ye6eutMsK01nVECx6VVhURDAWi3TPr9zYPZ9OypgBPREREZJh4AV5ReSzLlujEy32P0/N77pc72bvuKMd3NUYDlaLSWKGWbLtoetHg2e9exKTFVbHFKbbhT+CNmVKW9HrQC/C6w/R2h/nDJ9by/H27CffmP9Cr293Ig59/hj1rsg/C/AF3UXkhAI9+cxMbfmo5sqUu+tqh5+uS3isjU3t9J3//4gbW370dSL6x8fhtzw9DqwZOAZ6IiIjIMOnp7CVYECAYCjJmSjlVsyuT1tnwE5vRtvxBV9OhVhr3t1BaVRxXiTPrLpruuuUTSzjvg6fFlqcKynwR3sxzJye97M2/19sTpu1EB5HeCHuePMyGn+7IuD1DpW53+qC5YV9L0hQTfv7qpN3tvXHLnr5re/S1xv0tg22m5EliIZVj2+sBmLd66nA0Z9AU4ImIiIgMgwMba2nY20Khm2ELBAOsePuC6OuzzosFSf11Z4xEIjxw45ro85aj7XS2dDNuRkXcell30YzE3gdQs9QZJ5iqi6Y//1hYmjy9g1e0ItwTAV8bDj9fl/fKml5bUtn4ixdZ898vpH197Xe3RB93t/fEVRaVk1MgISKafWENADPOnhRd1njw5AnYFeCJiIiIDINnf+pk5iYtinV99Fee9AdntbYBgIMba/nTZ9ZFJxv39HbF9ynzgo7EQCwQDMBAqmi67Trr3YuYf/l0Vr13cfLKvpjJP+7P443B2/aHPUkB6xN3bE5a/9iO+pwVKvEf51Qa9qW/mG/3TUzf3dbDrkcOpl1XTg6J3YS9c7LS19V49z8OjbgpPtJRgCciIiIyjLzpAwCKKwujj0PFyUHShp9aetp7efDz6+OWH9l6Iu758Z2NQGzcmycQcMbgtdd30tPHHHuNB1ro7uiJZfDceChUEOS018ymdFyqIiuxoKmwJDmDV+B+nmPb65PGNNXviS9icWJPE+u+u5Un70yfSRuMxOMyUD1dvdHPVbMsuQrq03dto7u9J2m5jCyJAV7I7U4cDAVZ+Y6FgFMd9YEb15wUmTwFeCIiIiJ51tsdy7j5q2f6A6NIOMKS18wGYOvv48vxF1UUxj1vOujMx3b6m+bFLU/sihgIQndHLw9+/hke/cZzSe06vquRYzvqefSbm1j7nS3RsXZ9TYEQ3XY/GTz/fGKJGUeIHxvYuN/5PM2H2/rd70CkK2iTrc6mLjoanYzestfP5aIbl3PhDadHXz+y5QQPfeGZIdmX5E7i3Iz+cauJ1WH7yu6OFArwRERERPIoEo7wx0+ujT73io94LvnUGUw9YwJTl0+MBk1Nh9oI94ajmbPxc8bEvefwC07FxsTqlYmBTCAYoNudXD1xioDuth7W3PkC6767FYCGvS00H20jEISSsRlM9uzbVaoxeMUJQamnYnIpEH+Rvfn+3f3vbxACvuOy6JUzM35f06H4ic27Wns4uPE4AKXjihk/u5IJc8fEFZnxCrHIyOUvGlRUXhB3Q2Paiuq433ZX28jPyCaffS5jzML+3mytfXFomyMiIiIyurX5xnABlCWMkxs3vYKz3+2Mcetsjo1B+/uXn42OXetq6ebAs7VMWzGR4zsbaTnqjLkrSOgaGQwl3MtPGHsWCUei49FSTcdQv6eZcTMqot0Q++LfcqoMnn+aBU+1GUeoKEjL0XZ6eyKEMogjh0LQdxySjlEabXUdrP2fLWlf94/rW/zKWex76ujAGyh55e+iufDq+IA/GApw3e0XcuLlJp64fTNdWcyTOFzSBnjA08BzpJ+2cjmQ3NlYRERERNLq7Y5ldArLCph8WvrLKX+RB39xj7rdTdTtbqKzuYvtf9obXR4sCDB2RkW0RH9SF82E7b/85GHKxpcwceFYnvrBtpRtGDu9vN/P5Gw8tvWi8uRLzFBhkMlLqji6rT667NwPLGHjz518gT+DV1ReQFerkynxB6GZaK5t46XHD9HR2MWSV8/uc93KmjLmXjKVYzvqOb6zkZKxRXQ0diV1gQXY+sDLcZOgn3bdbLb+fk/K7YaK1UnuZBLudX57p103m3mXpJ4awRsr23iwNeXrI0lfAd791tr3pXvRGHNXDtojIiIiMqqFe5xswdgZFVzy8eV9jm+bcfYknvvVzrhpBfy2/F/82LxAIMAZb5nPY9/aBMCEeWPjXk+c7+ulxw7Rerwj2k0yFW8y7/74P0a6rFhvwlinYCgY7aIadscl9nT1RoM7cILQuRdnNh9ZT1cvv/7Uk9Hn81ZPpbgyOS3Y444BnLd6KqHCIBd8eBk9nb2EioL8+TNP0dXSTVdbD0W+sVj+cZMX3bicthOxLq7XfvO8uO2nKjIjI5d3TpakKh7kKq4sYsyUMmptAzv+so9F12TetTff0t5e8II7Y0zIGPMvxpgvGmNWG2Mm+l8XERERkcx52YLqhWP7zUwFAgFmpZg0PJ2C4lBc9ixxHrxEhe66XpClEPMAACAASURBVBfPVIaq4iTAmCmxbKA35583fYIX/PknEgd44bcvZbz9cHd8AHkioTrn4RfqWPe9rTz/610AcV1PC4pDBAKB6P43/HQHx2wDR91Jr71AvKi8gPGzK6OVFgEKipK7pGYztk+Gl9dFs7/iO96NEPvXfUmVN0eSTM7Y7wOzgCuBSuBnOW2RiIiIyEmq+WgbDQf6rrLnZQsyDZxOe+2cjNY7423zKRlbRNn4Epa8ZjYXfnRZ0jqJQUdiMJWofGIJcy+ektH+e93P5a8Kmmjxq2Zx6WdWcN0dFzLzHCdw9bqRPvszZ17AxIqG2WhviJ8fMLEK5/q7t3Nse6yLaGVNfFEagNLxThandkcD676zhae+t5W2Ex3Rdl31hVVp3+s379Jp0cf9TVQvw8v7bvsL8LxzFxjR019k8pdlnrX2P4B2a+0fgLH9vUFERETkVBOJRHjkqxt57JubUl7Qh3vDHNp0nDXu3G6ZFC4Bp7vfxIX9X375s3ULLp+e1D0TYMrpE+KedyQERP4xf+e+fwlXfO6sjLtoRtzMZGKhF7+ColBcFg8g7MZzjQecsU3edAmTFo0DYNrKiRntH+DRW+OnfvCypelUTErumnrWO03SsvaGrlhg7gYB/X1/BUUhJi9xCsv4i+XIyNOwz8n09vdbn3ZmdfTxYG5E5FomAV6B1y3TGFMJjNxPIyIiIjJM/NUxU2XGnv2Z5Zkf74g+r+mjuEqiVF0AE4UK+18ncTs9nfHtXPG2BdHHFf1kqBJ53U2zvfD1B1ndHT3RQMqrxOnPmvQlVTAX8S3qbO5Kej1VFrVkTIoxex09hHvDBIKxz1kypoilr5vDRR87PWl9j9e986XHDvXbfsm/3u4wHY1dNOxvpai8gKrZlX2uP23FxGiG2j9v40iTSYB3M7AGOAt4CvhiTlskIiIichLyF+Fob4ifCiESjnBoU130+eQlVf128fPzCpGUjivm4o8vT7lOqLD/yzpvAud064Z8WanE6Rv6482V153lPGFzL4p1Ae1o7IoGal6GrK2+kwdufJLf3/BknxfVbXWdScv867/40IGM2lM2voQV1y+IW7btgT10d/QmBYTzVk9LmpPQb/5lTjfNdHMAyvD64yfX8rf/WE/biQ4KSkJ9FjwCZxzm1DOcjPJI7nXb718Ca+1jOOPv5gHvstY+lPNWiYiIiJxken3ZsMSxYGu/E5s/be7FU1j+lvlZbdsLyMLhMFWzKlN2Iyws679yY2FpAZfffCZXf2kVE+YlByZFZQVc9fmzuezfV2Y1NQFkOBl6CoFggAVXTAec+f16u50r58IyJyhq3N8SzcTV73W60rUe7+DBzz/DgY210e1s/GXy9MxegPfyk4ezyqJVzYrP5DQdbqP5cFu/Y7TSbcc+uD+r90nuNB5sZesDe+KLpEQynw8xGgSO4Aiv309ijPke8GZrbS3wDmPMHblvloiIiMjJpdvXLbO9PpZNikQiHN/ZGH2+7A3zKO2jHHsqXoAXcS9Kp62sjgtCysYXZzymr6K6lMLSgrj3L7xqBuPnjKG4opDSqmIqJ2fXPROgZukECopDLHvD3KzfW1zpBHNPfvsFWo46hVEKS1N8Hvfaet/6o7TXd7Lpnp3Rl1qPdyStHu6N0FrXweb7die9ds77l6RtT/nEUmqWjadmWXw32oFWFM02qym58/htm9j18AH2P3MsbnniFCLpBNyfQGQED1rLZJKOldbaDwJYa28wxjyeyYaNMecA37DWrjbGTALuAqqAEPDP1trdxpj3AR8AeoAvW2v/OKBPISIiIjLMOptiWTv/Bb1/DJm/smI2vGxal2+7JeOKwJ3j/Oz3LM56m14Go6A4xOJrZw2oXX7FFYVce+t5/a+YQtmEkujjLb9z5vZLVazFC3Rf/Nt+d51YEDhmahnHX2zk8o+t4OHbnWIrkXAkZeAHfVf7DIYCnPPeJexbf5QjL5yILs+2WEq2WVDJPe989N8cyIaXwRvJlVEzug1hjJng/juODIJCY8yngbsB72y9FfiltfZinDF9i4wxNcBHgQuAq4GvGWOyu50lIiIikmdPfT82j5pf3UtN0ce93bFsnr+c+kAnR642bkXJM2IVJf0FUwIDSCx5hWD8QdJwmTA31l3UC47Lxhdz+hvnxa0XDAVprYsFbGVVscCwq7mbgpIQc1bVsOq9TsDbfKSN3q7Yd+Efv1hU1v+4uFQFV7LRz5AuybMhKYziBu25KLIS7o2w/kfbObDhWP8r9yGTPwdfBDYYYzYCz5JZkZXdwOt9zy8Aphtj/g5cDzwKrALWWGs7rbWNwC4gfRkiERERkRHg6LZ69qw9En1eaxs4tqOeEy/7AryuWP+tpkNO+f+FV83IuBtloinLJnDBR5Zx+ptiAY8/AzWQTJFXQTM0wDYNpaLyQs5+z6K4ZeUTSpKqGkaANl+AFyF2kd3e2BUdB1jqFog5vrMxGsie8db5VM2qZPWnV3D6m+Yxdlr8dA2pTFpUxcp3LKR4TKH7fFxWn2vCfM0uNpLU+rpKJ0o1rjWV6BC8HHTRbDnaxuHn63j258njSbPRbzbOWvtHY8xfgInAMWttv+Gqtfa3xpjZvkWzgXpr7RXGmP8AbgJeBPxHuZkM59irru67hKkMPR3z/NMxzz8d8/zTMc8/HfOh4x3L39/wJABjfOPWigoKoq+Hm50AY/qiCYM6/onvrRwXm15gwsQKxlVXJL6lT3PPnMyhTcdZctmMEfG7qL6ikmd+FJtGYtaiagqKQzzGpuiyop4AbQ2xjGgwEqC6upLujh6623qYNM8JwOacNonH3HVKCpzL3arqCqqrK53PekZN5u16ZSXmnKlsuG8n57xjESUV2WX1SioLKRlTPCKOcS6dDJ+vfX9snN3iK2ey/aF90ednXJ3Z2NGycuf7rxpXxoQh/syBlljUOJjjmTbAM8b8t7X2w8aYdRC7PWKMwVp7fpb7qQMecB//AfgKsAHwt7wSaMhkY7W1zVnuXgajurpSxzzPdMzzT8c8/3TM80/HfPD83bKOHW3i6Lb66PNuX1fAbQ/uZc4VU5kyfRyN9c5FZXtX95Ae/7a22Ji/hsY2uguz6zI2fuk4LvnkGYydXj7ifhcXf2I59U1tScv/9s1n4553tHZRW9tM8xFn3VC5k42sb2wjVBSkYlIp9cedDGp7Z9egPueSN8yhub2T5vbk6Rj6FAzQ3dmT82PcfKSNtd/dwsIrZjDHN/VEPpwMf1vaTnSw+S8vAVBZU8bCV81kwbUz2PbAHmqWTci4/R0dzjjMEydaCZcPbR/c2sOxXgD9taevALCvDN6X3H/fDWRWVia9J4FXAj8HLga2AuuBrxhjSoBiYDGwJe0WRERERIaZfxLvNf/zAnW7/N0yewkVBaPdM7f+/mUq37IIvKBwiAtueF0/YWBdNAPBAONmZJf1y7VLPnUG7fWdVM3sP3tRUBKira6T3q7eaEVE/5i3UGGQcE8kNtawNJPagkMvEArEl+QfYuGeMAeerWX3owfpaOhi8/27qTbj4iaQ9+x7+iiBUIAZZ03KWXtGqke+tjF6bnrFjgKBAKddNyer7QRyOAbPX5wpEo4MuEhP2l+6tfao+/Bua+2FA9p6zCeAu40xH8Lplvl2a229MebbwBM4YwE/a61NXeZIREREZARoPRHL3viDO4Cu1h5KxhbR2+Vk1vasOcKeNUeYdd5kYGCFUPpS5Jv3LpDl/Gwj1bjpFYybnlnQWTWzktoXG3j8tuepnOJ0jy3yTSgeLAjS290bLXJTOEzFZIKhAD2duaupv+sfB9n+x71xy9pOdCQFeF2t3Tz3K6dy5KkY4PnHxfZVQbU/R7Y6VVWP72pMmi9xsHq7Y23s6eqlMEUl2Uxk8q5WY8x/ARYIA1hrf9Dfm6y1e4Bz3cd7cSZLT1znLpzpE0RERERGvCfv2Nzn64WlBXQ0xk9y3nzU6QgVGOKSinF390fwnFy50t3hBG5Nh9uYtKQKgJqlsXnrSsYV0XSglQ53+orCYcrgBUNBIr25mwevbndT0rJ1393Ka26/IO4317C/JWdtGOn8BZAAQgOczxCgxT2fDzxby4LLpw+qXUl8SUH/9Crh3jA7/rKPQ5uOc9Y7F/Wbec/k063FGRs3GZji/iciIiJyyulvwupQcfKlVUejk/Ub6jnR/FkIf+bqVOGfhmzXwweB+KkjKiaWEu6NROexK0g1cXoeBEOBuIv1odbVlnpuvvYT8WMFvaqpJ7uWY+0896sXs/o8idOaePMpDsTS1zldOnPRRdM/t56/O/j2P+1l50MHaK3tYFOKKVoSZTpNwmagA3jWWvuFrFsrIiIiMoJt+Kll3fe29jt5cVF531mgYCj50qqr1QkKh7qLprlmJpMWjeOST50xqAvWkW7igtRF1oMpuqX6g15vygTPQLu7DVagIEA4B8GAxxtjmKizJT7w83dRPJmt/c4W9j19jF2PHMho/fq9zTQdji/YExzE+VLhVsttPtxG3e700y4Mlv+mwHHf9A6ZnOuZfLq7gLfgBHj/bIy5LesWioiIiIxQ4Z4wBzfWcmx7PQ37+u7GNmFe3zM6pQo6vAvwoe6iWTq2mPM+tDTjMWsnq/M+dBozViWPGVt5/cKkZSFfBq/bF/jMvWRqbhqXgWAwGJeNGWrptv34bc/HPR8NGbxIOEJ7vZOZTOwKnU7icYDBZfD8NxwOPV834O2k4p9bz/+9+r+7MneOx75k8umWWWvfaq29w1r7ZiDbKRJERERERix/Vyv/JNqp9NdFs+VYe9oCDkPdRfNUEQwFKR0Xf1F75S1nUTGplMnu2DuPf2zV7Atic90NdIL5oRAsCEAkN136wBcUBODaW8+j2jcZe8uxWCH8zfftzsn+86Wns5d9649Fn+9dd5RIJML6H25nz5rDWW1rMAFeqCDIlf9xFgAvPXYoOhZ0aKTuounPvvZ09mL/to++ZPLpdhlj5gAYYyYBfW9RRERE5CTi75XZdCR57jW/rva+L+Y6GrtYcEXqwgtD3UXzlOKLjS/9t5WUjS8BYOFVM+JWK/BVyvRnNguGqYImxLK6uZgqoau1O5rRetU3z6egOMS5718Sff3hrzzL7294MqddCfPlyTtfYNM9O+OWHd16gsOb63j+N9kFr4k3DLJVOj72/tbaIZwEwF9kxfd76fXNsXlkywl2/HnwAd65wHZjzIvAHuBKY8xhY8yhbNorIiIiMhL5x93555ZLpTdFN7fzP7w07vmMsycRKgxy9rsXxS3P5Vxoo52/d2vl5Fj5//FzxmBeEQvy0mVJhzODF4gGeEPfTdNfVt/LSgVDwbhqokB0eoSTVbgnTGOKKqBbH9iT8TbKJpSw6JUzOfvdiygsG9x4zEAgwNQzJgCpu2UPlP9m00uPO6FWuDdCVz89BxL1++mstfOya5qIiIjIScR33X3khROEe8IE05RR919Qe8ZOLY8+rlk6nrLxJbzqW86IljNeO49Nv3OyC0Vlp16ly6HiH7+YOJYxk66v6b7PfPAK7+SikqbX7XPG2fFjFJMCWv8xOgl7CnsT2SfypizIRFFFAebqmUPVJErGOlk8/42bjqYunvr+VsZMKWf/M8eYee5kVrxtQeYb9UV4BzbUsvzN82k63AoRmLpiIoeeO57RZtRZQERERE5piZUz6/c2p103McBbcf0CispjgdvUFRPjXvcHH+UTSwbTzFNaQV9z2GVQvCZX498yESxw2hfJQQbXCy4Sg9zEaTOK/McvkvybH+k23Rs/NUBidrzM7TK5+7FDNB5MnYVf+fbkojyD4X2v/gDvb59bT+OB1mhAuu+po1ltM/FrOfFyE0/8lzP3ZuXkMqrNuBTvStG2rPYqIiIiMsokXlQ1p8kKHNhYm1RkJTFzUpgw1mvaMifgW/5mdYgajLLx6cdMTV7sFFqZf/m0pNcWvXImBGD87Mqcta0/ueyi6QWuid0EE6eISKw4GTmJZkzoThj3Ouu8yUw5fULcsrYTnRx87jhb/vclHr31ubjXisoLqKwpo7KmbEjbFcvMOgczXdD8+xuepHZnQ2YbTdiEf3zfvEunsuz1cwGYfX4NfRmeCUFERERERgjvIjkQDBAJR+IKGvg9+1ObtCyxu2BiV8AaU8W1t543rGPARgOvNH2qycrHzajgmq+eQ2GKLN/Cq2Yw95KpwzYHHvgCgRxk8NobnMAtMbiomOSMUwwVBQn3RGhviJ/03Fn/5Oir6Z/n7+JPLKdqphOsn/VOw86HD9B4wMnYbfjJjuh6kUgkem729kSi2bah5GVN19z5ApMWV3H6G9PfxFn731u47o4L+92m9z3WLB3PkS0n2Hy/0717+tnVFJYUUFhTwCu+kvq37tdvBs8Yc9AY022MOWSM6TLGNBljdhpjruy3lSIiIiIjnHdR5QV6W/7v5QFvK9WcZAruBq+wpIDVn17BpZ9ekfL1ovLClGPxAoHAsAZ3kNsqms/8eDvgTBngV3PaeJa9cS5X3HxWyu6pw9llNVvdbmGjWedNjgZ3ANNWVrP6U6l/D8/8OBbs9Xb2RrOoQ6myJlbs59j2ev7+pQ3R530Fe5lIvJFROjaWwS6uKOy3sEsmXTQfB5Zaa6cCi4HfAdcAX8qyrSIiIiIjSsOBFvauOdLveqm6X02YNyb6ePlb5lM+sYQJ8/ueCF0Gbuy08uj0CCcT72I8MsDJzttOdLD1gT3Rya6bj7TR2exk7nraU2ebA8EAcy+aSsnYopTZq5MpwPMyeP1lrfwOuxOQd7Z0A9CwN7kC52BNPm18yuVjZ1Qw56IpccuKyjNru/e9TJgb/3ekvDq7330mAd50a60FsNbuBmZaa3cBQzmrn4iIiEjePfbNTdi/7Qdi3QDHTi9PWs8/0XDN0vHMu3QaS93xMOCMibnic2cNe7ZIRh6v2+5AM3iPfnMTux4+wO5HD9LT2csjX9vIP77xXP9vdKXKIJ9MY/C8wDbbuQwj4QjN/cxrORgFRanb01bnjJu78IbTmXnOZADGTqtIuW46xRWFcVNdVE7ObvxgJgHeYWPM140xrzHGfB044nbP7OrvjSIiIiIni1CRc1mUKkvU3RG7r11cWcjS186Jm0hbJB2v66gX4HlZmsMv1PU772JXW0+0sE9nSzettU4BoM7mblqOxYoBJVaV9PNnmrybGA370leKHWl63HMvXVfncTNSn4eNB1ujweGia2flpnEpeNn+CXPHcMZb58ct6/+97oMArPynWNXPMdOSbzr1JZMA75+BQzjdMvcB7wJagLdltScRERGREay3M301vN2PHIw+DqW5cy+SSqycfpij207wx0+tZdsf9rD+7u39ZuI6mmL5lEgYWmpjQd2Ov+yNPvZ3F060/M3zOeudhtf81wU07HO6Kq777lYOv1A3oM+TLz2dvbQ3dMYyeGkCvEXXxM9t5034vvY7W+hqdbpolozJ/RyUV39pFeNmVnDeB06LLXR7x2bSJTYSjnB02wnnbQHiegOEspzHMZN+BJ3AU8Am9/kqa+3jWe1FREREZIRJrJYZcWuUJ85XFu4Ns/vRQ9HniWXbRfriVdF8+gfbol19d/79QPT13p5w2gv4Ht9vLRBwMneegxtjk16H+ijkEyoMMm1ltbO9zthvvm5XI1OWTUj3tmH38FeepaOxi8WvcrJvBWm6PxePiZ8Swpursruth+d+uRMgbq7KoXT2exax8+EDnP2uRZSMKeKST5wR93ogECAQzKxL7M6HD3D8xUbvnQAEC4OEu8MpCwj1JZMA73+BicB+d28RnMIrIiIiIiet47sa454vumYWa+58gZ7OXpoOtTJmqtMtqrOpO2699hPxJedF+uJVcPSP4/Tb+PMXqVk6nmozjpKEYKXbN0XAy08cTtvV0MtaZSNxMvSRItwTpqezNzp3X+0OZw65qlmpu2KOm1FB1exKiisLqTltPBWTSnny2y/ErVOco886dflEpi6f2Oc6gUAgowzeiZeafO9x/n3Fl1dFexZkI5MAb7K19vystywiIiIygvkvngGqZjoXkHW7m/jHN55j5jmTCAQDlI6Ln2R78avzN55HTn79lbQ/tOk4hzY52biz3mmi2TaA9vr4mwk7/rSXVBLnY0ynZtl4jrzgdANMF3AOp4PP1bLhJ/HzTbYe76CovKDPCqoX37g87nnJuCI6GmLdW3OVwcuEN79mX+r3NXN0W330uTdes7CkgMIBFI7NJNzfYYyZmv2mRURERIZXJBKhp7M3buySJ+x25Vrx9gVcd8eFSZOU73v6GHvXHaXxYKwQxorrFzB+dvrxTiKDseGn8cHNnicP97l+QXEoq+57i30ZQG982kgRCUeSgjuA7s6ePrugppIYvJZWFadZM/d6u8M07E89TUPLsXZ2P3aIx//z+fgXBjltXyYZvIuAfcaYWvd5xJ0TT0RERGREe+w/n6fRvbi65mvnUlQWu/TxxuoEve5taS6q2k50RB9nMxeXCEBrXUfK5eXVJbTWpn7N09HYRcWk0riKmX7XfO0cMizQCMCYKeXMv3waux4+yJ41R1hwxfQRM7dg0+HUFUV72nuTuq72x6s8Ck5WdCBdWIdauDeSlM19+CvPxj2ftLiKmedMipsiYSD6/StlrV0wqD2IiIiIDINIOBIN7gA6m7tSBnjexV+6bm7+whaFWc7FJWKunsnhzXWMnzOGyslljJlSRq1tYMl1s/nzTU+lfV8kHKGzpZtAQYA5F0/h5ceTs3leAZdsLL52NrsedqrCPvSFDVzwkWVMnD+2n3flXmJ3VL+2QYx79Xd5HQ4T5o2hbncTDfub+83+n/lPC4ekO2naAM8Yc7O19svGmHuAuHsD1tq3D3rPIiIiIjnUejw+O/LIVzfymtsviAZync3OGJ3+CjD4u7IVKIMnWSquKOTqL6yKWzb1DKcwR6oKi211HZRNKOHQ8840Bh0NXcw+vyYuwLvoxuWUTxhYt8PELNKuhw/kLcDrauvhuV+9yPzLprN37RGmrZgYnaevx+1WueQ1s9n2wJ649y197Zys9lNQHKKns5eKyaVD0u7BmDB/LHW7m3julzu5/LNnAk7wfuj543HrLbxqxpCNFewr7P+D++/3gO8n/CciIiIyou165EDSMn8FzK5WpxtXfxdV4Z7YfW510ZSh5I2fGzs9NpH103dvI9wTZsNPdgAweUkVY6bET3RdNauC4srsui36FfvmhSuuzF8BklrbwJEXTvDkHZvZ/8wxnrprG+CMlfWmLUnVnjkXTslqP0tfPxeAxa8c/oJIs86dDDjj7Tbdu5Pujh4euHFN3HjD5W+ZHzc+crDSBnjWWm+03y6gFjiCM+l5Y7r3iIiIiIwU/sDM89AXN1C/txmIjdMpLMs8aCsdO/CLapF0/DcO2uo7qXs5VjK/2oxLWj/TqpnpXPSx5VTNrgRiXZXzIZIwYLBkTBEn9jTxwMfW8PxvdgNQUBTi4o8vH1ShkZnnTOLqL62KZkqHk3+M4951R6MVUz3Tz6pm9vk1Q7rPTDru/gqYDHwFeAj4r0w2bIw5xxjzaMKytxtj1vmev88Ys8EY85Qx5lWZN1tERESkH2kuEJ//zS4gNmF5YWlm4+ou/beVSZU2RQbD657p/131tPdy4Jna6HOvSMvsC4YuCCifUMJ5HzgNyO90Cb2d8VOTdDR2sfZ/tgAQcacGCBUGqZpVyfSzBj52LhAIZF2YJZcKfH9jNt2zK+61/qbRGIhM/kqFcSY2H2etvdd93idjzKeBu4ES37IVwL/g/rk1xtQAHwUuAK4GvmaMGb4apiIiIjJqNB1pY//6Y9Hny98yP/rYm+C5u62HUFEw40IVlZOGfzyPjC5eRitYEH+Rv+/po9HHE+c54+OGutR/qMj53fd09Sa9FglH4ipRDpVU2cLEANObEsHLUmYzDcRItfqTZ6T8HJU1ZZhXzBzy/WXyF60QuBV43BhzKZBJOLwbeL33xBgzAfgq8DHfOquANdbaTmttI05X0NMzbbiIiIhIOnW7YiNKVrx9AbPOmxx9XlhaQLg3QtPh1qzu8o+GC00ZYdwei+kyw9NWTGTK8glArEtxYjA4UMGCIIFgIGUGb/uf9vLnf3uKpiNtQ7Ivz7Y/7Ol3ncSqtkP1eYdT+cRSznnv4rhll3zyDC77t5U5maYik07n7wauxMnIvRZ4Z39vsNb+1hgzG8AYEwJ+CHwc8E/iMYb48XzNQEYlfKqrKzNZTYaQjnn+6Zjnn455/umY59+pcswP4GRAZp01mTNf5WTv3nbnau75yKPQHeHJ254nEnYqbfqPybn/tIinfr4j5TYHeuxOlWM+kpxsx7ysPPWNhkmzxzJpklNaf/qiCex65CALLpo2ZJ+voDhEIJJ8vHb+3SlQ1GCbmLdscqq3JumvTS/8+eWMuoNOnjaWsdXllJY5x6SgKHTSfZ+pVK+uZOKUMfzxC87UGFNnV1GeozkIMwnwXsLpVvlfwItAckmqvp0JLAC+i9Nlc4kx5nbgEcD/bVUCDZlssLa2OcsmyGBUV1fqmOeZjnn+6Zjnn455/o2WY37o+eNsvv8lLvnEckrHFbN33RG6WntYcMV0wMl0PHvfTgCql1bFfeZQcYhDW+ritud/vTOc3F0t1XqZGi3H/GRyMh3zovICulp76Oru5bwPnca6726Ne729ozv6WcYsrOSCjyxj/Jyh+3zBwgCdbd1J2yuqKKSrpZuD2+uYWdv/2L9MjvnTv0h94yRRc0cnXbVh2tudaUwCocBJ8332p7UjVsW3pauTttruPtbuW19BbyZdNH8AzMUpsDIbJ5OXMWvtemvtadba1cBbgW3W2o8B64GLjDElxpixwGJgSzbbFhERkVPPMz/aQWdTFxt/8SI9Xb1sundXXNevtvrY/Hf+OewAQgndvRYllCb3Cj14Ji1KrmAoMlSi40Hbe5i0qIqrvxQ/X17pQuXTFgAAIABJREFU+Ni4u0AgwMT5Ywc0uXk6BUWhlFk1rzvyse31SefQQETCsfOq2ozj/H9dmnZdb17KoNuGcG9yNdyTVYlbhbdyStmQfo+JMsngLbDWXuw+/p0xZu1Q7Nhae8QY823gCZxA87PW2o5+3iYiIiKnuGBBgHBPhOM7G9l0z8641/Y/c4yNv3gx+rywJL5CZmVNGXW73RL0ATBXzYh7PfFisqAkswqbIgNRUllEy9F2OlucICpxTOiEubmdgDxUGEwZwHU2dUUfn9jTTI07GflA+fdRXFEYnaKhz7a5RWASK2+ezIrKC7n85jOjgV6uZBLglRhjyqy1bcaYUiCjv3TW2j3AuX0ts9beBdyVcWtFRETklOef3+7gxticUpFwJC64q5hcyrSV8aXWT3/zfP7xtY3uG5K3nXiBncu77CLeDYTeFJUsAcon5maMlidUFKK7vZemI22UTyih6XArlZPL4tZpOdoGgwzwutvjP19iwaJAKECkN8LkJVWxtrnVNPM5T18+VFTnvhpvJgHeHcDzxpgtwBLg8zltkYiIiEga9fvSj8Wp290Y93zhlTOSLiTH1JRRUBqip72XyinxF7IANUvHM/+yaex65CBAtIpgcWXhYJsuksSbJsHfhXHC/DHU7WpK95ah5Z4e//jaxujvPrHb8lB0kfTmnASnm2LiPO2R3giXf/bMuC6pBcXKng9Uv7elrLW/BM7Bmej8fGvtPTlvlYiIiEgKe9ceSfvapnudCYTLJ5aw4voFaSdKvuLms5i2ciLnvm9J0muBYIBZ58eKSsxbPRWAldcvHEyzRVJaeJUzB9qyN8yLLrvgw8uYt3oqi64Z+vnREgV8kZZ3U+OIW4SoYrKTaYoMwRA4f4C38OoZcfv1VEwqJeSbLmLstHIAJswbM/gGnGLSZvCMMfeQovOCMQZr7dtz2ioRERGRFApLnUuXiz52Ok/cvjnutdbjzlD+KcsnMnNV+tLuxRWFnPXORWlfD/qyftULx/Ga2y9IeUEqMljjZ1dy3R0Xxi0LBAIsfd3cvOw/kCJJ1rC3BYCiMjdMCA9FBs/pornsDXMpLCmIZi49qcakTVpUxXkfOo2qmSf/FAn51lcXze/lrRUiIiIi/Wit64hmGYr7mKB8RprMXab83TqDBUEFdzJqBYPpO/N5N1MSg7GB8DJ43jYDgQDmFTOprCmlq7UnbRGXSYuqUi6XvqUN8Ky1jwEYY14NnGWtvcUY81ec+fBERERE8urAM8eij4srCqNj6fzGzqhgzNTyQe3HH+CFClVkRUavQCj9zYvCcjfAG4IaJ974WH8Ro3x0QT1VZfJX6wvAbe7jtwC35K45IiIiIql5c4aBU4Dh4huXU1Aa38cs2q1sEAK+qyMVepDRLNBHJOCNh3vxwf2D3s/h5+son1jCxAW5nfZBHJkEeN3W2kYA99/RMxmFiIiInDT8hRoAKieXce3Xz+OVX4/NylQ4BAFeqMgJ6sZMTa6yKTKa9DkNiK9rctuJgU9VHQlH6O0OUzquOKmqreRGJn8F1xtjfgWsA84Gnsttk0RERESSeQHenIumxC33XzQORQavoDjEK768Stk7GfUSu2iGCoPReecqqmNz8NXvbaFs/MDm5OvucHJDbfWdA2ylZCuTaRI+AvwGKAPut9Z+NOetEhEREUnQ3ZYmwPNdpFZMGppJhIsri6KZPJHRyl8xdvycSi6/+czo8wnzY90pwz2xgXgvP3mYHX/Zm/E+1tzpVLttqxt4FlCyk9FtLmvt73LdEBEREZG+eKXWvUp8Hv9F6phpgyuwInIq8We/V/6ToXRcMeWTSikoCsa9tvEXLzJxwVhKxxWz+b7dACy8cgbBgv5HezUdahv6hkufVBpKRERETgqHNh0HUoyz8/Uyq5yscXMimfKy3yVjiyif4HTBvPSmFVx4w+kkjpZ79mc27vmmX+8iEo6w8+EDtDWk737pna/L3zwv7ToytAbfUV1EREQkx3q6YjXeQglZA/88dcW+Spsi0rfxcyrZ99RRJi2OzTfnnV/h3vj579rrO+PmxNu//hgnXmqi9XgH+9Ydjeve6dfd1kMwFGDWeTU5+ASSStYBnjHmvdbau3PRGBEREZFUejszK+KtKn0imZu5ajJl40sYP7sy6bWxCd2dA8EAD31xQ9yy1uPOuLqW2vaU24+EnYAw3BvRuZlHA+mi2TrkrRARERHpQ9i9UJy2snqYWyIyegSCAaoXjktZUChYEGThVTOiz9tOdNB+IrtKmD3ujZnJp1X1s6YMpawDPGvtPbloiIiIiEg6Ebe7WDCUPgsQ0rQGIkPKy8A5j2PLM83GeQFeQbFGheVT2qNtjLnfWvtGY8xhIELcEGY6caZM+GSuGygiIiLijQdKnLfL88pvnNv3pM0ikjV/gOc5/U3zopU0PYFggHBvOOkc9ObUCxXq3MyntAGetfaN7r9TEl8zxhQCj+SwXSIiIiJR0QxemsxBYYkyBCJDbe4lUzm6rZ7mI7GpDgpLkjPlkXCEpkNtjJtREbfc/mUfAKEiBXj51FcG78c4mbsk1tr3GGMuz1mrRERERHz6y+CJyNArHVfMZf+2kr/e/DSdzd0AFJQUUFpVTHu9Mx6vsKyA7rYe6vc2xwV43R09HHi2FiCj+fJk6PR1tO8Ffg2MB3YAPwQ2AyUA1tqunLdOREREBDi69QTgXHCKSH5NXDAu+rigJERXS3f0+cIrpwNOERa/rb/fE32sLpr5lfZoW2v/Zq39G1Bmrb3VWrvGWns7oPJVIiIiklfb/7QXgNkXaC4tkXybtnJi9HFhSYhV71tC5ZQyzv3Akugcej0d8VOZ1Nr66ONggTLv+ZRJh/UKY8xlwDPA+bgZPBEREZF82PGXvdHHhaUaayeSb/5xd0WVhYydXsFln1kJxDJ3e9YcYeFVM6JZ9gJfVVvNgZdfmeRL3wPcCGwAPgC8M6ctEhEREfGxf90/3E0QOaUV+IoYlYwpin/NF8g9eMsz0cf+ufWaD7ch+dPvbTBr7Q7g1d5zY0xSVU0RERH5/+zdZ2Bc5ZXw8f90dcnqsiXLRdbjjm1s44LBGEIPpJBN32RT3tRNSN8lPaSRRuom2fTew0JCIIRmwBjcu33dm2Sr15E0mnLfD3d6kUZlZlTO7wsz9055dD3M3HOf85wjxsLd78FsNces1elq6A3eljQvITIjvP+kyRT5/6E1Qf/JQOVbgOorZYVXOg0b4Cml7gXeCdiBHOA4sCTF4xJCCCHENHF+RxN7f3MCgJu/cBW2bCu6rtPXOsDOnx4LPu6aD16RqSEKMa3llGRhsZmp3RC7BjZRhUxd17E4LNx879qEQaBIjWQS2V8KVAP3A98A/ieZF1ZKXQXcp2naZqXUCuA7gBejSfq/a5rWpJR6O0bapwf4vKZpfx/F3yCEEEKISez01sbg7Uc//iJgzNb5PKEZgDvu3yjreITIEKvDwu1f25Bw/7XvXM7WHxwgryI7uE336pjNiWf4ROokswbvkqZpLiBf07STGDN5Q1JKfRT4MaGCLN8C/lPTtM3AX4GPKaUqgfcBG4GbgC8ppaT2sRBCCDHNxAvcwoO7ymXFEtwJMYHNWm5U2Syoyg1u03UprpIpyQR4F5VSbwGcSqkvAUXDPQE4Bbwi7P5rNE3b579tBQaAtcA2TdNcmqZ1ASeB5ckPXQghhBBTgXfQN+T+NW9emKaRCCFGI6vAjsVuxtnSH9ym+/SY9XoiPZJJ0XwHUAP8CXgz8LrhnqBp2l+UUnPC7l8CUEptAN4LXIMxa9cV9rQeoDCZQZeV5SfzMDGO5Jinnxzz9JNjnn5yzNNvoh1zl9NNz+X4Ffbmrati/ZsXkV0wuRN8Jtoxnw7kmKdfcU0+bee68ba52fqDA/Q295MzwyH/FhmQTBVNHxBoQPOd0b6RUurVwMeB2zRNa1FKdQPh/+L5QGcyr9XS0jPaYYhRKCvLl2OeZnLM00+OefrJMU+/iXbMuy85OfGvi3H3zdlQybJXz6fXNUhvy2CaRzZ+Jtoxnw7kmKdfWVk+2aUOfKd09j1ymp5mYyZP1+W8PVWGCpzT0i1UKfUGjJnAzZqmtfs37wC+oJTKAhzAIuBQOsYjhBBCiMzSdZ2nvrw3eL9mbTkXdjQH73vdQ6dtCiEmlkAxFa8r9P+uKZnFYGLcjfiwK6VmjPDxFuDbGDN0f1VKPa2U+qymaZf9258FngQ+rmnawEjHI4QQQojJJzots+66WSy+Yw6ldcZqjdL6pFZtCCEmiEBBFc+AN7gtUQsFkVrJ9MH7jqZp/+m/fRNGmmb9cM/TNO0ssM5/tzjBY34E/CjZwQohhBBiatj3u5Mx2xZcX03ddbPovNhLUU1eBkYlhBitQIDnHvAEt4U3SBfpk0yKZrdS6stAHrAUuCW1QxJCCCHEVFc4K5eOc6G1OYH+WSaziRmzpSiDEJNNIMDrPN8b3CYzeJkx7FHXNO3jgAWo0zRts6Zpp1I/LCGEEEJMZW2nu4O3i2bnYbbIiaAQk1m8nndmq8zgZULCGTyl1CUg0GXUBFQopRoBNE2bmYaxCSGEEGIKcrYORKzBW37X/AyORggxHuIHeHLhJhMSBniaplUFbiulcjVNcyqlZmqa1pieoQkhhBBiKgpPzQQorM7N0EiEEONF1/WYbXll2RkYiRg2rFZKfRqjfx3At5RSH0vtkIQQQggxlQUCvLots1h21zxJzxRiCjjzTOwcUKEUS8qIZL5R79A07R4ATdNeBdyR2iEJIYQQYirzuIwy6rXrK5m3SVZ9CDEVOArsMdvyK2QGLxOSCfB8Sik7gFLKluRzhBBCCCFiuPs9nH+hCQCLQ04phJgq1r51Ucw2i92SgZGIZL5ZfwAcUkr9BdgHfD+1QxJCCCHEVLXz58eCt21ZyXRrEkJMBvkVOSy5c06mhyFIrk3CT4CNwH3ANZqm/TLloxJCCCHElNRyrDN42+qQq/tCTCV1W6q581tXZ3oY014yRVaWAf8AHgIeV0qtTPmohBBCCDGlbXzv0kwPQQiRYvEqa4rUSyZF89vA2zRNqwT+A/huaockhBBCiFTxenx0NzpH9Vxd15M+YdN1Hd0X+dhBpxsAs81M6YKiUY1BCDHxrXz9AkoXFFI4S1qgZEIyAZ5J07T9AJqm7QM8qR2SEEIIIVJl18+O8dR9e+m80Dui5w063Tx09zYOPXAmuff5hcZTX9kbEeSdfLIBAJ/bN6L3FkJMLrPXVrDxvcukBUqGJLO62auUuh14FrgGcKV2SEIIIYQYb/2dLgZzs7h8qB2AvvYBipLoUXVu+2UKa/LwDhpB2emtjSx7xbwhn9PT1Efj3lbACAwd+Ub5dHe/cY04p9gx6r9DCCHE0JIJ8N4CfA34MnAEeFtKRySEEEKIcdV+pptnv3kgYlsy5cvbTnWx7/cnAdj80dASfF3XMZlMNB/rYMdPjnLth1aQX5ljBIPVeTx7//7gY90DXhz5xm1nywAA135ElvMLIUSqJDNv+hJN016ladpSTdP+DXhZqgclhBBCiPFz+KGzMdu8g95hn/fctw8Gb7doHcHbXQ3GGr7t3z+Md9DHk1/ag7NtgH2/P8nWr+3D5w2lZZ7410XAmNVrOW5U0LTnSHsEIYRIlYTfsEqp1wJ3ANcppbb4N5uBZRiFV4QQQggxCcRrR+DzjKy63eEHzwZvd110UlQdmd7Z1zoQ93nnX2xi/pZZ7PzJ0RG9nxBCiNEZ6hLao8AloAT4oX+bDziV6kEJIYQQYuwGugbZ9/sTNB/tiNnndcefwWs+1sH27x8esvqdq2cQjyvy+dt/cDjxODpdWLOMIHPl6xckM3QhhBCjlDDA0zStA3haKbUVyMcI7l4OHErT2IQQQggxQrqu4/PqWKxmjvztLE1HjOCusDqXrouh9gjesEqWA92D7P/jSaqvLGfXz48BoTTMeLob+3j4o9sj39eXeEbwxR8dCc4Y1qwuH/kfJYQQImnJJMH/Dvg7sAEjRfMVGIGeEEIIISaY449d4Ng/zrPotlp6W/qD28vqi1j9ygXs/r+TdJ7rDbYquLinhd2/0AC4fLA97mtmFzvobw8V0W7Y0zLsODbdvZyey33s+/3JiHRQk9k0qr9LCCFEcpIpsjJT07RfA4s0TXsnxmyeEEIIISaAXb84xos/PhK8f+wf5wE4+vA5Os72BLeXLihk7lVVLLq1FgjN4AWCu2iBFgrr37mEDe9emvD9yxfNiLu9eG4B1Wtktk4IIdItmRk8u1LqFcARpVQpEuAJIYQQE0bDntbg7Yu7mhM+rrS+CACLzbi2G72GLsDqsHDrfetAB1evm6wCO65ed8LXrV1fEXeNH4DFGnkdef0QgaIQQojxkUyA9xXgNcAHgfcB96Z0REIIIcQInHnuEvZcK7NWlkVsbzrczgv/a8xsLblzDqX1RTGVHye73uZQCqbu0zn9zKWYx1QuK2bFaxYEgy2zP8A7+UQDFYuLceTZcPW6I9IwTSYTmCCrwGhQHiiQEo891xazrbS+MO5jy1VRkn+ZEEKI0RqqTcLdmqZ9U9O0vwJ/9W/+VPT+VA9QCCGESMTj8nLgT0Zx55krSo3AxC8Q3EGoxP+d37o6reNLteaw3nS6T6dwVi4d53qoXFbM5YPtFM/N56q3LY54TmAGD2Dbd4w+d8XzCrjyDfW8+OMjLLljbsz7RM/EReyzh/ZVXVFCzZpySufHD/CEEEKk3lAzeB9QSiVKnjcBrwUkwBNCCJERB/58ijPPhmasXvjBYda/K5QCmF3koL/TFfEcXdcjgsDJLrzwic+nB//eVa+rp+lYB5VLimOek1+RE7OtXBWRU5LFdR9bNeIxOPJCM3hWh4WqZSUjfg0hhBDjZ6gA71ND7AP49FA7lVJXAfdpmrZZKVUH/BzQMdosvEfTNJ9S6tPAbYAHuFvTtB1Jj1wIIcS0Fh7cATQf64y4X1JXwMVdkdUeL+1vY+aK0pSPLV18nlCrA92n42wbwJZjxZZjpXpVWdznmMwmVrymjn2/PxncVrYw+dTJORsrObvtMovvmEPhrFxyirOC+3JLsmIev/L1Czj419Ns/vCKpN9DCCHE6A3VB+8Xo31RpdRHgTcCgSY63wA+oWna00qpHwB3KqXOAdcCVwE1wF+ANaN9TyGEEOLFHx1hzVsWYraYI4KfgJ0/Ozal0jTD2w/oXp3+dhd5FdnDPk+PalmXXxk7q5fI8rvms/QV8+KmbRbVxtZhm722gtlrK5J+fSGEEGOTTJuE0TiF0S8v4Epgq//2I8ANwNXAY5qm6ZqmnQesSqn4lxuFEEKIMHp0hOJ3+VA7ztYBALyDsQEeEDfwm6zObr8cvP3IPS/idfviFj2J5h2MrKBpyxq+5trV71vGmrcsxGQ2xQR3K1+3gPJFMyhdIGvvhBAi05Kpojlimqb9RSk1J2yTSdO0wK9xD1AIFABtYY8JbB+2e2pZmXRqSDc55uknxzz95Jin32iPefPJzoT7Bi4NMG9phZH8b4Jr37UcdNj6/QMA/O1Dz/Oa72wmr2T4ma6JqK/TxaNf3knd1TONhQ9R8ouzhzyuZWX5FL20jv5mFz6vj3nrqpL6dxjyNW/P58rb65Ia/3Qk3y3pJ8c8/eSYTxwpCfDiCL9cmg90At1E9tQLbB9WS0vP8A8aZw17Wmg53snCW2uDZaOni7Ky/Iwc8+lMjnn6yTFPv9Ec8+5LTmzZVh779M6Ej3n+Z0fQs0y0X+zBlm2lSBUAkF+VQ8+lPgCe+N4+NrxrcvZkO/VUA+3ne9jx2/gNyq351oTHNfyYL/23ecHt8tlPHfluST855uknxzz9hgqoU5WiGW2vUmqz//YtwLPANuAmpZRZKTUbMGua1proBTLJ4/Ky6xca57Y38dR9ezM9HCGEmJYGugd56st74wZ36uaaiPvbv38Yd58HW3aof5s9N3RNU/fET/GcyAJpqe6BqPTKnMhrtXnlk3NmUgghxPhI1wzeh4AfKaXswFHgz5qmeZVSzwLbMQLN96RpLCN26qmG4O3BXncGRyKEENOXq3swZlv9jTVkFdjJq8wGLsTs72sLtRHIKc6ijW7AqCQ5GXSc76HpcDunn7mEu8/DtR9egdcdGeAVVOXQfqYb3Z8rUybNxIUQYlpLWYCnadpZYJ3/9nGMipnRj/kM8JlUjWG89HW4hn+QEEKIlPLEKZqy6Lba4O3NH1nB01/dl/D5i26tpfloB64eN4XVuSkZ43hy93l45uv7I7Zt/do+atZGtqi1Oizc+uX1XD7cTtXSYix2C0IIIaavdKVoTmp9bQPB2xabHDIhhMiE6MqP0Qqr81jzHwsjtmUXOUK3ZzjY9IErACPdc6LranTG3d50uD0i3bSkrhCrw0L1qjIJ7oQQQkiAl4zwH9LC6rwMjkQIIaav6LYHV/zb/JjHWByRAc6at0QHfHYwQb8/M2Pnz47y8Ee3J2y7kCk+r87Rh8/F3Tfo9FC6oCjYsL1kXkE6hyaEEGKCS9cavEltsM8DgNliwjPMFWQhhBCpEf39W7W8JOYxtqzIAK+oJvKinNliJqvATtupbh6/d1ewZ96g04Mjb/j+cely7JFztJ/uTrjf5/Wx+k0LUbfMpmAETcqFEEJMfTKDlwTvoM9o7Oqw0N3gnBSpPUIIkQm6ruN1j18jcY/Ly6mnGhjs8wTT5Ve9oZ7bvrIeR35sy5r8sGDnmg9dEbeYSk6xkbYZCO4ABp0Tq4DWiX9dDN6+7avrqbu+mqWvCLU1WPqyeVhsZgnuhBBCxJiyM3idF3qx51jJKcka82v5PD7MVhNu/0zezp8eZdPdV4z5dScDXddl1lIIMaxBp5vuS32cePwizUc7uP3rG7BYx3YNUdd1Hv7odsD4Tr+4uwUAi92M1RF/rZktO/SzFj17F5Bblk37mch+Te7+ifM95x7wBG+vekM9VruFJXfMwef1MdDponZ9JbmlY/9tE0IIMTVNyQBP13W2fs2opHbnt64e8+v5PDpmqzm4/qPb3yh3Ojj/YjMP/e4EV79vGSXzCzM9HCHEBNLV4CSrwIYj386z3zxAb3N/cJ+7z4OlIHaGbSR6m0Kv13KiM3g7UXAXcMsXr0LXwWSK3wohe4YjZtu+359gy3+tGuVIx1fP5dDfXb5oRvC22WJmyZ1zMzEkIYQQk8iUTNH0hTWwDb8SOurX8/owh12J9gx4J9yC/FTQfTon/mX0lXru2wczPBohRKZ4B73s/9MpnK2hwMPV6+bpr+zl0U/swD3giQjuAHxjSNP0DHrpbe7nhf89HHq/7lAK5XABnj3XNvR6ujhf3z0T6MKdz2McO3VTzYRaFyiEEGJymJIBnscVSrX5x8deGPPrGTN4kVeCG/e1jvl1Jyrdp7PnN8d56APbItaoCCGmp1NbGzn73CW2fe9QcFuLFppRi/c9O5bU7m3fPcgTX9gd0aQ83Iza/FG/NoBnHC78pVIgwDONMcVVCCHE9DQlfz08A+O3lqLpSDv9HS5sWRbWv3tpcPuun2vj9h4TzfHHLnBhR3PM9ukwaymEiBWYnetvDwVcHed6Ej0cgDPPXuLoP+KX+R9O57nehPsqlsyIWzhlJNzj+BuRCj6v8V1rtozt7xRCCDE9TdEAL/Lq7GirXnac6+GFHx4BjIX75apozGObDM4+fznu9vGsjCeEmBx0XY97wcfVM3TVybPbLnP8nxfGfTzr/t+SMb9G3ZZZMdscBRMnFTIwgycBnhBCiNGYmgGeK/Lq7IWdsScnyXC2hNaUmG3GoYrXd2kqaTnRyUBX/IB4PGdGhRCTQ/j3aVZRqGiKuz/yQtqi22u54tV1McGTq3ds7QcqFs9g0W21ALzk06vH9FoBBVW5MQW4xpr2OZ6CM3iSoimEEGIUJuWvR1dDL//67E60x+JfHY4ORPo74q/jaNzfmnC2CiLTeALlvssXhmbxAldZp5LLB9sT7huq6a4QYmrSw77mBjoHadjbgq7rNB/tAGD9u5aQVWinYnExczZUkl0cWaFSe/T8iN8zPJAsnldA/Y013Pmtq8kpTl1rAN07cVLQgzN4VpnBE0IIMXKTMsBrP9NDX7uLYw/HX9/hjprBS5RauPOnx9j/h5M8/dW9tJ7sitnfF7bepMyfnlm7vhJrllHBzTOY3gBP13WO/P0s7WdTF2jllWcHb698/QLuuH8jy19qlOXe+bNjKXtfIcTEFH0ha9fPNXb85GjwfvnCGdz0ubUUzsoFjFL+4QLNyUcivIF55ZLiET9/NHy+zAV4F3Y2c/LJUGNzPbgGb1L+RAshhMiwSffr0XXZyYE/nQred/W66e+MnKGLLs/tHaaaW9dFJzt+fCRme2DGqm7LLOZuqgLAZDYFTziGe93x4uoZxOvxceyR85z410Wevf9Ayt4rUH48pySL2WsrMJlN5BRJQ10hpis9TuAz1Ez/oDMyJTNeA/Gz2y5x8qmGhK8R+A5f+9ZFFMzMTXaoI7b2rYtY+nLjAlamZvDaz3Sz59fHOfzg2WAhK6/M4AkhhBiDSdfo/E8ffCbi/qMffxGIbGjui/qh9iYz0xanIW77GSPAW3T7nIiGuRa7ERf3dw5iz7MF0zdTwTvo5dFP7IjZPuh0Y8uxJmzkO1qBY6durgluy4nTFFgIMT1Ef58Op3Z9JR1ne6i/qYbt3z8cs1avp6mP/X80LtJdcVP8pt0+j4+sQnvK1zxXLS9B13UOPXAmbiCbagNdgzz7zdAFu/4OFznFWegemcETQggxelPy1yP6SmyiE5TssMAlECfpuo6ztT+iJUB0JTOL3Zjlevb+/Wz/n0OMhnvAQ8f5npiCMLGPi7//kXte5KG7t43bScmppxvovNCLu8+4+m4OK0M+96rKcXkPIcTkM9Ra4+s/fmXMNkeejavevpgZs/Ox2MwRKfKunkG6LoRaIPQ2x28u7hn0YrGl5+fJZDJn+hH8AAAgAElEQVRhMpsyEuA5o9JXz21vAsKLrMgMnhBCiJGbdDN4eWXZ9IZVtww4/q8L1L/EmHXyeY0TinnXVHH6mUsJU29MYecPgZmwi7ta2PPr4yy8dTYAJfMLYp4XSGMEaDs18vVwlw60RaxhufGza8guij9LFp1+Gq2v3UVu6dhSKHua+jj0wJmIbeHV20wmEznFDvraXeg+fcw9qIQQk8dQbWayCu0J9wGYLKbg92/3JSdPfXlvxP49fznJlW9dCEDH+R5sWVZsOVZc3W7yF+SMceTJM1lMGVmD52yN/C07/tgFataWB3/DZAZPCCHEaEy6AC9ecAdw9O/nwgI844e6TBVx+plLwR/LaOHV4QJzmU1HjMpwx/5hVH4zx7mKPNYAJzy4A+i62JswwNv9i6Ebqnee78E76B3TOpV47Q8CaagBuWXZ9LW78Pl0LBLgCTEt9He5eP67RpbC8lfNp3BWLv2dLnb93Pheiv6eiGY2m/D4ZwA74jQvL6gMfW898/X9EfvCCz6lmslsStsavIu7W7i4u5nZV1Vw8K+ng9tzS7Nwtg7wxOd3U31lGSAzeEIIIUZn0gV4yQivQGa2mBKmaEam5Bg/pNE/qCXzYmfwxj2VJ8E6uq4GJ87WoSvQ7fIHgJs/soLC6rxRvX33JWfMNj3qTwykqepefYp+aoSYvnRdBz324tW+350M3i5TReSVGUFXb3M/ngHvsGuAjRk8I8BrOxVbqbinxUjRjHcRrrcp/sW8VDCb01dkZfcvje/spsMdwW2bPrCcU081Br/vL+5uAcAkM3hCCCFGYfL+egxxXhFY1G/NshipNwnWkIQHaq7uQVpOdMas+5i1sizmeWPpfxddcABiq84F7Pn10LN34bb/8PCox3TkobMx26Kvnlv8aanugdjxCyEmt2fu38/Wr++L2d7TZARg9lxrMLgDUDfNZsmd8QukhDP7Ux/bz/ZwYUdzzP5AE/Sjf49teWPNtsRsSxWTxZyWFE09+sqZny3LGnc5gMzgCSGEGI1JG+DVrC6PuB/eGLfP39g8u9iB2WpOcgYPnv/uoYjCKxA/BSn69frak+/zFO+xfW3x19lZHbFTZfmV8delDPaOLvDSdZ1Bp/HcVW+oD24PP5kDgimk/R2J1+MIISanznO9dF108uw39+NsG6C/w0XzsQ4GnR4KZuVyyxfXjep1TRYzulfH1RP/e2PQX9Tp5JOxLRMW3T5nVO85GukqstJ+pifudmuWhdp1FbHrqSdO73UhhBCTyKQN8MIDL3uuFZ8n9EvY1z6A2WIiK9+O2WLCO+jj3PbLETNlPq8vGNiEs2VHBlWBipnhotOY/vXZXUmPO15PqP6O+AGiI98Ws83iCP3d4SXEowPTZAXXKy4swpFnS/i+2f4AemCYoi9CiMml+VgoVbD9TA+Pf24Xj31mJ9u/fxivyzumNikms9GmprshNg3cnmult3Ug4RppmyN9M3gWmym5djpjdHprY9zttmwrFruFNW9ZFLE9ryJ96xCFEEJMHZMywFv6inkRhUFyS7Nx93vw+BuP97e5yJ7hwGQ24XX76GsbYN/vT/LIPS8GU4IGOuNfUY6+ihtvBq9uy6xRj71hT0vMtr72+EFToEKdyWyisMZYX6duMqp7Vl9ZxsrXL2DVG+rJr8xJGCQOpaepj2f8aVkWq5myhUWseG0d135oRcxjAwFkYHZUCDE1bP/+0OndI+2DF67rghHYHXvkfMy+QacHj8sb/E4OVzyvAEdB7IWmVHHk23H1uFMyi9d6sou9vz1Od6OTxn2tMfstDkuwMnP0EgFrnAuMQgghxHAmXbmMkjkFzF5TTsfZUKpL6YJCOs710Hq8k9L6Ily97mBVyegKka0nupi1sjR4wjH7qgrOv9gU3B+90D66Bx5AVsHQpcET8bp9nN12OWKbxWGJuy4PQq0KNr53KfmVOXhcXnKKs7juv1aSW5aNxWqmZk055164jH7ZSLccSePzvb89QXdjX/C9TCYTtevi97wLpGjKDJ4QU0eiNWHhCqtHX6E3OsdwyZ1zaD/Tg3vAQ+txo+hKvNm9Te9fPob3HDlHvg3dpzPY5wlmMoyX/X88SW9TP+dfjF2DCHDDJ0K9BEebiSGEEEKES1uAp5SyAb8A5gBe4O2AB/g5xlnAIeA9mqYNmSfz8i9upKWlhyUvmxOcDQtUj3zxR0fZcs8qAHJK4v9QXjrQRuWyYi7sNH5sc0uzWHBDNScevwgQs9A+UcA0VHXORKJTkcrqi+hq6I1IL433eGuWBXuuDXuuceJRUBV5wmU2G4Gg7tUxjWBRfniq6XAN1wOpq6eebmTpy+cl/R5CiInL3Tf02l11Uw3zrxt9xoLFYcETlpZuy7ay9q1GGuK+P5zk3POXeeGHR0b9+uMlcNHO1T047gFevFlBdXMNHWd7WP6quogLhhabGVuOddh/FyGEEGIo6UzRvBWwapq2Afgc8AXgG8AnNE3bhFEX885kXyz8R9HrDp1A9PtTL7MK4wd4DXta+PuHng/et+fZKJodai/Qc7kveHvZXYkDmZu/eFWyQw0Knx2s2zKLtW9fxKDTQ8/lvrhX0sPbPQzF5J9lfOore+lp6uO5bx+g6WjHkM8BIk5kFt1WO+Rjw1NVW050cmprY7Ci5tNf3csLY6jiKYTIjEAftjlXV7HghupgKjjAdR9bycJba2PWJY9E9AWy8HXP0RUiA99jlcuKR/1+oxVIhx+qqfuoxbl+V7qgiPXvWhpbVAVY+9ZFzKjN56Z7147/WIQQQkwL6UzRPA5YlVJmoABwA+uArf79jwA3Ag8k82Imk4kt96zClm2l83woXfPgX04BYMtKbu2C1WGhankJVocFj8uLs8VYy7b6zSpui4QAW9bID114DFe7vjJifUXPpT6yix0RrxuY2TPFSRMNF0gj7W3q58kv7gGg7dRh7vzW1UM/zxoK2oZrlB4+1kDj446z3ax+00K6LjrpwsmFXc0x1U2FEBNTb3M/F3cZWRA5xQ4WXF9Nx7mDwf3ZxWNPF4yevZoxJz9422KNvHBlsZq55mNXkFMcG/SkmiPfP4PXE79lzWj1tvTH9DItnldA4azE37eldYVc88ErxnUcQgghppd0Bni9GOmZx4BS4HbgGk3TAmcAPUBhMi9UVpYf8d/qucW8+KOjxpsEmuO69OD+oRQUZFFeXsDqV9fzwi+Povt73FXUFCb1/OgxDaXPavzQZxc5mLskMhB66r69FFbl8qqvXxPcZrcZQVVZeT55pYmrqWXlxE8pMvfrPP3d/Sy9dQ5qc03Mfov/6vq171xORWVsD6ZwM2tnUDQrj86G3uC2hj2tXP2m0Edoz6+Os+qW+UO+jkjeSD5/YnxMp2Pe+EKo4NOy6+eQX5ZN7YpyWk90Yc+xMrNmxpjfI7AG+sq7FjB/YxUFFaHApqA48jtt2W1zmbe0YszvORpdJd0A5Dhs4/YZ0HWdB9//XMS2kjkFvPzzG8fl9cdiOn3OJwo55uknxzz95JhPHOkM8D4A/FPTtP9WStUATwLh1Urygc5kXqilJbaXkD3XGpH+oztMcR8Xrbt7gJaWHgYG/dU1/RXduroHsCbx/KHGFK3fX4GypK4g+HhHgR2XPy2o65Iz4nX6+4ztHZ199OuJ12S4EzRef+C/tgHw7P8eonhJUcz+vh5jPAUL8oYcf1lZPi0tPVQuL44I8KrXlPGH9z8d8dimy13DppSK4QWOuUifyXzMdZ9Oz+W+YWfiw+38nQbAqjfWM4CHgZYeZm0ox1HmoHRB4bgci8LqXLouOsmfm4vL7It4zazK0EzdgpdUU7OpImPH39nv/w7u7B+3MXjdkd/L69+1hKLZmf+MTebP+WQlxzz95Jinnxzz9BsqoE7nmXgH0OW/3Q7YgL1Kqc3+bbcAz472xaPXiczdVBXzmOK5sQcisA4kUJ46EIRF97obD4ECLuGvXbYg8aRlIEUzeq1KtHjrOJLh7vNgtpkjUjWHEl1UxmqLTYPtT9B+QgiRGrqu88w39/PUfXtpPtrByacaOPnkxaSfP2tlafC2yWyiTBWN2/ffuncs4er3LQsWwgqXVxaawSuZV5CS79xkBdLgfQkulg3H2TbACz88TE9TaA13dOGq8oUzsOdMusLVQgghJqF0/trcD/xUKfUsxszdPcAu4EdKKTtwFPjzaF/cGrXmLnp9B0B+VS7tZ4yrC8tfNZ+Ocz1ULjEW9Ec3NE9FP6RA0ZTwE5mLu2P74kWPYbg1eCO5ah98bV3H2TZAzgjW2USf/DjbY3vvHXnoDGv+Y1HMdiFEapx97jKd54yZ9dPPNNJ0xCiwdPjBsyy/a37ci10AthwrWYX2lM64ZxXYE7aVCf/OzWRwB6Hfi0QVjYez7bsH6W93kVOaxfJXGmnqw1UmFkIIIVIlbQGepmm9wL/F2XXteLx+oFEswLxrQic0NWvLubDDaIkQXtCtdl0Fc68OPc4c1WDWOzi2H2ev24d30BtsbQChnlPmJE9mAgHVcCdgjjxr1H1b3ObB4dx9Hjz9XnLnJT/7Fz2D1xdWPGD+5pmcerqRxn1tuPs82ORKtRBpcflwe/B2ILgLOPDnUwkDPJ/HF/dCWLpYwyrzRl+gS7dAe5nRzuD1txuZH10XQins3rAAL/r3RQghhEilKfOrE36CUFofWm+26vX1wduB9EuInRWLbncXvX4iWfv/cJJ/fmoH//zUDh6558WIfcEZvLD3XveOxQlfyxdskzB0QBhenfPaj6zgps+v5br/XhX5mKgZyQN/NqqNjuTKecHMHACKao10q0B1uOrVZcy+KlQcoVkbvkWDEGLsdF2neZiWKPF6qum6jtfty2jgYbaaqb6ilNyyLGbUZnZhfnAGb4S9TQEa97UGbwcyRCByBs8ygv6kQgghxFhNmWmW8PYC0Y1qb/niVZhMJs7vbKbpSAdL7pwT058p+n6Zii1KEk3dXIP26AVySrJoOd7JkYfO0hl2BReME6nAa+tx1uANdWLj8+pgGj4ICw/wivxrXQoqcyIe09XoDO4DowImGO0ZkjV7bQVZ+XZK6gp54ou7GfCvtytdUEjBzNxgw/iB7vEtNT6d9Xe5aNzbyvHHLpBdnMWmu5dndNZFTCzRF5EAqpaXsOI1dWiPnuf0M5e4fKQ92L7E5/Vx6UA7u35+DAitPc6Umz+2ZkIsyjcHUzRHfmFv58+Oxd0eHuDVrM1MdVAhhBDT05Q5Uwy/Eh0dNNlzbdhyrMzbVMV1H1vJ/M2zYp5vCjsSq95YH9H3LZGFt9Riy7FisZt5/nuHYoI7iGxuHiyyEhavha9DiQ5Mda8+7OwdQMncfApm5bLitXUR2xffMSd4e+tX9wVve8LST5MJZANMZhMVS4qxOiyUzg8VhwmkkAZe68jfzib9mmJoj31qJ4ceOMOg00PXhV6evX9/poeUUMOeFh795A4GuqTQTjp0nOsJzs7lh13QmXN1FfZcGzP9fTy7G53BfeeebwoGd5D5AG+iCHzPNh3pwOfxoet6MKV+pAJB4ol/GYVuKpcWs/Rlc8dnoEIIIUQSpsyve2BdmyPflnDGy2Q2UTAzN+7+7LDmutVXJm5wHvOapqELsoSn/MQrmhJeIdMX9To+ry+pAggWu4XrPrqS2nWVEdsXXF+NI06BA2dLf/D2aE88bLmhGdPg3+D/j2+U6a1ieN2X+kZ94pkK7We6+eendnDk72fZ9QsNV/cgxx45l+lhTXldDU6e+YYR7JfVFwUbY+dVZFPuv9Bi86etewa89He4eOb+/cHU7IDhCjhNF4EZvN7mfrR/XuChu7fx0N3b8I5iRu/wQ2dpPdlF60mjaHReRXbGi8gIIYSYXqZMiqajwJj9yi5KvipkuNySLK798ApyS7Ji0jWHZDINGdBEBnj+p4T92Ie/l+7T6bzQy4nHL7LslfMY6Boc8wlY/Y3VHPzz6Yht7n5jBq/+ppqY6qHJssapgFcyb+hm6WJkomeEC2vy6LrQS3ej07hQMZLPaQrous6z3zwAhGYrAHouJ5/2K0aup6mPbd89GLw/a1UpVoeFm+5dG9EuJrAu2dXj5sjfztJxNjYVMrw41XQWfqHt+GMXgrfdfR4sCaqAArj7Y9c3nt7ayOmtjcH7czZUxjxGCCGESKUpE+DVbanG4/IxL0HFuGQU1cT2ahqOyQTewcQBnu4N7QvM4EVX0Vz3jsW88MMjeAa8bP2akUoZWLhvzx3bP9Hcq6s4+9xlnK2hWTvt0fPA2E7uBp2hdXaBq99mixlbtoXsGaMLskWki7uag7dXv0nRebGXrgu9PP2VfeSWZXHDJ1ZnZFzeQS9nn79MxeLimH22HCvtZ3rwuLwSPIwjXdc59XQjWfk2dv/qeHD7xv9cRmmdkS4d3Y7A6l+XfOlAW8T2/MocfB4fztYB6q6LTVefjhJlSgzXLucf//XCkPtzih3klmYP+RghhBBivE2ZAM/qsGRmnYPJuEKeSMQMXpwqmkDcE+WA0VbzDA7PZCKr0E7P5T68Hh8+t4/WE0bqkHcMfZrCG5qHt4Iw28x4R9lLSkTqbTaC8rVvW0TVshLcA6F/L2fLAF63LyNrqPb+7gQNe1o59MCZiO2r36RoPdXN2ecu8fBHt3PrfeuwZVnRdZ3BXjeO/MQzIWJop59u5PD/RR7vymXFweAunngB9rp3LKa0vgiTyZidkn8TgzlBlcuRFF2p3VDJuecvR2wrnitZDUIIIdJvygR4mTJcmpyrxx1MGw2uwRtBat1YAzwIpWqdf6EporF5TknyPfBihK0DK5wVarRusZnxDTGjKZKj6zrtZ3vIKXFQtawEMNaGXtjRFCzF3tPUF1EZNV2aDkeW5c+vzGGLvy1HaX0RZ5+7BMCRB89yNuyEN3y2SSRH13X2//FUTOCQXeRgxavrEjzLEF2gac6GyoiLSRLchZgTVKZNdLHK2TpAV0MohXr+5pnUXV8d8+80c2Xp+A1SCCGESNKUKbKSKfEqBlYuC51E7fqlFrzti1NkZTjjkeaW40+ZPPCnU7zwwyMAzJiTT82a8lG/5rxrZwKw6g31EbNIZqt5VIUJBPS1DTDQbXyefB4f7j5PRHqX1WFh091XsOyV8wBoO9WN1+1j58+OsfuXGme3XUp5ARaf1xcz27Hghurg7fBKsGejTnYP/OkUDXtbUjq+qUT36Tx097aYoAHA4jCPKEBb+op5XDFMQDidJZrB0xN8lz3++V3s/GmoGmnd9dXYc2Kvl4aviRRCCCHSRX59UiC86IqzObT2LZDuE7f1gQmIc26+6f3LxzyehbfWcurpxohttesrxlTZrWJxMS/9xoaYtSsWm1mqaIZxtvaTVWDHYrcw6HRjzbIkXO/zr8/tAqCgKofFN8wGwGKPfWyhf9bu0F+N4jmB9ZoXd7eQV55N6YLkW1+M1MknGxh0epi5spTVb1L0d7jIKY6cCb7hk6t5/N5dMc/tudzHrp9ruPu9UngiCQ1hDbQBVrx2Aft+fwL05AsaOQrsuLoHmbVCZpKGkui7MGG6edhmi90cs/4xfJ8QQgiRbhLgpUD4ujswrsRv//7hYP+5eOlAG96zlOe/eyhi2+o3L6RgZm7MY0cq3iygxTb2mcF4gYrM4IU42wZ4/N7dlC8souaqCnb/QmNGbX6wpH248IqZ3Zf6eOFXxuxAvH6MjvzQLFkgyAsIX6eXCl0XjZ5qi2+vxWQyxQR3ALmlWVjs5mDxoeK5+cG0UoD9fziJZ8BD9epyLDazzHIkcPlge/D2uncspmJxMdWry2jVOilfNCOp17jm7uUM9AySVSjpmENJlDYfXiQrwNUbteZ6iJT70VYpFkIIIcZCLi+O0fw4Veii180NdA3ScrwzWKY8XoAXf23S+KXb5ZZGnoin6sqyxWZG9+rDVp+bDvraBgBoPtbJ7l8Yqbod52JL1es+nZ0/Oxr3NSxxgvOhAiLtkfOjGWrS+jtdmMzxA7tw6mZjBrKwOpdNd1/Bdf+9innXhCrcHn7wLP/85I5hqxBOZy2asdbxhk+uDq6ds1jNVCwpTnr2Packi+I5UugjGYtfOicmu8Lp/38YoL/LxbbvHeTyofaIx5iG+Cq1SiN5IYQQGSC/PmNUUJUTsy16bVv0mjt3X2zVzbhXkMcxRlrzHwsj7oevlRpPgeB1PIrDTHaeOFVKzf4TPq/Hx46fHKXpcDt/+9A2+tpcQOR6NoDeptiecrbs2KDvxs+sAYwG2E98YXfEPveAh50/O8aD73+O3b/WRr1Oz+f10dvcT1aRfdgAY8H11dz4mTVs/shKAAoqc1j2yvnc8KnY1g69YWnM6XLpYBvHH7uAeyC2j9lE4XX7yC52xFycEamx4IZqlkRVYt77mxPB2yefaKD1eBf7fnci4jFDFc2SFE0hhBCZILlRY5RXHiqCMXdTFfmVOcy9uorCWbnBJtB97QMRzylTyaVXjecsWGFUtcX8OIHpeAgUXPF5fDDJ+6AFAqHRNhQfjE7lwvg37e9y0Xy0g0sH2mJ6lC1+6RzmbqrisU/vBOKnXJotZla/eSHdjU56mvq48g31Ealg0QHT0YfPhdbp7WxhyR1zE64ZSqS/w8VjnzHGFO+iRjzx+iHmlmRRVJMXkZLadror4v+jVDv0wOngmtTGA21s/vCKtL13snRdNwK8IukpmU5x10djfIeHNy8P5+5LfJFAUjSFEEJkglxeHKPwHnDL75rP3KuNNLTw/keBSpsFVTnccf/GiLYCQzElKMYxHmxZqYntAwHeVJjB2/ULjYfu3saFsIbjyXL1utn3+5Mx23WvzmOf2sm+38XuW//upYBRAn/JTbUAFMyMH0zNWlnKottqWfuWRcGTyJvuXRvc39Nk9D08t/0y3Y3OiOe2n+ke8d8TXjRl8UvnjPj54WatKsWRZ6P+xhogvTN4fe0DEQWHusICzYnE59VBJyN9DqezeDPTzrYB9v/xVFLPX/u2RRTVhi6myb+fEEKITJAZvDFKpo3ByScbAGMWbSSVK+Ol4k10gRTNkTQInqga9xqzXnt+dZya1SNrKbHjx0ditlUtL4mZsQu47SvrIz5Lq19dj+4wMXdTVdzHx5NVYCe3PBtncz9PfnEP866dGZp1MMGyV8zj4F9Oc2FHMzOvSK6qYl/7AAf/ejqicNBYmzfXbammbks1rl43xx+7kNYA79RTDRH37bkT8yuw+Yix/k7SM9OrYU9rzLbLh9rxupMrXlS1rISqZSVs/8Fhui72jqlSsRBCCDFacnlxjBL1TwIomW+cCAeKqwy1GD9a7YZKyurHt9z9Sz69moKqHFa8JnX9sNz9RrrS4/fu5twLsf270u3gX07xxBd2jznd9Ykv7o5JtU1E9+kRVSMDlr9qftzHL7xldsyFAluWlQXXV8etojmUwrCqqxEpZTrMu2YmeRXZtBzvDB6PtlNd9He4Il7DM+ilq6EXn1dn96+OR1RzDHymx4M914o9zxaszJkOHpdx4WH5q+ZTsWQGg05PsBjORNJyvBOAmrUVGR7J9DJrVZwLH7qOuz82wKtYnDjVfv07l3Dz568az6EJIYQQSZuYl68nkXgVMQNcPZFrsIa7mltWX0TL8U7MVhMrUtCUOKc4i+v+a9W4v2648Bmqfb87Se269PQ72//Hk5zddpkbP7eG7EJj3ZKr183pZy4BMNjnGVFhmQff/1zE/d6mfnb+/BjXfjC0XkvXdQa6BmPWSYUHgkteNpeZV5Qw6PSQVWBn7dsW0d/honhuAY48G9Zsy7imy6583QLaTnfh6o787K192yIAZszOp7epn64GJw17Wzn5xEUAXvr1DcHP8s6fHqP5aAd118+i/XRkOueVb1TjNlaTyUTxnHwuH2qnv9OVlvVmgWC2Zm05uk+n6XAHbae7ySnJwtk+gO7TJ8SsS6AUf/YMaW+QTmUq9qLaiScacHUPxmxf+vJ5NB3ZHbNdCCGEyDQJ8MZoqAAvpkLfMMU6ataW03K8M+FMz2Rgtpgi0vkGne6IdYqpcnabMVuoPXKeFa9ZAESur3L1upMO8LyD8dOxwoutDHQP8tind6D7jKv14X3JwgP7On8bjRyjyj1Vy0qSGsNoWR0Wrr/nymD7gYrFM1j3jiXB/eWLZnBhZzNbv7Yv4nnnXmjC1eNmzsZKmo8a6YEnn4hMZ4T4hVPGYkatEeB1XexNOsDr73LhdflGXJhlsM9jzIyZjP6Cgfdz9brpbe7nwS88h8kMy14ZWksb7eKuZhoPtDH36ipMZhOldYV4XF7MVnPCAh3J8g56aT/XQ8m8AjrO9BgNtPMlwEsnW07sT2K84A7Anmdjw3uWQuavBwghhBARJMAbo6FO6ta+ZVGwkiYMG99RvbqM8oVFOCbxSd2K1y5gz6+PB++f2XaZ+huq8Qx44548jYfwxsOBFFEIrX0EGOwZhMrhqz+6Bzw8fm/8q/LhAd4LPzyM7l9m2HI8svF0f6cxS7TslfOS+wPGmS3birq5Bu3RC9TfNDtiX6K0sgN/MopIaI8m7qPnGGHlzWRkFxtBVn9n/JPoeJ771kH62gYomV/A3E0zmbmiJGGlU2frAE9+aTfL75pPn3/2LpD2Glh/d/yfob9Z9xnHomLxjJhef862AXb/yvhsX9pvzFSveE0d+35/kuwZDgqqclh4Wy1FURVrk+Hz+Pj7R7ZHbJu7qWpCzCZOJ8nOps+Yk489xzruafRCCCHEeJAAbxxsuWdV3ObT0cUohjtZM5lMkzq4A2IqhHr6PTz0gW0AXPexlRTMTK6C6Ehc2NEUvK2H1XYJrGMC8A7GL/oy0D3I3t+eYOnL55JfkUPbqe6I9gazryrn/ItGFc3+DldM6iaEgpSAQLCSVZi5f0t182xq11fGzIoN1SQ93KLbajn68DkAFt8xh0v721h8x5zxHmbwGCWaJYknsGau7VQ3bae6qb+xhrmbqnOxiEUAABiiSURBVGJaPwz2eYLVP8Mrmq54rTHDa80yAj13v5fD/3cm4rmeOJ+Xxz+3K2Zb4HX7O1z0d7hoOtLBHfdvHHFgdvKp2NnSui2zRvQaIvVu+ORqei47Iy7oCCGEEBONFFkZB/kVOUn1FZsOV+Oji5n0XA416t7x06Mpec/DD54N3k7UuDo8bTTc8ccu0Hy0gxf/16h6GfgvwKo31rPydfVs/oix7i4wMxft9DOX8Hp86D6dU1sb6TxvFFjJZA8zk8k07PtXLivm+k9cGbFtwUuqmbOxkrrrq4NNmuddO5NrPngFpXWF4z7OQMAZr99fPJ1x2hocf+wC//zkjojZW4BTT8cGTQClC4y/o6Aq8cWG8y800XKiM2JbsjPQfe3xPyfxDHQN8uD7n+Po389FbDdbTTEziCI9KpYYwduqN9TH7MstzaJyaQnmFLawEUIIIcZKZvDSyJY1+doejJQpKmW1yV/uHcDZkpoiFjPm5AcrlSZqOuzzxp/BC/SpcrbGVlIMtEaIbhIfzdncz86fHGXmylIO/fV0cHtW0cScjb3mQ1fg8+iUzIucYa5YPIPFt88J3r/9qxtSPhaz/7OQbJXTI387a9wwwYZ3L+X57x0K7tv1Cw3dp1O9uozZayuwx5mtXPGauuBaTJPZxG1fWc/prY2c39GEs3WAmVeU0LivjVNPNUS0VMivzMHd5yG/MofV/7GQxr0taI9eCO4vrMmjdH4Bp55u5PF7dzH/ulksuXNOwtTRgKYj7THbZszJl9m7DLrqbYvBxISsriqEEEIkQy5Dpkn1mjLmXTsz08NIuYKqXK74t/lsuSd+tc6BJFLxnG0DnH62EV0f+qS/7VQXDXtbgrOnjnwbzraB4PPCUycTzeCFr9PrvBiaHbrhU6sjHle5rDjmueHFcJqOdNBzObKf20QtkDFjdn5EcLfp7uVYbGYW3lab9rGYRhjgeVzGTN+q19dTVl9E9eqy4L7mox20aJ3s/c0JuhudHPKnXYbPxES3o7A6LNTfWMP191zJ6/5nC3VbquO+b2Amump5CQWVOcEm7QA3fPJKNn94RURK9qmnGriws3nIv+XoP85FpI4C3HH/Rq75wBVJ9ykU489kNhnp8ilYcyqEEEKkQ1pn8JRS/w3cAdiB/wG2Aj8HdOAQ8B5N0yZ/h+w4lt81f1zL4U9kczYaFQhXvm4Be397ImLfY5/eyZ3funrI5z/5pT343D46zvYMWZb/uW8fBEK92Qpm5dJyrJMnv7SHDe9eSn9YqpzPM3wAsfWr+/zjryS3JDI9ri9shq/+xhp6LvdRu64iWJwECLYcCJgsKbnFcwu4/Wupn62LZzQBnjXbQs0aY3Z11RvqKZyVG5GmC/DUfXuDt2deUUL5oqu4tL+VmSviB04ms4mcQgczavNZdHttTMpkQKC6ptli5toPr8CWbQ02Iy9fNIPsYkfwc7f3NyeYPUQfu+P/vBBxv6SuYNJ8ZqaD6B6UddfLrKoQQojJIW0Rh1JqM7AB2AjkAB8GvgF8QtO0p5VSPwDuBB5I15jSaTqeuFnDUlKziuwMJFkp0ec2YvyLu1oSBnjhAUHbKaNXm92/Rqq3qZ+9vzMCS3uulUGnh32/O0H5wqKIdWmJggprnEC8v8sYe2FNHovCZro2feAKnr1/f8Rjb//q+phUVRHfSAK8nqY+ei71RWwzmUzUbammt7mfc9ubYp5TUleAxW7BYrcELzwMp/4lNRRU5bLjJ0dZdHsts6+qoHFvK4U1eRGFc4pqIlN3rQ4L1//3KpqPdbLjJ8Z60wff/xzr3rGYrgYntesr47bqmLmiBHXT7JhiPWLiWPOWhTKrKoQQYtJIZ4rmTcBBjADub8DfgSsxZvEAHgFuSON40iJQmGGsPbImo/AZyxmz84O3A9UZh+MoSNy3bqArNlgMX+/UcswokFHtX0cHxuwhwNNf3cue3xyPKcoRYLXH/m+R72+xMHdjZOP24jn53PiZNcH7N927FovdIkUYkmTyHyZfEgFei9aZcN/yV83nyjfWc+uX1gW35VflsP6dS0c1rsqlxdz8hatYcH01jjwbczdVUTwnf9jnWewWqpZH9jp84YdHOPr3c7zww8MAnN/RROsJox9fbmkWK19XT8HM3Gkzwz8ZJVt9VgghhJgI0vmrVQrUArcDc4GHALOmaYEzux4gqTJ9ZWXDn2hNFK/9znW4et3kl42sKfNEM6pj3h2qjDizvpj20924et0cf+wC17w58Yl3dqGd/q5BfG6dkhm5cZvJt3THVl2sXlzKxd0tEdvKZxdwmsbgfXOfTtdFJ10XndSvN9ZEzqjJp+NCT/AxVfNnxPy9N394Ned3N6G21MQWzijLZ90bFzLY76Fm/vg1Mp9Mn/PRclqNIN5hswb/XveAh/4uFx0XeymoyKGwKpfWM91k2Y3HXvP/lsU9NhW3RH59+Fw+KmeOrPJnKo955/lePK2D7P1NKG25sn4GVdXTu5faZPicF5fkTopxJmsq/S2ThRzz9JNjnn5yzCeOdAZ4bcAxTdMGAU0pNQDUhO3PBxJfog/T0tIz/IMmmIFJOOaAsrL8UR3znv7QGrjyK0uwP9cQbEo+1OsN+B/j7vdw8Ux73HL/TRe7YraVryrmCtd89v8xtC5uEB8Lb5nNsUeMZtYP3LMtuO+Jbxlr7vJn5UQEePYKR9zxlSyfQWtrbJl+gIrVpcP+XSMx2mM+2bh6jJnYU89f4tTzlyitK6S/y4WzJbaCYaDohS9r6ONcd301J5+4yMLbZo/oGI7nMd/43qVs/8FhSuYVRvRjfPjeHRGPy6nKnhb/zolMls95V3c/lkkwzmRMlmM+lcgxTz855uknxzz9hgqo0xngPQe8Xyn1DaAKyAWeUEpt1jTtaeAW4Kk0jkekWPgaPHuOlaLZecFqhInaJfi8PvSwipfuPk/cAG+g2x1xf/7mmZjMJuZsrMLr9nHoAaOCoiPXxszlJcEAjziZgHnl2Sx52VwsNjMFVcn1NBTjwxJVyKL1ZGzgHhBohm6Ps44t3JI75rAkBU3ZR6J0QREv/fpGwKgce/Cvp2nc2xrzuIolsdVZxQQ0/TLshRBCTGJpWyikadrfgb3ADow1eO8BPgR8Vim1HaOy5p/TNR6RetFripbfFWor8PDHtsdtSu51RRZRjdefDkIn+wBFtXksffm84P1514TaUdjzbFjsFm798joSsWVbqbtuFnOvrqJk/vg38xaJWR0WSuoKhn9gGHvu0AHeRJNVYKd6VaidQ1FtHle+sZ6N/7ksplqrmJgstqnfw1QIIcTUkdaV45qmfTTO5mvTOQaRPpaoYiVWh4W662dx8okGvIM+uhucMQGVZzBybd2Onxxly3+vChY5AfC6fTQeaAOMKpb5lZHrG01mE2abGZ/bhyPfCAZs2VbWvnVRsLrhlntW8eQX9wBgtsrl+Uwqqsmn7WR3xLbsGQ5yih3BCqnhAtVSJ5Pw6pvr3r4YxwTtkSgibbp7Oc1HOyiYmTP8g4UQQogJYvKdKYlJI6YYCZGzL+7+2EIp3kFjBs9iNwdvP/mlPdzxzY3B19v5s6N0XTDWws2YnRc31fMln1xN9yVnRHpn1fISlt01D6vDQn5FDnkV2fQ29U/KgGEq6Txv5OybraZgv0J7rpWr37ecI387i8ls4vhjRs+4JS+bOylbjhTMzKVMFVFUkyfB3SRSPLcgooG9EEIIMRnIma1IqZs/vzbihNwWti7POxgb4AVm8KqWlURUxPS6fcHGw02HO4LbE53sZxXaI2ZNAuZtCqVvbnzvMhr2tFC+aEayf45IgUDhncqlxTTuM2ZmF9xQDcDil84BoHhuPiazifKFk/PfymIzs+Hdo2vZIIQQQggxEtKsS6SUI98eMWtXc1UFtmwjUHP1ujn419P0d4aqbXpdRoAX3fTZE2e2b6yyCuzM3zxLetZlWKCojslsZtFtteSWZsUEchWLiydtcCeEEEIIkU5yZivSymI1s+I1CwA4/s8LnN7aGGwADaEUTWtUdcX2M7FrscTUECiQU7dlFvU31nDDJ1dLY2khhBBCiFGSAE+kncURmsED6G7s46n79nD4wTPs/+NJ4zFRBVp2/uxYzOsEZgLF5Fa5tJg7vrmRopq8TA9FCCGEEGLSkwBPpJ3VHvux627s4+STDfS1G+maZfVF3PKldczyl5e3+oNC3RdqZHfNh1akYbQiHeIV5BFCCCGEECMnAZ5Iu8AMXiKFNXkUzMzFnmNl9ZsUBbNyg42GfR4jhbN8YRF5ZdlDvIoQQgghhBDTjwR4Iu2i19dFi25bYLVb8Li86LqOz1+Qw2yVj64QQgghhBDR5CxZpJ3FMfTHLjrAszjMoIPPo9Pb0g9AV4MzZeMTQgghhBBispIAT6RdVoHdSLsM3I/qVxddQdFiMz6mXrePo387B0B/hwshhBBCCCFEJAnwRNqZTCau++hKyuqLyCq0c/3Hr4zY7x6I7HkXSOn0DhppmkIIIYQQQoj4THLCLIQQQgghhBBTg8zgCSGEEEIIIcQUIQGeEEIIIYQQQkwREuAJIYQQQgghxBQhAZ4QQgghhBBCTBES4AkhhBBCCCHEFCEBnhBCCCGEEEJMERLgCSGEEEIIIcQUIQGeEJOAUsqS6TEIIYQYH0opU6bHMJ0opayZHoMQ6TQpG537vxhvBg4AfZqmdSilTJqmTb4/ZpJQSpmBdwEngbOapmkZHtKU5/+cf1jTtK/675s1TfNleFhTmv9zfg+wFTigaVpXhoc05fk/5y8F9gHtmqb1ZnhIU57/mL8WOAR0app2Xn5DU8v/3fJhoBk4qGna7gwPacrzf86/rGnax/z3LZqmeTM8rCnN/zm/F9gNPKtpWkuGhzRtTboZPP+H5w/AvwOfAD6ulFoqP0yp4z/mvwLWAuuBdymlHP7tInUWAXcrpb4BIMFdavlPBn4LFAIFwKBSKjtsnxhn/u+QPwJvBO4DtmR2RFOf/7P8O+AW4DXA/UqpdZqm6fI5Tw3/5/yXwFyM75dPKaWKMjuqaaEE43zlDwCB4E4+56nhP66/ATxAN6AHPudyzNNvMp6g3wkMaJr2WuB7wBngI0qphZkd1pR2F9CtadqbgD8BcwCvBBwp1wo8C+QrpX6slKpXSlVnelBT2HVAG/Ap4D+ALwDfUkqtlgtIKfNqjO/zVwGPAi9TStUopeoyPK6p7HrAomnaG4EvAg8A98nnPKVuAtA07V3ATzFOgE0gJ74p1otxMcOslHpcKbVMKTVfPucpsxoYAL4OvB9jEubPSqnNcszTbzIGeK2AC0DTtEPA3zBSe25USllkVikl7IDTf/so4AAsAEqp3EwNahqwYJyIvR2oBJ4AyiF4RViMrzYgByM74G/AD4GdGLOoRXIilhLdgEsptQhYAyjgfcB3lFKzMjqyqasFaFZK2fzpsL8Ffg68USmVK5/zlDABF/xp9j1AFmDz7yvI3LCmLqWUDeM3NMt/AWkQeB7jt1TWtadGO0aA93rgLxgpyf8LfEEpNTOTA5uOJsVJolLKrJS6WSl1E/AcsEAp9S0ATdPOA3uBxZqmyazSOPEf81uUUtdjTLn/wL+rApilaZpLKXUX8AH/F6kYo7DP+UsBNE27BOxWSq0C8jHWnH7Yv08+5+Mg7JjfrGnafozj/G7gef860weAPoxZJrkCOQ7Cvlteomnawxjf3/8JXKtp2kbgk8ARjBMyMQ6UUial1DX+u8eAauArEPwueRzjQp58zsdJ1DF/Avippmk+pVQhMBvoUEr9G8bJrz1jA51Cwo+5pmluTdOcwDGl1GaM893tQGDJg6zFGwdRn/MLGL+hbwJaNE3zaJr2R4xzF3emxjhdTfgAz3818WHgdowTr+8BbwXWKaW+7X9YKTDT/8UpxijsmN+GcTX9N0CPf7cPI+i4C3gv8AdN0+R/3DGK+py/WSn1F6VUFbAJ40rYZzRNuw1okyth4yPqmL9TKfVD4KMY34sfV0qVY6SzLcb40RJjFPXd8h6l1M+Av2IEGCf9D7sDuAp/loAYF0uAR5RSL9M0zQW8DlitlPqGUmoOsAFjze+MDI5xqok45pqmnfB//geBgxj/Bm8HvqNpmlzMGB+BY3572LYVGOsfP6dp2o3AfqVUbUZGNzUFjvmd/s/x/8OYyXuZUmqTUur1GPUbpIppmk34AA/jBKtT07T3app2J8aP/nsxTgLKlVL/A3wQ+JhUvBs30ce8Hfi6UioHI9C7FeOH6R2app3I4DinkvBj/kqgCWMm47PALZqmPeV/3N2apjVmapBTTPgxf5l/23uBGzBSqr4AvBN4m1QCGzfRx7wPo+Lac0CtUurHGN/nb9M07XIGxznVVAMNwPeUUm/1z2zchJEq+G6Mk7J3aprWmsExTjXhx/zfgf/f3t2E2lWdARh+RRAbYm0bBzqonaifSAfSDKx/JSotEloVi5qmaLQDFX+iZiDxZxSsFgQRwVLFQak1ZNJBG8FOKh0EURD/o35q4BgQHYhGWxXF9jpYO3q4xOTc3O13ztm+zyTh5OaweNl337vO2nttutXRBWA17YPqa92Ruld7mz8QEVd0r90KXJyZTwBk5pWZ+ea0BjhAe5v/sTu3fEy3bwNwIe0+6992VySp0DxM8BL4brfETmZeRdsZ6feZuQ64CVibma9Mb4iDs7j5dbT7Hv/SffM+C2zyB1OvFje/hnav4+bMfPXLL/Kykj7t69xyFHBnt6HQVcD54/21bIubX0u7NHBLZv4EuA34pc17dwhwEbAG2BIRV3Tn8usz82bgApv3brz5XRFxaff658BTwJWZ+dqUxjZU483viIjLMnNXZj7ZXRru/aX9+7pzy82ZeRPwG88t0zGTz8HrvglvBF6jbXxwMu2X3R17nx0TEY/QPv3aM7WBDsiEzbdm5vrweUm9mLD5w7RfwjzOe+C5pd6k5xbaCtKHUxvogCxqviszX42IYzLz7Yg4hfaoobsy84GpDnRAltLcn6H9OEDznwLbgDsy86GpDnRAJjzO78zMB6c6UM3eBK87eLYDu2g3ZX5OW7F7g7bi+ArtPrDbgZ97WebyLbH5L4AP/OG0PB7n9Wxez+b19tH8cOClzPzT2NecQdsl9lTgP57Pl2eJzU+jNXejrGXwOK9n8/kyi5do/pC2+84NtHszHqPd9/UD2uU9l9Aezvo7fxnozVKa7/Ebthce5/VsXs/m9cabb6E9SH51RFwObRfTzNwBrM7MDz2f92IpzT9wctcLj/N6Np8jM7OrTbTneq0FTgBWjC35PgespH0a8E/gUeCwbM/v0TLYvJ7N69m8ns3rHaD5EcDPup2m914G++mUhjoYNq9n83o2n08zcYlmt+z7d2A37UG359A28jgvM9/qdm98GLjFm5L7YfN6Nq9n83o2rzdB8+8Af6VtfLBreiMdDpvXs3k9m8+vWVnBuwF4NzOvi4hDgbtp92o8HhEbgOOB7/HVs9i0fDavZ/N6Nq9n83qTNv94imMcGpvXs3k9m8+pWZngjYBV3ScBq4CTM/Psbvn3POBYYGP6HI0+jbB5tRE2rzbC5tVG2LzaCJtXG2HzaiNsXm2EzefSrGyysgN4MDM/oX0ysKJ7/SPaAxQ3ZObOaQ1uoGxez+b1bF7P5vVsXs/m9Wxez+ZzaibuwRsXEUfQdlt7nLY0vNGD55tl83o2r2fzejavZ/N6Nq9n83o2ny+zconmuCOBjcApwGWZ+fqUx/NtYPN6Nq9n83o2r2fzejavZ/N6Np8js3KJ5rj3gG148FSyeT2b17N5PZvXs3k9m9ezeT2bz5GZu0QTICIOy8zPpj2ObxOb17N5PZvXs3k9m9ezeT2b17P5/JjJCZ4kSZIkaelm8RJNSZIkSdJBcIInSZIkSQPhBE+SJEmSBmIWH5MgSVKpiLgc2AI8BDybmdsj4l7gnszcfRDvtxJ4FDgxM4/udbCSJO2HEzxJkpqtwG7gdGB7Zt54sG+Umf8F1kTEO30NTpKkSTjBkySpORTYDKyIiCeATcDVwDrgOOAoYBVwP/Br4ARgQ2Y+GRHXA+uBBWBbZt43hfFLkuQ9eJIkdf4H/AHYmpn/WPRvn2TmucDfgLWZ+avua9dFxEnAJcAZwJnABRERheOWJOlLruBJknRgz3R/7gFe7v7+PnA48GPgR8C/ute/DxwPZOUAJUkCV/AkSRr3f/b9s3FhP/8ngZ3AWZm5Bvgz8ELvI5MkaQJO8CRJ+sqLwPkRsW7S/5CZz9NW73ZExNO01bu3vqHxSZK0X4csLOzvQ0lJkoave0zCiZm5uef3fcfHJEiSKrmCJ0lSsz4iNvXxRhGxMiL+3cd7SZK0FK7gSZIkSdJAuIInSZIkSQPhBE+SJEmSBsIJniRJkiQNhBM8SZIkSRoIJ3iSJEmSNBBO8CRJkiRpIL4Aa/RkmCcs5Y4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.plot(stock_data.index, stock_data['Adj. Close'], color='#9b59b6')\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim([pd.to_datetime('01-01-2000'), pd.to_datetime('31-12-2017')])\n",
    "ax.set_ylabel('[stock adj. closing price]', fontsize=10)\n",
    "ax.set_ylim(50, 220)\n",
    "\n",
    "# set plot title\n",
    "plt.title('International Business Machines (IBM) - Daily Adjusted Historical Stock Closing Prices', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pls. note that we plotted the \"adjusted\" daily closing prices of the IBM stock. The stock prices are adjusted by the quandl team by several types of regular corporate actions, e.g., stock dividends, stock splits. For further details on the applied adjustments pls. refer to the following reference: https://blog.quandl.com/guide-to-stock-price-calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the obtained and validated stock market data to the local data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save retrieved data to local data directory\n",
    "stock_data.to_csv('data/ibm_data_2010_2017_daily.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will obtain daily returns of the retrieved daily adjusted closing prices. Also, we will convert the time-series of daily returns into a set of sequences $s$ of $n$ time steps respectively. The created sequences will then be used to learn a model using an LSTM based neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Daily Returns Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the daily returns of the \"International Business Machines\" (IBM) daily adjusted closing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data['RETURN'] = stock_data['Adj. Close'].pct_change()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect the obtained daily returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'International Business Machines (IBM) - Daily Historical Stock Closing Prices')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.plot(stock_data.index, stock_data['RETURN'], color='#9b59b6')\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set axis labels and limits\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim([pd.to_datetime('01-01-2000'), pd.to_datetime('31-12-2017')])\n",
    "ax.set_ylabel('[daily stock returns]', fontsize=10)\n",
    "\n",
    "# set plot title\n",
    "plt.title('International Business Machines (IBM) - Daily Historical Stock Closing Prices', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Transform Time-Series Into Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of time-steps $n$ each individual sequence $s^{i}$ should be comprised of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of timesteps\n",
    "sequence_length = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the initial return of the return time-series which is usually not applicable and therefore 'nan':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_daily_returns = stock_data['RETURN'][1:len(stock_data['RETURN'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract individual time sequences of length $n$ from the obtained daily returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over distinct normalized closing prices\n",
    "for i in range(0, stock_daily_returns.shape[0] - sequence_length):\n",
    "\n",
    "    # extract normalized closing price sequence \n",
    "    single_stock_sequence_data = stock_daily_returns[i:i + sequence_length].T\n",
    "\n",
    "    # case: initial sequence\n",
    "    if i == 0:\n",
    "\n",
    "        # convert to numpy array and collect sequence of normalized closing prices\n",
    "        stock_sequence_data = np.array(single_stock_sequence_data)\n",
    "\n",
    "    # case: non-initial sequence\n",
    "    else:\n",
    "\n",
    "        # convert to numpy array and collect sequence of normalized closing prices\n",
    "        stock_sequence_data  = np.vstack((stock_sequence_data , np.array(single_stock_sequence_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the top five collected normalized daily closing prices sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03396552,  0.03515974, -0.01724138, -0.00438596,  0.03964758],\n",
       "       [ 0.03515974, -0.01724138, -0.00438596,  0.03964758,  0.00847458],\n",
       "       [-0.01724138, -0.00438596,  0.03964758,  0.00847458,  0.00420168],\n",
       "       [-0.00438596,  0.03964758,  0.00847458,  0.00420168, -0.01046025],\n",
       "       [ 0.03964758,  0.00847458,  0.00420168, -0.01046025,  0.01158562]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_sequence_data[0:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Prepare Sequences for Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set split fraction to split return sequences into training (in-sample) and validation (out-of-sample) sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.9\n",
    "split_row = int(stock_sequence_data.shape[0] * split_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split obtained return sequences into training (in-sample) sequences $s^{i}_{train}$ and validation (out-of-sample) sequences $s^{i}_{valid}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = stock_sequence_data[:split_row,]\n",
    "valid_sequences = stock_sequence_data[split_row:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine count (shape) of adjusted daily return train sequences $s^{i}_{train}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4068, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine count (shape) of adjusted daily return train sequences $s^{i}_{valid}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(453, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate each training sequence $s^{i}$ into time-steps of input returns denoted by $s^{i}_{train, input}=\\{r_{t-n-1}, ..., r_{t-1}, r_{t}\\}$ and the time-step of the to be predicted target return denoted by $s^{i}_{train, target}=r_{t+1}$. In addition, we convert both the input returns as well as the target returns to PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences_input = torch.from_numpy(train_sequences[:, :-1]).float()\n",
    "train_sequences_target = torch.from_numpy(train_sequences[:, 1:]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate each validation sequence $s^{i}$ into time-steps of input returns denoted by $s^{i}_{valid, input}=\\{r_{t-n-1}, ..., r_{t-1}, r_{t}\\}$ and the time-step of the to be predicted target return denoted by $s^{i}_{valid, target}=r_{t+1}$. In addition, we convert both the input returns as well as the target returns to PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sequences_input = torch.from_numpy(valid_sequences[:, :-1]).float()\n",
    "valid_sequences_target = torch.from_numpy(valid_sequences[:, 1:]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an LSTM neural network, we tailor the dataset class provided by the PyTorch library. We overwrite the individual functions of the dataset class. So that our dataset will supply the neural network with the individual training sequences $s^{i}_{train, input}$ and corresponding targets $s^{i}_{train, target}$ throughout the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define daily returns dataset\n",
    "class DailyReturnsDataset(data.Dataset):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self, sequences, targets):\n",
    "\n",
    "        # init sequences and corresponding targets\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    # define the length method \n",
    "    def __len__(self):\n",
    "\n",
    "        # returns the number of samples\n",
    "        return len(self.targets)\n",
    "\n",
    "    # define the get item method\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # determine single sequence and corresponding target\n",
    "        sequence = self.sequences[index, :]\n",
    "        target = self.targets[index, :]\n",
    "\n",
    "        # return sequences and target\n",
    "        return sequence, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have specified the daily returns dataset class we instantiate it using the new daily closing dataset using the prepared training input sequences $s^{i}_{train, input}$ and corresponding targets $s^{i}_{train, target}$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DailyReturnsDataset(train_sequences_input, train_sequences_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works by getting the 10th sequence and its corresponding targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0324, -0.0042,  0.0210,  0.0000]),\n",
       " tensor([-0.0042,  0.0210,  0.0000, -0.0196]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural Network Implementation and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement the LSTM architecture of the to be learned time series model. Furthermore, we will specify the loss-function, learning-rate and optimization technique used in the network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Implementation of the LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will implement the architecture of the LSTM neural network we want to utilize in order to predict future returns of our financial time series data. The neural network, which we name **'LSTM_NN'** consists in total of three layers. The first two layers correspond to LSTM cells while the third layer corresponds to a fully-connected linear layer. The individual gates of the LSTM cell are calculated according to the following functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "$i=σ(W_{ii}x+b_{ii}+W_{hi}h+b_{hi}), \\\\\n",
    "f=σ(W_{if}x+b_{if}+W_{hf}h+b_{hf}), \\\\\n",
    "g=tanh(W_{ig}x+b_{ig}+W_{hg}h+b_{hg}), \\\\\n",
    "o=σ(W_{io}x+b_{io}+W_{ho}h+b_{ho}), \\\\\n",
    "c′=f∗c+i∗g, \\\\\n",
    "h′=o∗tanh(c′).$ \n",
    "</center>\n",
    "\n",
    "(Source: https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the two LSTM cells exhibits a hidden state consisting of 51 dimensions. The third linear layer squeezes the 51 hidden state dimensions of the second (last) LSTM cell into a one-dimensional output signal. This output signal refers to the by the neural network predicted return of the next timestep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the LSTMNet network architecture\n",
    "class LSTMNet(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        # define lstm nn architecture\n",
    "        self.lstm1 = nn.LSTMCell(1, 51)  # first lstm layer\n",
    "        self.lstm2 = nn.LSTMCell(51, 51)  # second lstm layer\n",
    "        self.linear = nn.Linear(51, 1)  # final linear layer\n",
    "\n",
    "    # define network forward pass\n",
    "    def forward(self, input):\n",
    "\n",
    "        # init predictions\n",
    "        predictions = []\n",
    "\n",
    "        # init the lstm hidden states\n",
    "        h_t = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "\n",
    "        # init the lstm cell states\n",
    "        c_t = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "        \n",
    "        # iterate over distinct time steps\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "\n",
    "            # propagate through time step data\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            prediction = self.linear(h_t2)\n",
    "            \n",
    "            # collect predictions\n",
    "            predictions += [prediction]\n",
    "\n",
    "        # stack predictions\n",
    "        predictions = torch.stack(predictions, 1).squeeze(2)\n",
    "\n",
    "        # return predictions\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have implemented our first LSTM neural network we are ready to instantiate a model to be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is initialized, we can visualize the model structure and review the implemented network architecture by execution of the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] MNISTNet architecture:\n",
      "\n",
      "LSTMNet(\n",
      "  (lstm1): LSTMCell(1, 51)\n",
      "  (lstm2): LSTMCell(51, 51)\n",
      "  (linear): Linear(in_features=51, out_features=1, bias=True)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "print('[LOG] MNISTNet architecture:\\n\\n{}\\n'.format(lstm_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like intended? Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Definition of the Training Loss Function and Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now good to train the network. However, prior to starting the training, we need to define an appropriate loss function. Remember, we aim to train our model to learn a set of model parameters $\\theta$ that minimize the prediction error of the true return $r_{t+1}$ and the by the model predicted return $\\hat{r}_{t+1}$ at a given time-step $t+1$ of sequence $s^{i}$. In other words, for a given sequence of historic returns we aim to learn a function $f_\\theta$ that is capable to predicts the return of the next timestep as faithfully as possible, as expressed by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $\\hat{r}_{t+1} = f_\\theta(r_{t}, r_{t-1}, ..., r_{t-n})$. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereby, the training objective is to learn a set of optimal model parameters $\\theta^*$ that optimize $\\min_{\\theta} \\|r_{t+1} - f_\\theta(r_{t}, r_{t-1}, ..., r_{t-n})\\|$ over all time-steps $t$ contained in the set of training sequences $s_{train}$. To achieve this optimization objective, one typically minimizes a loss function $\\mathcal{L_{\\theta}}$ while training the neural network. In this lab we use the **'Mean Squared Error (MSE)'** loss, as denoted by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $\\mathcal{L}^{MSE}_{\\theta} (r_{t+1}, \\hat{r}_{t+1}) = \\frac{1}{N} \\sum_{i=1}^N \\| r_{t+1} - \\hat{r}_{t+1}\\|^{2}$, </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the training process, the PyTorch library will automatically calculate the loss magnitude, compute the gradient, and update the parameters $\\theta$ of the LSTM neural network. We will use the **\"Adaptive Moment Estimation Optimization\" (ADAM)** technique to optimize the network parameters. Furthermore, we specify a constant learning rate of $l = 1e-06$. For each training step, the optimizer will update the model parameters $\\theta$ values according to the degree of prediction error (the MSE loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-06 # set constant learning rate\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate) # define optimization technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully implemented and defined the three ANN building blocks let's take some time to review the `LSTMNet` model definition as well as the `MSE loss` function. Please, read the above code and comments carefully and don't hesitate to let us know any questions you might have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training the Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train the LSTM neural network model (as implemented in the section above) using the prepared dataset of daily return sequences. Therefore, we will have a detailed look into the distinct training steps and monitor the training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Preparing the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now start to learn a model by training the NN for **2,000 epochs** in mini-batches of the size of **128  sequences** per batch. This implies that the whole dataset will be fed to the NN 2'000 times in chunks of 128 sequences yielding to **32 mini-batches** (4'068 training sequences / 128 sequences per mini-batch) per epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the training parameters\n",
    "num_epochs = 2000 # number of training epochs\n",
    "mini_batch_size = 128 # size of the mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, lets specify and instantiate a corresponding PyTorch data loader that feeds the image tensors to our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = dataloader.DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Running the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start training the model. The training procedure of each mini-batch is performed as follows: \n",
    "\n",
    ">1. do a forward pass through the LSTMNet network, \n",
    ">2. compute the mean-squared prediction error $\\mathcal{L}^{MSE}_{\\theta} (r_{t+1}, \\hat{r}_{t+1}) = \\frac{1}{N} \\sum_{i=1}^N \\| r_{t+1} - \\hat{r}_{t+1}\\|^{2}$, \n",
    ">3. do a backward pass through the LSTMNet network, and \n",
    ">4. update the parameters of the network $f_\\theta(\\cdot)$.\n",
    "\n",
    "To ensure learning while training the LSTM model we will monitor whether the loss decreases with progressing training. Therefore, we obtain and evaluate the mean prediction performance over all mini-batches in each training epoch. Based on this evaluation we can conclude on the training progress and whether the loss is converging (indicating that the model might not improve any further).\n",
    "\n",
    "The following elements of the network training code below should be given particular attention:\n",
    " \n",
    ">- `loss.backward()` computes the gradients based on the magnitude of the reconstruction loss,\n",
    ">- `optimizer.step()` updates the network parameters based on the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:48:55] epoch: 0 train-loss: 0.0036956439653295092\n",
      "[LOG 20191027-17:48:55] epoch: 1 train-loss: 0.0036453486827667803\n",
      "[LOG 20191027-17:48:56] epoch: 2 train-loss: 0.0035876172623829916\n",
      "[LOG 20191027-17:48:56] epoch: 3 train-loss: 0.0035272282257210463\n",
      "[LOG 20191027-17:48:56] epoch: 4 train-loss: 0.003466034075245261\n",
      "[LOG 20191027-17:48:56] epoch: 5 train-loss: 0.003404784918529913\n",
      "[LOG 20191027-17:48:57] epoch: 6 train-loss: 0.003343850803503301\n",
      "[LOG 20191027-17:48:57] epoch: 7 train-loss: 0.0032834291487233713\n",
      "[LOG 20191027-17:48:57] epoch: 8 train-loss: 0.003223641040676739\n",
      "[LOG 20191027-17:48:57] epoch: 9 train-loss: 0.003164556903357152\n",
      "[LOG 20191027-17:48:58] epoch: 10 train-loss: 0.003106229138211347\n",
      "[LOG 20191027-17:48:58] epoch: 10 new best train-loss: 0.003106229138211347 found\n",
      "[LOG 20191027-17:48:58] epoch: 11 train-loss: 0.003048688915441744\n",
      "[LOG 20191027-17:48:58] epoch: 12 train-loss: 0.0029919592416263185\n",
      "[LOG 20191027-17:48:58] epoch: 13 train-loss: 0.0029360501503106207\n",
      "[LOG 20191027-17:48:59] epoch: 14 train-loss: 0.0028809713330701925\n",
      "[LOG 20191027-17:48:59] epoch: 15 train-loss: 0.0028267292291275226\n",
      "[LOG 20191027-17:48:59] epoch: 16 train-loss: 0.0027733197130146436\n",
      "[LOG 20191027-17:48:59] epoch: 17 train-loss: 0.0027207464881939813\n",
      "[LOG 20191027-17:49:00] epoch: 18 train-loss: 0.0026690028025768697\n",
      "[LOG 20191027-17:49:00] epoch: 19 train-loss: 0.002618085898575373\n",
      "[LOG 20191027-17:49:00] epoch: 20 train-loss: 0.0025679924146970734\n",
      "[LOG 20191027-17:49:00] epoch: 20 new best train-loss: 0.0025679924146970734 found\n",
      "[LOG 20191027-17:49:01] epoch: 21 train-loss: 0.0025187141145579517\n",
      "[LOG 20191027-17:49:01] epoch: 22 train-loss: 0.002470246748998761\n",
      "[LOG 20191027-17:49:01] epoch: 23 train-loss: 0.002422580888378434\n",
      "[LOG 20191027-17:49:01] epoch: 24 train-loss: 0.0023757115923217498\n",
      "[LOG 20191027-17:49:02] epoch: 25 train-loss: 0.002329630369786173\n",
      "[LOG 20191027-17:49:02] epoch: 26 train-loss: 0.0022843296319479123\n",
      "[LOG 20191027-17:49:02] epoch: 27 train-loss: 0.002239802859548945\n",
      "[LOG 20191027-17:49:02] epoch: 28 train-loss: 0.0021960396952636074\n",
      "[LOG 20191027-17:49:03] epoch: 29 train-loss: 0.00215303443474113\n",
      "[LOG 20191027-17:49:03] epoch: 30 train-loss: 0.002110777742927894\n",
      "[LOG 20191027-17:49:03] epoch: 30 new best train-loss: 0.002110777742927894 found\n",
      "[LOG 20191027-17:49:03] epoch: 31 train-loss: 0.002069262041914044\n",
      "[LOG 20191027-17:49:03] epoch: 32 train-loss: 0.0020284798556531314\n",
      "[LOG 20191027-17:49:04] epoch: 33 train-loss: 0.0019884222238033544\n",
      "[LOG 20191027-17:49:04] epoch: 34 train-loss: 0.0019490830673021264\n",
      "[LOG 20191027-17:49:04] epoch: 35 train-loss: 0.0019104534876532853\n",
      "[LOG 20191027-17:49:04] epoch: 36 train-loss: 0.0018725272748270072\n",
      "[LOG 20191027-17:49:05] epoch: 37 train-loss: 0.0018352972838329151\n",
      "[LOG 20191027-17:49:05] epoch: 38 train-loss: 0.001798755820345832\n",
      "[LOG 20191027-17:49:05] epoch: 39 train-loss: 0.0017628970454097725\n",
      "[LOG 20191027-17:49:06] epoch: 40 train-loss: 0.001727713108266471\n",
      "[LOG 20191027-17:49:06] epoch: 40 new best train-loss: 0.001727713108266471 found\n",
      "[LOG 20191027-17:49:06] epoch: 41 train-loss: 0.0016931978425418492\n",
      "[LOG 20191027-17:49:06] epoch: 42 train-loss: 0.0016593442669545766\n",
      "[LOG 20191027-17:49:06] epoch: 43 train-loss: 0.0016261457712971605\n",
      "[LOG 20191027-17:49:07] epoch: 44 train-loss: 0.0015935969204292633\n",
      "[LOG 20191027-17:49:07] epoch: 45 train-loss: 0.0015616900491295382\n",
      "[LOG 20191027-17:49:07] epoch: 46 train-loss: 0.0015304184416891076\n",
      "[LOG 20191027-17:49:07] epoch: 47 train-loss: 0.0014997772341303062\n",
      "[LOG 20191027-17:49:08] epoch: 48 train-loss: 0.001469759674364468\n",
      "[LOG 20191027-17:49:08] epoch: 49 train-loss: 0.0014403589739231393\n",
      "[LOG 20191027-17:49:08] epoch: 50 train-loss: 0.001411570017808117\n",
      "[LOG 20191027-17:49:08] epoch: 50 new best train-loss: 0.001411570017808117 found\n",
      "[LOG 20191027-17:49:08] epoch: 51 train-loss: 0.0013833864395564888\n",
      "[LOG 20191027-17:49:09] epoch: 52 train-loss: 0.0013558024584199302\n",
      "[LOG 20191027-17:49:09] epoch: 53 train-loss: 0.0013288119735079817\n",
      "[LOG 20191027-17:49:09] epoch: 54 train-loss: 0.0013024095969740301\n",
      "[LOG 20191027-17:49:09] epoch: 55 train-loss: 0.0012765900864906143\n",
      "[LOG 20191027-17:49:10] epoch: 56 train-loss: 0.0012513469973782776\n",
      "[LOG 20191027-17:49:10] epoch: 57 train-loss: 0.0012266752873983933\n",
      "[LOG 20191027-17:49:10] epoch: 58 train-loss: 0.0012025703144900035\n",
      "[LOG 20191027-17:49:10] epoch: 59 train-loss: 0.0011790254775405629\n",
      "[LOG 20191027-17:49:11] epoch: 60 train-loss: 0.0011560364546312485\n",
      "[LOG 20191027-17:49:11] epoch: 60 new best train-loss: 0.0011560364546312485 found\n",
      "[LOG 20191027-17:49:11] epoch: 61 train-loss: 0.001133598492742749\n",
      "[LOG 20191027-17:49:11] epoch: 62 train-loss: 0.0011117050107714022\n",
      "[LOG 20191027-17:49:12] epoch: 63 train-loss: 0.0010903520396823296\n",
      "[LOG 20191027-17:49:12] epoch: 64 train-loss: 0.0010695346809370676\n",
      "[LOG 20191027-17:49:12] epoch: 65 train-loss: 0.0010492474411876174\n",
      "[LOG 20191027-17:49:12] epoch: 66 train-loss: 0.0010294860057911137\n",
      "[LOG 20191027-17:49:13] epoch: 67 train-loss: 0.0010102454161824426\n",
      "[LOG 20191027-17:49:13] epoch: 68 train-loss: 0.0009915211121551692\n",
      "[LOG 20191027-17:49:13] epoch: 69 train-loss: 0.0009733081569720525\n",
      "[LOG 20191027-17:49:13] epoch: 70 train-loss: 0.0009556021432217676\n",
      "[LOG 20191027-17:49:13] epoch: 70 new best train-loss: 0.0009556021432217676 found\n",
      "[LOG 20191027-17:49:14] epoch: 71 train-loss: 0.0009383991837239591\n",
      "[LOG 20191027-17:49:14] epoch: 72 train-loss: 0.0009216933540301397\n",
      "[LOG 20191027-17:49:14] epoch: 73 train-loss: 0.0009054814363480546\n",
      "[LOG 20191027-17:49:14] epoch: 74 train-loss: 0.0008897589905245695\n",
      "[LOG 20191027-17:49:15] epoch: 75 train-loss: 0.0008745209997869097\n",
      "[LOG 20191027-17:49:15] epoch: 76 train-loss: 0.0008597638516221195\n",
      "[LOG 20191027-17:49:15] epoch: 77 train-loss: 0.00084548306040233\n",
      "[LOG 20191027-17:49:15] epoch: 78 train-loss: 0.0008316747498611221\n",
      "[LOG 20191027-17:49:16] epoch: 79 train-loss: 0.0008183339396055089\n",
      "[LOG 20191027-17:49:16] epoch: 80 train-loss: 0.0008054575373535044\n",
      "[LOG 20191027-17:49:16] epoch: 80 new best train-loss: 0.0008054575373535044 found\n",
      "[LOG 20191027-17:49:16] epoch: 81 train-loss: 0.0007930414449219825\n",
      "[LOG 20191027-17:49:17] epoch: 82 train-loss: 0.0007810806164343376\n",
      "[LOG 20191027-17:49:17] epoch: 83 train-loss: 0.0007695719104958698\n",
      "[LOG 20191027-17:49:17] epoch: 84 train-loss: 0.000758511521780747\n",
      "[LOG 20191027-17:49:17] epoch: 85 train-loss: 0.0007478948718926404\n",
      "[LOG 20191027-17:49:18] epoch: 86 train-loss: 0.0007377179608738516\n",
      "[LOG 20191027-17:49:18] epoch: 87 train-loss: 0.000727978176655597\n",
      "[LOG 20191027-17:49:18] epoch: 88 train-loss: 0.0007186707262007985\n",
      "[LOG 20191027-17:49:18] epoch: 89 train-loss: 0.0007097915931808529\n",
      "[LOG 20191027-17:49:19] epoch: 90 train-loss: 0.000701337064128893\n",
      "[LOG 20191027-17:49:19] epoch: 90 new best train-loss: 0.000701337064128893 found\n",
      "[LOG 20191027-17:49:19] epoch: 91 train-loss: 0.0006933043396202265\n",
      "[LOG 20191027-17:49:19] epoch: 92 train-loss: 0.00068568836832128\n",
      "[LOG 20191027-17:49:19] epoch: 93 train-loss: 0.0006784857350794482\n",
      "[LOG 20191027-17:49:20] epoch: 94 train-loss: 0.0006716929501635605\n",
      "[LOG 20191027-17:49:20] epoch: 95 train-loss: 0.0006653061473116395\n",
      "[LOG 20191027-17:49:20] epoch: 96 train-loss: 0.0006593213574888068\n",
      "[LOG 20191027-17:49:20] epoch: 97 train-loss: 0.0006537347753692302\n",
      "[LOG 20191027-17:49:21] epoch: 98 train-loss: 0.0006485429712483892\n",
      "[LOG 20191027-17:49:21] epoch: 99 train-loss: 0.0006437413785533863\n",
      "[LOG 20191027-17:49:21] epoch: 100 train-loss: 0.0006393270286935149\n",
      "[LOG 20191027-17:49:21] epoch: 100 new best train-loss: 0.0006393270286935149 found\n",
      "[LOG 20191027-17:49:21] epoch: 101 train-loss: 0.0006352955197144183\n",
      "[LOG 20191027-17:49:22] epoch: 102 train-loss: 0.0006316433755273465\n",
      "[LOG 20191027-17:49:22] epoch: 103 train-loss: 0.0006283658631218714\n",
      "[LOG 20191027-17:49:22] epoch: 104 train-loss: 0.0006254599584281095\n",
      "[LOG 20191027-17:49:22] epoch: 105 train-loss: 0.0006229214286577189\n",
      "[LOG 20191027-17:49:23] epoch: 106 train-loss: 0.0006207462920428952\n",
      "[LOG 20191027-17:49:23] epoch: 107 train-loss: 0.0006189304649524274\n",
      "[LOG 20191027-17:49:23] epoch: 108 train-loss: 0.0006174703112264979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:49:23] epoch: 109 train-loss: 0.0006163620346342213\n",
      "[LOG 20191027-17:49:24] epoch: 110 train-loss: 0.0006156018444016809\n",
      "[LOG 20191027-17:49:24] epoch: 110 new best train-loss: 0.0006156018444016809 found\n",
      "[LOG 20191027-17:49:24] epoch: 111 train-loss: 0.0006151856441647396\n",
      "[LOG 20191027-17:49:24] epoch: 112 train-loss: 0.0006151097877591383\n",
      "[LOG 20191027-17:49:25] epoch: 113 train-loss: 0.0006153704462121823\n",
      "[LOG 20191027-17:49:25] epoch: 114 train-loss: 0.0006159639433462871\n",
      "[LOG 20191027-17:49:25] epoch: 115 train-loss: 0.0006168863437778782\n",
      "[LOG 20191027-17:49:25] epoch: 116 train-loss: 0.0006181337766975048\n",
      "[LOG 20191027-17:49:26] epoch: 117 train-loss: 0.0006197021575644612\n",
      "[LOG 20191027-17:49:26] epoch: 118 train-loss: 0.0006215876355781802\n",
      "[LOG 20191027-17:49:26] epoch: 119 train-loss: 0.00062378603797697\n",
      "[LOG 20191027-17:49:26] epoch: 120 train-loss: 0.000626293429377256\n",
      "[LOG 20191027-17:49:27] epoch: 121 train-loss: 0.0006291056424743147\n",
      "[LOG 20191027-17:49:27] epoch: 122 train-loss: 0.0006322182471194537\n",
      "[LOG 20191027-17:49:27] epoch: 123 train-loss: 0.0006356275007419754\n",
      "[LOG 20191027-17:49:27] epoch: 124 train-loss: 0.0006393288649633178\n",
      "[LOG 20191027-17:49:28] epoch: 125 train-loss: 0.0006433182597902487\n",
      "[LOG 20191027-17:49:28] epoch: 126 train-loss: 0.0006475909367509303\n",
      "[LOG 20191027-17:49:28] epoch: 127 train-loss: 0.0006521433115267428\n",
      "[LOG 20191027-17:49:28] epoch: 128 train-loss: 0.0006569708912138594\n",
      "[LOG 20191027-17:49:29] epoch: 129 train-loss: 0.0006620689591727569\n",
      "[LOG 20191027-17:49:29] epoch: 130 train-loss: 0.0006674333444607328\n",
      "[LOG 20191027-17:49:29] epoch: 131 train-loss: 0.0006730595378030557\n",
      "[LOG 20191027-17:49:29] epoch: 132 train-loss: 0.0006789435637983843\n",
      "[LOG 20191027-17:49:30] epoch: 133 train-loss: 0.0006850805884823785\n",
      "[LOG 20191027-17:49:30] epoch: 134 train-loss: 0.000691465755153331\n",
      "[LOG 20191027-17:49:30] epoch: 135 train-loss: 0.0006980948173804791\n",
      "[LOG 20191027-17:49:31] epoch: 136 train-loss: 0.0007049633732094662\n",
      "[LOG 20191027-17:49:31] epoch: 137 train-loss: 0.0007120662012312096\n",
      "[LOG 20191027-17:49:31] epoch: 138 train-loss: 0.0007193985966296168\n",
      "[LOG 20191027-17:49:31] epoch: 139 train-loss: 0.0007269556572282454\n",
      "[LOG 20191027-17:49:32] epoch: 140 train-loss: 0.0007347329337790143\n",
      "[LOG 20191027-17:49:32] epoch: 141 train-loss: 0.0007427247073792387\n",
      "[LOG 20191027-17:49:32] epoch: 142 train-loss: 0.0007509263723477488\n",
      "[LOG 20191027-17:49:32] epoch: 143 train-loss: 0.0007593328537041089\n",
      "[LOG 20191027-17:49:33] epoch: 144 train-loss: 0.0007679385507799452\n",
      "[LOG 20191027-17:49:33] epoch: 145 train-loss: 0.0007767381121084327\n",
      "[LOG 20191027-17:49:33] epoch: 146 train-loss: 0.0007857265245547751\n",
      "[LOG 20191027-17:49:33] epoch: 147 train-loss: 0.0007948985421535326\n",
      "[LOG 20191027-17:49:34] epoch: 148 train-loss: 0.0008042476511036512\n",
      "[LOG 20191027-17:49:34] epoch: 149 train-loss: 0.0008137681743392022\n",
      "[LOG 20191027-17:49:34] epoch: 150 train-loss: 0.0008234548331529368\n",
      "[LOG 20191027-17:49:34] epoch: 151 train-loss: 0.0008333021942235064\n",
      "[LOG 20191027-17:49:35] epoch: 152 train-loss: 0.0008433026778220665\n",
      "[LOG 20191027-17:49:35] epoch: 153 train-loss: 0.0008534505923307734\n",
      "[LOG 20191027-17:49:35] epoch: 154 train-loss: 0.0008637400133011397\n",
      "[LOG 20191027-17:49:35] epoch: 155 train-loss: 0.0008741648107388755\n",
      "[LOG 20191027-17:49:36] epoch: 156 train-loss: 0.0008847167446219828\n",
      "[LOG 20191027-17:49:36] epoch: 157 train-loss: 0.0008953903416113462\n",
      "[LOG 20191027-17:49:36] epoch: 158 train-loss: 0.0009061791060958058\n",
      "[LOG 20191027-17:49:37] epoch: 159 train-loss: 0.0009170751945930533\n",
      "[LOG 20191027-17:49:37] epoch: 160 train-loss: 0.0009280721187678864\n",
      "[LOG 20191027-17:49:37] epoch: 161 train-loss: 0.0009391622079419903\n",
      "[LOG 20191027-17:49:38] epoch: 162 train-loss: 0.0009503386954747839\n",
      "[LOG 20191027-17:49:38] epoch: 163 train-loss: 0.000961593237661873\n",
      "[LOG 20191027-17:49:38] epoch: 164 train-loss: 0.0009729192315717228\n",
      "[LOG 20191027-17:49:38] epoch: 165 train-loss: 0.0009843091174843721\n",
      "[LOG 20191027-17:49:39] epoch: 166 train-loss: 0.0009957542752090376\n",
      "[LOG 20191027-17:49:39] epoch: 167 train-loss: 0.001007247987217852\n",
      "[LOG 20191027-17:49:39] epoch: 168 train-loss: 0.0010187819843849866\n",
      "[LOG 20191027-17:49:40] epoch: 169 train-loss: 0.0010303476537956158\n",
      "[LOG 20191027-17:49:40] epoch: 170 train-loss: 0.0010419372520118486\n",
      "[LOG 20191027-17:49:40] epoch: 171 train-loss: 0.0010535419896768872\n",
      "[LOG 20191027-17:49:40] epoch: 172 train-loss: 0.0010651544289430603\n",
      "[LOG 20191027-17:49:41] epoch: 173 train-loss: 0.0010767659932753304\n",
      "[LOG 20191027-17:49:41] epoch: 174 train-loss: 0.001088366561816656\n",
      "[LOG 20191027-17:49:41] epoch: 175 train-loss: 0.0010999475216522114\n",
      "[LOG 20191027-17:49:42] epoch: 176 train-loss: 0.0011115025790786603\n",
      "[LOG 20191027-17:49:42] epoch: 177 train-loss: 0.001123018366342876\n",
      "[LOG 20191027-17:49:42] epoch: 178 train-loss: 0.0011344875765644247\n",
      "[LOG 20191027-17:49:43] epoch: 179 train-loss: 0.0011459012239356525\n",
      "[LOG 20191027-17:49:43] epoch: 180 train-loss: 0.0011572477469599107\n",
      "[LOG 20191027-17:49:43] epoch: 181 train-loss: 0.0011685184999805642\n",
      "[LOG 20191027-17:49:44] epoch: 182 train-loss: 0.0011797037041105796\n",
      "[LOG 20191027-17:49:44] epoch: 183 train-loss: 0.001190790793771157\n",
      "[LOG 20191027-17:49:44] epoch: 184 train-loss: 0.0012017713033856126\n",
      "[LOG 20191027-17:49:44] epoch: 185 train-loss: 0.0012126341516704997\n",
      "[LOG 20191027-17:49:45] epoch: 186 train-loss: 0.0012233685356477508\n",
      "[LOG 20191027-17:49:45] epoch: 187 train-loss: 0.0012339626428001793\n",
      "[LOG 20191027-17:49:45] epoch: 188 train-loss: 0.0012444077401596587\n",
      "[LOG 20191027-17:49:46] epoch: 189 train-loss: 0.00125469113845611\n",
      "[LOG 20191027-17:49:46] epoch: 190 train-loss: 0.001264802624064032\n",
      "[LOG 20191027-17:49:46] epoch: 191 train-loss: 0.001274731974262977\n",
      "[LOG 20191027-17:49:46] epoch: 192 train-loss: 0.0012844669472542591\n",
      "[LOG 20191027-17:49:47] epoch: 193 train-loss: 0.001293998215260217\n",
      "[LOG 20191027-17:49:47] epoch: 194 train-loss: 0.001303314304095693\n",
      "[LOG 20191027-17:49:47] epoch: 195 train-loss: 0.001312405369390035\n",
      "[LOG 20191027-17:49:48] epoch: 196 train-loss: 0.0013212618105171714\n",
      "[LOG 20191027-17:49:48] epoch: 197 train-loss: 0.001329872728092596\n",
      "[LOG 20191027-17:49:48] epoch: 198 train-loss: 0.0013382286888372619\n",
      "[LOG 20191027-17:49:48] epoch: 199 train-loss: 0.001346320455922978\n",
      "[LOG 20191027-17:49:49] epoch: 200 train-loss: 0.0013541379303205758\n",
      "[LOG 20191027-17:49:49] epoch: 201 train-loss: 0.001361673614155734\n",
      "[LOG 20191027-17:49:49] epoch: 202 train-loss: 0.00136891772490344\n",
      "[LOG 20191027-17:49:49] epoch: 203 train-loss: 0.0013758639179286547\n",
      "[LOG 20191027-17:49:50] epoch: 204 train-loss: 0.0013825030837324448\n",
      "[LOG 20191027-17:49:50] epoch: 205 train-loss: 0.001388826931361109\n",
      "[LOG 20191027-17:49:50] epoch: 206 train-loss: 0.0013948313790024258\n",
      "[LOG 20191027-17:49:51] epoch: 207 train-loss: 0.0014005063858348876\n",
      "[LOG 20191027-17:49:51] epoch: 208 train-loss: 0.0014058486340218224\n",
      "[LOG 20191027-17:49:51] epoch: 209 train-loss: 0.0014108505201875232\n",
      "[LOG 20191027-17:49:51] epoch: 210 train-loss: 0.0014155088938423432\n",
      "[LOG 20191027-17:49:52] epoch: 211 train-loss: 0.0014198166209098417\n",
      "[LOG 20191027-17:49:52] epoch: 212 train-loss: 0.0014237717368814629\n",
      "[LOG 20191027-17:49:52] epoch: 213 train-loss: 0.001427369745215401\n",
      "[LOG 20191027-17:49:53] epoch: 214 train-loss: 0.001430607069778489\n",
      "[LOG 20191027-17:49:53] epoch: 215 train-loss: 0.0014334810657601338\n",
      "[LOG 20191027-17:49:53] epoch: 216 train-loss: 0.0014359905253513716\n",
      "[LOG 20191027-17:49:53] epoch: 217 train-loss: 0.0014381323417183012\n",
      "[LOG 20191027-17:49:54] epoch: 218 train-loss: 0.0014399067149497569\n",
      "[LOG 20191027-17:49:54] epoch: 219 train-loss: 0.0014413128556043375\n",
      "[LOG 20191027-17:49:54] epoch: 220 train-loss: 0.001442350618162891\n",
      "[LOG 20191027-17:49:55] epoch: 221 train-loss: 0.0014430196788453031\n",
      "[LOG 20191027-17:49:55] epoch: 222 train-loss: 0.0014433221986109857\n",
      "[LOG 20191027-17:49:55] epoch: 223 train-loss: 0.0014432580828724895\n",
      "[LOG 20191027-17:49:56] epoch: 224 train-loss: 0.0014428294271056075\n",
      "[LOG 20191027-17:49:56] epoch: 225 train-loss: 0.0014420386360143311\n",
      "[LOG 20191027-17:49:56] epoch: 226 train-loss: 0.0014408876631932799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:49:57] epoch: 227 train-loss: 0.0014393800774996635\n",
      "[LOG 20191027-17:49:57] epoch: 228 train-loss: 0.0014375186219695024\n",
      "[LOG 20191027-17:49:57] epoch: 229 train-loss: 0.001435306083294563\n",
      "[LOG 20191027-17:49:57] epoch: 230 train-loss: 0.0014327465833048336\n",
      "[LOG 20191027-17:49:58] epoch: 231 train-loss: 0.0014298442438303027\n",
      "[LOG 20191027-17:49:58] epoch: 232 train-loss: 0.0014266024772950914\n",
      "[LOG 20191027-17:49:58] epoch: 233 train-loss: 0.0014230258348106872\n",
      "[LOG 20191027-17:49:59] epoch: 234 train-loss: 0.0014191190421115607\n",
      "[LOG 20191027-17:49:59] epoch: 235 train-loss: 0.0014148867412586696\n",
      "[LOG 20191027-17:49:59] epoch: 236 train-loss: 0.0014103337889537215\n",
      "[LOG 20191027-17:49:59] epoch: 237 train-loss: 0.0014054650528123602\n",
      "[LOG 20191027-17:50:00] epoch: 238 train-loss: 0.001400286600983236\n",
      "[LOG 20191027-17:50:00] epoch: 239 train-loss: 0.0013948032501502894\n",
      "[LOG 20191027-17:50:00] epoch: 240 train-loss: 0.0013890208392695058\n",
      "[LOG 20191027-17:50:01] epoch: 241 train-loss: 0.0013829450981575064\n",
      "[LOG 20191027-17:50:01] epoch: 242 train-loss: 0.0013765816984232515\n",
      "[LOG 20191027-17:50:01] epoch: 243 train-loss: 0.0013699370756512508\n",
      "[LOG 20191027-17:50:01] epoch: 244 train-loss: 0.0013630188223032746\n",
      "[LOG 20191027-17:50:02] epoch: 245 train-loss: 0.0013558329847001005\n",
      "[LOG 20191027-17:50:02] epoch: 246 train-loss: 0.001348386187601136\n",
      "[LOG 20191027-17:50:02] epoch: 247 train-loss: 0.0013406846846919507\n",
      "[LOG 20191027-17:50:03] epoch: 248 train-loss: 0.0013327366214070935\n",
      "[LOG 20191027-17:50:03] epoch: 249 train-loss: 0.001324548644333845\n",
      "[LOG 20191027-17:50:03] epoch: 250 train-loss: 0.0013161282295186538\n",
      "[LOG 20191027-17:50:03] epoch: 251 train-loss: 0.0013074837916065007\n",
      "[LOG 20191027-17:50:04] epoch: 252 train-loss: 0.0012986212350369897\n",
      "[LOG 20191027-17:50:04] epoch: 253 train-loss: 0.0012895500294689555\n",
      "[LOG 20191027-17:50:04] epoch: 254 train-loss: 0.0012802752025891095\n",
      "[LOG 20191027-17:50:04] epoch: 255 train-loss: 0.0012708067042694893\n",
      "[LOG 20191027-17:50:05] epoch: 256 train-loss: 0.001261151846847497\n",
      "[LOG 20191027-17:50:05] epoch: 257 train-loss: 0.0012513174151536077\n",
      "[LOG 20191027-17:50:05] epoch: 258 train-loss: 0.0012413123586156871\n",
      "[LOG 20191027-17:50:06] epoch: 259 train-loss: 0.001231143298355164\n",
      "[LOG 20191027-17:50:06] epoch: 260 train-loss: 0.0012208184889459517\n",
      "[LOG 20191027-17:50:06] epoch: 261 train-loss: 0.0012103449680580525\n",
      "[LOG 20191027-17:50:06] epoch: 262 train-loss: 0.0011997302681265865\n",
      "[LOG 20191027-17:50:07] epoch: 263 train-loss: 0.0011889830566360615\n",
      "[LOG 20191027-17:50:07] epoch: 264 train-loss: 0.001178110069304239\n",
      "[LOG 20191027-17:50:07] epoch: 265 train-loss: 0.0011671195843518944\n",
      "[LOG 20191027-17:50:08] epoch: 266 train-loss: 0.0011560184375412064\n",
      "[LOG 20191027-17:50:08] epoch: 267 train-loss: 0.0011448151744843926\n",
      "[LOG 20191027-17:50:08] epoch: 268 train-loss: 0.0011335165800119285\n",
      "[LOG 20191027-17:50:09] epoch: 269 train-loss: 0.0011221305048820795\n",
      "[LOG 20191027-17:50:09] epoch: 270 train-loss: 0.0011106637175544165\n",
      "[LOG 20191027-17:50:09] epoch: 271 train-loss: 0.0010991238505084766\n",
      "[LOG 20191027-17:50:10] epoch: 272 train-loss: 0.0010875189273065189\n",
      "[LOG 20191027-17:50:10] epoch: 273 train-loss: 0.0010758562839328079\n",
      "[LOG 20191027-17:50:10] epoch: 274 train-loss: 0.0010641448934620712\n",
      "[LOG 20191027-17:50:10] epoch: 275 train-loss: 0.0010523916771489894\n",
      "[LOG 20191027-17:50:11] epoch: 276 train-loss: 0.0010406037654320244\n",
      "[LOG 20191027-17:50:11] epoch: 277 train-loss: 0.001028788816256565\n",
      "[LOG 20191027-17:50:11] epoch: 278 train-loss: 0.0010169552679144545\n",
      "[LOG 20191027-17:50:12] epoch: 279 train-loss: 0.0010051100580312777\n",
      "[LOG 20191027-17:50:12] epoch: 280 train-loss: 0.000993261426629033\n",
      "[LOG 20191027-17:50:12] epoch: 281 train-loss: 0.0009814175100473221\n",
      "[LOG 20191027-17:50:13] epoch: 282 train-loss: 0.0009695846656541107\n",
      "[LOG 20191027-17:50:13] epoch: 283 train-loss: 0.0009577709934092127\n",
      "[LOG 20191027-17:50:13] epoch: 284 train-loss: 0.0009459845132369082\n",
      "[LOG 20191027-17:50:14] epoch: 285 train-loss: 0.0009342322264274117\n",
      "[LOG 20191027-17:50:14] epoch: 286 train-loss: 0.000922521439861157\n",
      "[LOG 20191027-17:50:14] epoch: 287 train-loss: 0.0009108594076678855\n",
      "[LOG 20191027-17:50:15] epoch: 288 train-loss: 0.0008992533039418049\n",
      "[LOG 20191027-17:50:15] epoch: 289 train-loss: 0.0008877096934156725\n",
      "[LOG 20191027-17:50:15] epoch: 290 train-loss: 0.0008762350080360193\n",
      "[LOG 20191027-17:50:15] epoch: 291 train-loss: 0.0008648361090308754\n",
      "[LOG 20191027-17:50:16] epoch: 292 train-loss: 0.0008535195283911889\n",
      "[LOG 20191027-17:50:16] epoch: 293 train-loss: 0.0008422908158536302\n",
      "[LOG 20191027-17:50:16] epoch: 294 train-loss: 0.0008311557339766296\n",
      "[LOG 20191027-17:50:17] epoch: 295 train-loss: 0.0008201199725590413\n",
      "[LOG 20191027-17:50:17] epoch: 296 train-loss: 0.0008091888430499239\n",
      "[LOG 20191027-17:50:17] epoch: 297 train-loss: 0.0007983675968716852\n",
      "[LOG 20191027-17:50:18] epoch: 298 train-loss: 0.0007876606377976714\n",
      "[LOG 20191027-17:50:18] epoch: 299 train-loss: 0.0007770729134790599\n",
      "[LOG 20191027-17:50:18] epoch: 300 train-loss: 0.0007666077217436396\n",
      "[LOG 20191027-17:50:19] epoch: 301 train-loss: 0.0007562693826912437\n",
      "[LOG 20191027-17:50:19] epoch: 302 train-loss: 0.0007460617962351535\n",
      "[LOG 20191027-17:50:19] epoch: 303 train-loss: 0.0007359889332292369\n",
      "[LOG 20191027-17:50:19] epoch: 304 train-loss: 0.0007260526381287491\n",
      "[LOG 20191027-17:50:20] epoch: 305 train-loss: 0.0007162565980252111\n",
      "[LOG 20191027-17:50:20] epoch: 306 train-loss: 0.0007066034231684171\n",
      "[LOG 20191027-17:50:20] epoch: 307 train-loss: 0.0006970958929741755\n",
      "[LOG 20191027-17:50:21] epoch: 308 train-loss: 0.0006877353516756557\n",
      "[LOG 20191027-17:50:21] epoch: 309 train-loss: 0.0006785244904676802\n",
      "[LOG 20191027-17:50:21] epoch: 310 train-loss: 0.0006694653266094974\n",
      "[LOG 20191027-17:50:22] epoch: 311 train-loss: 0.0006605589542232337\n",
      "[LOG 20191027-17:50:22] epoch: 312 train-loss: 0.0006518073496408761\n",
      "[LOG 20191027-17:50:22] epoch: 313 train-loss: 0.0006432112395486911\n",
      "[LOG 20191027-17:50:22] epoch: 314 train-loss: 0.0006347705784719437\n",
      "[LOG 20191027-17:50:23] epoch: 315 train-loss: 0.0006264877156354487\n",
      "[LOG 20191027-17:50:23] epoch: 316 train-loss: 0.0006183641389725381\n",
      "[LOG 20191027-17:50:23] epoch: 317 train-loss: 0.0006103991645431961\n",
      "[LOG 20191027-17:50:24] epoch: 318 train-loss: 0.0006025934098943253\n",
      "[LOG 20191027-17:50:24] epoch: 319 train-loss: 0.00059494748711586\n",
      "[LOG 20191027-17:50:24] epoch: 320 train-loss: 0.0005874633425264619\n",
      "[LOG 20191027-17:50:24] epoch: 320 new best train-loss: 0.0005874633425264619 found\n",
      "[LOG 20191027-17:50:25] epoch: 321 train-loss: 0.0005801395809612586\n",
      "[LOG 20191027-17:50:25] epoch: 322 train-loss: 0.0005729765925934771\n",
      "[LOG 20191027-17:50:25] epoch: 323 train-loss: 0.0005659735452354653\n",
      "[LOG 20191027-17:50:26] epoch: 324 train-loss: 0.0005591320423263824\n",
      "[LOG 20191027-17:50:26] epoch: 325 train-loss: 0.0005524517182493582\n",
      "[LOG 20191027-17:50:26] epoch: 326 train-loss: 0.0005459311860249727\n",
      "[LOG 20191027-17:50:27] epoch: 327 train-loss: 0.0005395712214522064\n",
      "[LOG 20191027-17:50:27] epoch: 328 train-loss: 0.0005333721292117843\n",
      "[LOG 20191027-17:50:27] epoch: 329 train-loss: 0.0005273332290016697\n",
      "[LOG 20191027-17:50:28] epoch: 330 train-loss: 0.0005214537213760195\n",
      "[LOG 20191027-17:50:28] epoch: 330 new best train-loss: 0.0005214537213760195 found\n",
      "[LOG 20191027-17:50:28] epoch: 331 train-loss: 0.0005157332880116883\n",
      "[LOG 20191027-17:50:28] epoch: 332 train-loss: 0.0005101719279991812\n",
      "[LOG 20191027-17:50:29] epoch: 333 train-loss: 0.0005047685226600152\n",
      "[LOG 20191027-17:50:29] epoch: 334 train-loss: 0.0004995224835511181\n",
      "[LOG 20191027-17:50:29] epoch: 335 train-loss: 0.0004944338525092462\n",
      "[LOG 20191027-17:50:30] epoch: 336 train-loss: 0.0004895017291346448\n",
      "[LOG 20191027-17:50:30] epoch: 337 train-loss: 0.0004847257114306558\n",
      "[LOG 20191027-17:50:30] epoch: 338 train-loss: 0.00048010513910412556\n",
      "[LOG 20191027-17:50:30] epoch: 339 train-loss: 0.0004756387716042809\n",
      "[LOG 20191027-17:50:31] epoch: 340 train-loss: 0.00047132624331425177\n",
      "[LOG 20191027-17:50:31] epoch: 340 new best train-loss: 0.00047132624331425177 found\n",
      "[LOG 20191027-17:50:31] epoch: 341 train-loss: 0.00046716628276044503\n",
      "[LOG 20191027-17:50:31] epoch: 342 train-loss: 0.00046315855342982104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:50:32] epoch: 343 train-loss: 0.0004593019657477271\n",
      "[LOG 20191027-17:50:32] epoch: 344 train-loss: 0.00045559525506178034\n",
      "[LOG 20191027-17:50:32] epoch: 345 train-loss: 0.0004520377169683343\n",
      "[LOG 20191027-17:50:33] epoch: 346 train-loss: 0.00044862821596325375\n",
      "[LOG 20191027-17:50:33] epoch: 347 train-loss: 0.00044536549739859765\n",
      "[LOG 20191027-17:50:33] epoch: 348 train-loss: 0.0004422481065375905\n",
      "[LOG 20191027-17:50:33] epoch: 349 train-loss: 0.0004392754185573722\n",
      "[LOG 20191027-17:50:34] epoch: 350 train-loss: 0.0004364461442492029\n",
      "[LOG 20191027-17:50:34] epoch: 350 new best train-loss: 0.0004364461442492029 found\n",
      "[LOG 20191027-17:50:34] epoch: 351 train-loss: 0.000433758382769156\n",
      "[LOG 20191027-17:50:34] epoch: 352 train-loss: 0.00043121108319610357\n",
      "[LOG 20191027-17:50:35] epoch: 353 train-loss: 0.00042880295632130583\n",
      "[LOG 20191027-17:50:35] epoch: 354 train-loss: 0.0004265323336767324\n",
      "[LOG 20191027-17:50:35] epoch: 355 train-loss: 0.0004243977655278286\n",
      "[LOG 20191027-17:50:36] epoch: 356 train-loss: 0.0004223974556225585\n",
      "[LOG 20191027-17:50:36] epoch: 357 train-loss: 0.00042053010474774055\n",
      "[LOG 20191027-17:50:36] epoch: 358 train-loss: 0.0004187938011455117\n",
      "[LOG 20191027-17:50:37] epoch: 359 train-loss: 0.0004171867622062564\n",
      "[LOG 20191027-17:50:37] epoch: 360 train-loss: 0.0004157073367423436\n",
      "[LOG 20191027-17:50:37] epoch: 360 new best train-loss: 0.0004157073367423436 found\n",
      "[LOG 20191027-17:50:37] epoch: 361 train-loss: 0.0004143535670664278\n",
      "[LOG 20191027-17:50:38] epoch: 362 train-loss: 0.000413123572798213\n",
      "[LOG 20191027-17:50:38] epoch: 363 train-loss: 0.00041201539261237485\n",
      "[LOG 20191027-17:50:38] epoch: 364 train-loss: 0.0004110270688215678\n",
      "[LOG 20191027-17:50:38] epoch: 365 train-loss: 0.00041015650913323043\n",
      "[LOG 20191027-17:50:39] epoch: 366 train-loss: 0.0004094016635463049\n",
      "[LOG 20191027-17:50:39] epoch: 367 train-loss: 0.00040876036928239046\n",
      "[LOG 20191027-17:50:39] epoch: 368 train-loss: 0.00040823040217219386\n",
      "[LOG 20191027-17:50:40] epoch: 369 train-loss: 0.00040780958170216763\n",
      "[LOG 20191027-17:50:40] epoch: 370 train-loss: 0.00040749550544205704\n",
      "[LOG 20191027-17:50:40] epoch: 370 new best train-loss: 0.00040749550544205704 found\n",
      "[LOG 20191027-17:50:40] epoch: 371 train-loss: 0.0004072860483574914\n",
      "[LOG 20191027-17:50:41] epoch: 372 train-loss: 0.00040717878664509044\n",
      "[LOG 20191027-17:50:41] epoch: 373 train-loss: 0.0004071714483870892\n",
      "[LOG 20191027-17:50:41] epoch: 374 train-loss: 0.000407261383770674\n",
      "[LOG 20191027-17:50:41] epoch: 375 train-loss: 0.00040744625539446133\n",
      "[LOG 20191027-17:50:42] epoch: 376 train-loss: 0.0004077236194461875\n",
      "[LOG 20191027-17:50:42] epoch: 377 train-loss: 0.0004080908356627333\n",
      "[LOG 20191027-17:50:42] epoch: 378 train-loss: 0.0004085452733306738\n",
      "[LOG 20191027-17:50:43] epoch: 379 train-loss: 0.0004090847587576718\n",
      "[LOG 20191027-17:50:43] epoch: 380 train-loss: 0.0004097064761481306\n",
      "[LOG 20191027-17:50:43] epoch: 381 train-loss: 0.0004104079653188819\n",
      "[LOG 20191027-17:50:44] epoch: 382 train-loss: 0.0004111864595870429\n",
      "[LOG 20191027-17:50:44] epoch: 383 train-loss: 0.00041203953196600196\n",
      "[LOG 20191027-17:50:44] epoch: 384 train-loss: 0.00041296402378065977\n",
      "[LOG 20191027-17:50:44] epoch: 385 train-loss: 0.00041395801827093237\n",
      "[LOG 20191027-17:50:45] epoch: 386 train-loss: 0.0004150184049649397\n",
      "[LOG 20191027-17:50:45] epoch: 387 train-loss: 0.00041614280235080514\n",
      "[LOG 20191027-17:50:45] epoch: 388 train-loss: 0.00041732830550245126\n",
      "[LOG 20191027-17:50:46] epoch: 389 train-loss: 0.0004185726565992809\n",
      "[LOG 20191027-17:50:46] epoch: 390 train-loss: 0.0004198730189273192\n",
      "[LOG 20191027-17:50:46] epoch: 391 train-loss: 0.00042122692912016646\n",
      "[LOG 20191027-17:50:47] epoch: 392 train-loss: 0.0004226312221362605\n",
      "[LOG 20191027-17:50:47] epoch: 393 train-loss: 0.00042408391573189874\n",
      "[LOG 20191027-17:50:47] epoch: 394 train-loss: 0.00042558230279610143\n",
      "[LOG 20191027-17:50:47] epoch: 395 train-loss: 0.0004271240022717393\n",
      "[LOG 20191027-17:50:48] epoch: 396 train-loss: 0.0004287060942260723\n",
      "[LOG 20191027-17:50:48] epoch: 397 train-loss: 0.0004303260448068613\n",
      "[LOG 20191027-17:50:48] epoch: 398 train-loss: 0.00043198179537284886\n",
      "[LOG 20191027-17:50:49] epoch: 399 train-loss: 0.0004336705651439843\n",
      "[LOG 20191027-17:50:49] epoch: 400 train-loss: 0.0004353897761575354\n",
      "[LOG 20191027-17:50:49] epoch: 401 train-loss: 0.00043713688319257926\n",
      "[LOG 20191027-17:50:50] epoch: 402 train-loss: 0.0004389098858155194\n",
      "[LOG 20191027-17:50:50] epoch: 403 train-loss: 0.0004407063247526821\n",
      "[LOG 20191027-17:50:50] epoch: 404 train-loss: 0.00044252392353882897\n",
      "[LOG 20191027-17:50:50] epoch: 405 train-loss: 0.0004443599009391619\n",
      "[LOG 20191027-17:50:51] epoch: 406 train-loss: 0.00044621248889598064\n",
      "[LOG 20191027-17:50:51] epoch: 407 train-loss: 0.00044807905214838684\n",
      "[LOG 20191027-17:50:51] epoch: 408 train-loss: 0.0004499579431467282\n",
      "[LOG 20191027-17:50:52] epoch: 409 train-loss: 0.00045184674536358216\n",
      "[LOG 20191027-17:50:52] epoch: 410 train-loss: 0.00045374270075626555\n",
      "[LOG 20191027-17:50:52] epoch: 411 train-loss: 0.000455643787063309\n",
      "[LOG 20191027-17:50:52] epoch: 412 train-loss: 0.00045754832308375626\n",
      "[LOG 20191027-17:50:53] epoch: 413 train-loss: 0.0004594543943312601\n",
      "[LOG 20191027-17:50:53] epoch: 414 train-loss: 0.0004613600376615068\n",
      "[LOG 20191027-17:50:53] epoch: 415 train-loss: 0.000463262396351638\n",
      "[LOG 20191027-17:50:54] epoch: 416 train-loss: 0.0004651602166632074\n",
      "[LOG 20191027-17:50:54] epoch: 417 train-loss: 0.0004670520274885348\n",
      "[LOG 20191027-17:50:54] epoch: 418 train-loss: 0.00046893595754227135\n",
      "[LOG 20191027-17:50:55] epoch: 419 train-loss: 0.0004708099536401278\n",
      "[LOG 20191027-17:50:55] epoch: 420 train-loss: 0.0004726724841930263\n",
      "[LOG 20191027-17:50:55] epoch: 421 train-loss: 0.000474522254535259\n",
      "[LOG 20191027-17:50:56] epoch: 422 train-loss: 0.0004763574947901361\n",
      "[LOG 20191027-17:50:56] epoch: 423 train-loss: 0.0004781764782819664\n",
      "[LOG 20191027-17:50:56] epoch: 424 train-loss: 0.0004799773159902543\n",
      "[LOG 20191027-17:50:57] epoch: 425 train-loss: 0.0004817595286112919\n",
      "[LOG 20191027-17:50:57] epoch: 426 train-loss: 0.00048352101794080227\n",
      "[LOG 20191027-17:50:57] epoch: 427 train-loss: 0.0004852609627050697\n",
      "[LOG 20191027-17:50:58] epoch: 428 train-loss: 0.0004869778031206806\n",
      "[LOG 20191027-17:50:58] epoch: 429 train-loss: 0.0004886702154180966\n",
      "[LOG 20191027-17:50:58] epoch: 430 train-loss: 0.0004903362532786559\n",
      "[LOG 20191027-17:50:58] epoch: 431 train-loss: 0.0004919748225802323\n",
      "[LOG 20191027-17:50:59] epoch: 432 train-loss: 0.0004935852439302835\n",
      "[LOG 20191027-17:50:59] epoch: 433 train-loss: 0.0004951649652866763\n",
      "[LOG 20191027-17:50:59] epoch: 434 train-loss: 0.0004967136828781804\n",
      "[LOG 20191027-17:51:00] epoch: 435 train-loss: 0.0004982303371434682\n",
      "[LOG 20191027-17:51:00] epoch: 436 train-loss: 0.0004997114883735776\n",
      "[LOG 20191027-17:51:00] epoch: 437 train-loss: 0.0005011566208850127\n",
      "[LOG 20191027-17:51:01] epoch: 438 train-loss: 0.0005025653035772848\n",
      "[LOG 20191027-17:51:01] epoch: 439 train-loss: 0.0005039365159973386\n",
      "[LOG 20191027-17:51:01] epoch: 440 train-loss: 0.0005052674796388601\n",
      "[LOG 20191027-17:51:02] epoch: 441 train-loss: 0.0005065575060143601\n",
      "[LOG 20191027-17:51:02] epoch: 442 train-loss: 0.0005078055964986561\n",
      "[LOG 20191027-17:51:02] epoch: 443 train-loss: 0.000509010689711431\n",
      "[LOG 20191027-17:51:02] epoch: 444 train-loss: 0.0005101705255583511\n",
      "[LOG 20191027-17:51:03] epoch: 445 train-loss: 0.0005112856924824882\n",
      "[LOG 20191027-17:51:03] epoch: 446 train-loss: 0.0005123571445437847\n",
      "[LOG 20191027-17:51:03] epoch: 447 train-loss: 0.0005133832682986395\n",
      "[LOG 20191027-17:51:04] epoch: 448 train-loss: 0.0005143624230186106\n",
      "[LOG 20191027-17:51:04] epoch: 449 train-loss: 0.0005152937237653532\n",
      "[LOG 20191027-17:51:04] epoch: 450 train-loss: 0.0005161793997103814\n",
      "[LOG 20191027-17:51:05] epoch: 451 train-loss: 0.0005170183649170212\n",
      "[LOG 20191027-17:51:05] epoch: 452 train-loss: 0.0005178096071176697\n",
      "[LOG 20191027-17:51:05] epoch: 453 train-loss: 0.0005185550489841262\n",
      "[LOG 20191027-17:51:06] epoch: 454 train-loss: 0.0005192548478589742\n",
      "[LOG 20191027-17:51:06] epoch: 455 train-loss: 0.0005199079614612856\n",
      "[LOG 20191027-17:51:06] epoch: 456 train-loss: 0.0005205179568292806\n",
      "[LOG 20191027-17:51:06] epoch: 457 train-loss: 0.0005210834506215178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:51:07] epoch: 458 train-loss: 0.0005216054414631799\n",
      "[LOG 20191027-17:51:07] epoch: 459 train-loss: 0.00052208581018931\n",
      "[LOG 20191027-17:51:07] epoch: 460 train-loss: 0.0005225238201091997\n",
      "[LOG 20191027-17:51:08] epoch: 461 train-loss: 0.0005229230910117622\n",
      "[LOG 20191027-17:51:08] epoch: 462 train-loss: 0.0005232816583884414\n",
      "[LOG 20191027-17:51:08] epoch: 463 train-loss: 0.0005236024762780289\n",
      "[LOG 20191027-17:51:09] epoch: 464 train-loss: 0.0005238856583673623\n",
      "[LOG 20191027-17:51:09] epoch: 465 train-loss: 0.0005241328653937671\n",
      "[LOG 20191027-17:51:09] epoch: 466 train-loss: 0.0005243442528808373\n",
      "[LOG 20191027-17:51:10] epoch: 467 train-loss: 0.0005245212614681805\n",
      "[LOG 20191027-17:51:10] epoch: 468 train-loss: 0.0005246643713689991\n",
      "[LOG 20191027-17:51:10] epoch: 469 train-loss: 0.0005247756098469836\n",
      "[LOG 20191027-17:51:11] epoch: 470 train-loss: 0.0005248550423857523\n",
      "[LOG 20191027-17:51:11] epoch: 471 train-loss: 0.0005249042806099169\n",
      "[LOG 20191027-17:51:11] epoch: 472 train-loss: 0.000524922383192461\n",
      "[LOG 20191027-17:51:11] epoch: 473 train-loss: 0.0005249119749350939\n",
      "[LOG 20191027-17:51:12] epoch: 474 train-loss: 0.00052487287484837\n",
      "[LOG 20191027-17:51:12] epoch: 475 train-loss: 0.0005248064526313101\n",
      "[LOG 20191027-17:51:12] epoch: 476 train-loss: 0.0005247121544016409\n",
      "[LOG 20191027-17:51:13] epoch: 477 train-loss: 0.0005245921638561413\n",
      "[LOG 20191027-17:51:13] epoch: 478 train-loss: 0.0005244456606305903\n",
      "[LOG 20191027-17:51:13] epoch: 479 train-loss: 0.0005242736679065274\n",
      "[LOG 20191027-17:51:14] epoch: 480 train-loss: 0.0005240752425379469\n",
      "[LOG 20191027-17:51:14] epoch: 481 train-loss: 0.000523852878359321\n",
      "[LOG 20191027-17:51:14] epoch: 482 train-loss: 0.0005236059696471784\n",
      "[LOG 20191027-17:51:15] epoch: 483 train-loss: 0.0005233353067524149\n",
      "[LOG 20191027-17:51:15] epoch: 484 train-loss: 0.0005230395963735646\n",
      "[LOG 20191027-17:51:15] epoch: 485 train-loss: 0.000522720330991433\n",
      "[LOG 20191027-17:51:16] epoch: 486 train-loss: 0.0005223769803706091\n",
      "[LOG 20191027-17:51:16] epoch: 487 train-loss: 0.0005220099355938146\n",
      "[LOG 20191027-17:51:16] epoch: 488 train-loss: 0.0005216182726144325\n",
      "[LOG 20191027-17:51:16] epoch: 489 train-loss: 0.0005212030864640838\n",
      "[LOG 20191027-17:51:17] epoch: 490 train-loss: 0.0005207638114370639\n",
      "[LOG 20191027-17:51:17] epoch: 491 train-loss: 0.0005203003693168284\n",
      "[LOG 20191027-17:51:17] epoch: 492 train-loss: 0.0005198123462832882\n",
      "[LOG 20191027-17:51:18] epoch: 493 train-loss: 0.0005192999469727511\n",
      "[LOG 20191027-17:51:18] epoch: 494 train-loss: 0.0005187626184124383\n",
      "[LOG 20191027-17:51:18] epoch: 495 train-loss: 0.0005181998067200766\n",
      "[LOG 20191027-17:51:19] epoch: 496 train-loss: 0.0005176114082132699\n",
      "[LOG 20191027-17:51:19] epoch: 497 train-loss: 0.0005169975802346016\n",
      "[LOG 20191027-17:51:19] epoch: 498 train-loss: 0.0005163578962310567\n",
      "[LOG 20191027-17:51:20] epoch: 499 train-loss: 0.0005156924280527164\n",
      "[LOG 20191027-17:51:20] epoch: 500 train-loss: 0.000515000269842858\n",
      "[LOG 20191027-17:51:20] epoch: 501 train-loss: 0.0005142817562955315\n",
      "[LOG 20191027-17:51:21] epoch: 502 train-loss: 0.0005135360361236962\n",
      "[LOG 20191027-17:51:21] epoch: 503 train-loss: 0.0005127636231918586\n",
      "[LOG 20191027-17:51:21] epoch: 504 train-loss: 0.0005119626985106152\n",
      "[LOG 20191027-17:51:22] epoch: 505 train-loss: 0.000511134450789541\n",
      "[LOG 20191027-17:51:22] epoch: 506 train-loss: 0.0005102772865939187\n",
      "[LOG 20191027-17:51:22] epoch: 507 train-loss: 0.0005093921372463228\n",
      "[LOG 20191027-17:51:22] epoch: 508 train-loss: 0.0005084774275019299\n",
      "[LOG 20191027-17:51:23] epoch: 509 train-loss: 0.0005075344588476582\n",
      "[LOG 20191027-17:51:23] epoch: 510 train-loss: 0.00050656144048844\n",
      "[LOG 20191027-17:51:23] epoch: 511 train-loss: 0.0005055601959611522\n",
      "[LOG 20191027-17:51:23] epoch: 512 train-loss: 0.0005045279176556505\n",
      "[LOG 20191027-17:51:24] epoch: 513 train-loss: 0.0005034662481193664\n",
      "[LOG 20191027-17:51:24] epoch: 514 train-loss: 0.0005023739577154629\n",
      "[LOG 20191027-17:51:24] epoch: 515 train-loss: 0.0005012507353967521\n",
      "[LOG 20191027-17:51:24] epoch: 516 train-loss: 0.000500097718941106\n",
      "[LOG 20191027-17:51:25] epoch: 517 train-loss: 0.0004989131248294143\n",
      "[LOG 20191027-17:51:25] epoch: 518 train-loss: 0.0004976987120244303\n",
      "[LOG 20191027-17:51:25] epoch: 519 train-loss: 0.0004964530035067583\n",
      "[LOG 20191027-17:51:26] epoch: 520 train-loss: 0.0004951778619215474\n",
      "[LOG 20191027-17:51:26] epoch: 521 train-loss: 0.0004938707725159475\n",
      "[LOG 20191027-17:51:26] epoch: 522 train-loss: 0.0004925337934764684\n",
      "[LOG 20191027-17:51:26] epoch: 523 train-loss: 0.0004911662772428826\n",
      "[LOG 20191027-17:51:27] epoch: 524 train-loss: 0.0004897682674709358\n",
      "[LOG 20191027-17:51:27] epoch: 525 train-loss: 0.000488340880110627\n",
      "[LOG 20191027-17:51:27] epoch: 526 train-loss: 0.00048688306924304925\n",
      "[LOG 20191027-17:51:27] epoch: 527 train-loss: 0.0004853969262512692\n",
      "[LOG 20191027-17:51:28] epoch: 528 train-loss: 0.0004838808645217796\n",
      "[LOG 20191027-17:51:28] epoch: 529 train-loss: 0.0004823367967219383\n",
      "[LOG 20191027-17:51:28] epoch: 530 train-loss: 0.00048076447819767054\n",
      "[LOG 20191027-17:51:29] epoch: 531 train-loss: 0.0004791643664248113\n",
      "[LOG 20191027-17:51:29] epoch: 532 train-loss: 0.0004775381762556208\n",
      "[LOG 20191027-17:51:29] epoch: 533 train-loss: 0.00047588385132257827\n",
      "[LOG 20191027-17:51:29] epoch: 534 train-loss: 0.0004742044652630284\n",
      "[LOG 20191027-17:51:30] epoch: 535 train-loss: 0.0004724992122646654\n",
      "[LOG 20191027-17:51:30] epoch: 536 train-loss: 0.0004707694201897539\n",
      "[LOG 20191027-17:51:30] epoch: 537 train-loss: 0.000469016631541308\n",
      "[LOG 20191027-17:51:31] epoch: 538 train-loss: 0.0004672392642532941\n",
      "[LOG 20191027-17:51:31] epoch: 539 train-loss: 0.00046544030601580744\n",
      "[LOG 20191027-17:51:31] epoch: 540 train-loss: 0.00046361892145796446\n",
      "[LOG 20191027-17:51:32] epoch: 541 train-loss: 0.0004617764666363655\n",
      "[LOG 20191027-17:51:32] epoch: 542 train-loss: 0.00045991436309122946\n",
      "[LOG 20191027-17:51:32] epoch: 543 train-loss: 0.00045803178363712505\n",
      "[LOG 20191027-17:51:32] epoch: 544 train-loss: 0.00045613157908519497\n",
      "[LOG 20191027-17:51:33] epoch: 545 train-loss: 0.00045421273534884676\n",
      "[LOG 20191027-17:51:33] epoch: 546 train-loss: 0.0004522768776951125\n",
      "[LOG 20191027-17:51:33] epoch: 547 train-loss: 0.000450325548172259\n",
      "[LOG 20191027-17:51:34] epoch: 548 train-loss: 0.0004483575999074674\n",
      "[LOG 20191027-17:51:34] epoch: 549 train-loss: 0.0004463762788873282\n",
      "[LOG 20191027-17:51:34] epoch: 550 train-loss: 0.00044438117947720457\n",
      "[LOG 20191027-17:51:35] epoch: 551 train-loss: 0.00044237242082090233\n",
      "[LOG 20191027-17:51:35] epoch: 552 train-loss: 0.00044035288237864734\n",
      "[LOG 20191027-17:51:35] epoch: 553 train-loss: 0.00043832115625264123\n",
      "[LOG 20191027-17:51:35] epoch: 554 train-loss: 0.0004362800214039453\n",
      "[LOG 20191027-17:51:36] epoch: 555 train-loss: 0.00043422970065876143\n",
      "[LOG 20191027-17:51:36] epoch: 556 train-loss: 0.000432169935265847\n",
      "[LOG 20191027-17:51:36] epoch: 557 train-loss: 0.0004301041544749751\n",
      "[LOG 20191027-17:51:37] epoch: 558 train-loss: 0.00042803132737390115\n",
      "[LOG 20191027-17:51:37] epoch: 559 train-loss: 0.0004259528341208352\n",
      "[LOG 20191027-17:51:37] epoch: 560 train-loss: 0.0004238705632815254\n",
      "[LOG 20191027-17:51:38] epoch: 561 train-loss: 0.00042178392368441564\n",
      "[LOG 20191027-17:51:38] epoch: 562 train-loss: 0.0004196948284516111\n",
      "[LOG 20191027-17:51:38] epoch: 563 train-loss: 0.00041760396106838016\n",
      "[LOG 20191027-17:51:38] epoch: 564 train-loss: 0.0004155113042543235\n",
      "[LOG 20191027-17:51:39] epoch: 565 train-loss: 0.00041341978476339136\n",
      "[LOG 20191027-17:51:39] epoch: 566 train-loss: 0.00041132887736239354\n",
      "[LOG 20191027-17:51:39] epoch: 567 train-loss: 0.0004092392746315454\n",
      "[LOG 20191027-17:51:40] epoch: 568 train-loss: 0.00040715316936257295\n",
      "[LOG 20191027-17:51:40] epoch: 569 train-loss: 0.0004050699826620985\n",
      "[LOG 20191027-17:51:40] epoch: 570 train-loss: 0.0004029910860481323\n",
      "[LOG 20191027-17:51:40] epoch: 570 new best train-loss: 0.0004029910860481323 found\n",
      "[LOG 20191027-17:51:40] epoch: 571 train-loss: 0.00040091768414640683\n",
      "[LOG 20191027-17:51:41] epoch: 572 train-loss: 0.00039885027308628196\n",
      "[LOG 20191027-17:51:41] epoch: 573 train-loss: 0.00039679034762230003\n",
      "[LOG 20191027-17:51:41] epoch: 574 train-loss: 0.0003947375730604108\n",
      "[LOG 20191027-17:51:42] epoch: 575 train-loss: 0.00039269295666599646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:51:42] epoch: 576 train-loss: 0.0003906591987288266\n",
      "[LOG 20191027-17:51:42] epoch: 577 train-loss: 0.00038863460076754563\n",
      "[LOG 20191027-17:51:42] epoch: 578 train-loss: 0.0003866209804073151\n",
      "[LOG 20191027-17:51:43] epoch: 579 train-loss: 0.00038461981057480443\n",
      "[LOG 20191027-17:51:43] epoch: 580 train-loss: 0.0003826300430773699\n",
      "[LOG 20191027-17:51:43] epoch: 580 new best train-loss: 0.0003826300430773699 found\n",
      "[LOG 20191027-17:51:43] epoch: 581 train-loss: 0.0003806537947639299\n",
      "[LOG 20191027-17:51:43] epoch: 582 train-loss: 0.00037869210700591793\n",
      "[LOG 20191027-17:51:44] epoch: 583 train-loss: 0.00037674395707654185\n",
      "[LOG 20191027-17:51:44] epoch: 584 train-loss: 0.00037481080880752415\n",
      "[LOG 20191027-17:51:44] epoch: 585 train-loss: 0.0003728941433109867\n",
      "[LOG 20191027-17:51:45] epoch: 586 train-loss: 0.0003709926249939599\n",
      "[LOG 20191027-17:51:45] epoch: 587 train-loss: 0.0003691078936753911\n",
      "[LOG 20191027-17:51:45] epoch: 588 train-loss: 0.00036724115125252865\n",
      "[LOG 20191027-17:51:45] epoch: 589 train-loss: 0.000365391717878083\n",
      "[LOG 20191027-17:51:46] epoch: 590 train-loss: 0.0003635609623415803\n",
      "[LOG 20191027-17:51:46] epoch: 590 new best train-loss: 0.0003635609623415803 found\n",
      "[LOG 20191027-17:51:46] epoch: 591 train-loss: 0.0003617491938712192\n",
      "[LOG 20191027-17:51:46] epoch: 592 train-loss: 0.00035995636017105426\n",
      "[LOG 20191027-17:51:46] epoch: 593 train-loss: 0.00035818396781905903\n",
      "[LOG 20191027-17:51:47] epoch: 594 train-loss: 0.00035643167666421505\n",
      "[LOG 20191027-17:51:47] epoch: 595 train-loss: 0.00035469903332341346\n",
      "[LOG 20191027-17:51:47] epoch: 596 train-loss: 0.0003529879818415793\n",
      "[LOG 20191027-17:51:47] epoch: 597 train-loss: 0.00035129872367178905\n",
      "[LOG 20191027-17:51:48] epoch: 598 train-loss: 0.0003496301833365578\n",
      "[LOG 20191027-17:51:48] epoch: 599 train-loss: 0.0003479840879663243\n",
      "[LOG 20191027-17:51:48] epoch: 600 train-loss: 0.0003463607095000043\n",
      "[LOG 20191027-17:51:48] epoch: 600 new best train-loss: 0.0003463607095000043 found\n",
      "[LOG 20191027-17:51:48] epoch: 601 train-loss: 0.00034475927463972766\n",
      "[LOG 20191027-17:51:49] epoch: 602 train-loss: 0.0003431804054798704\n",
      "[LOG 20191027-17:51:49] epoch: 603 train-loss: 0.0003416253045998019\n",
      "[LOG 20191027-17:51:49] epoch: 604 train-loss: 0.0003400928524115443\n",
      "[LOG 20191027-17:51:50] epoch: 605 train-loss: 0.00033858387996588135\n",
      "[LOG 20191027-17:51:50] epoch: 606 train-loss: 0.00033709867534525984\n",
      "[LOG 20191027-17:51:50] epoch: 607 train-loss: 0.00033563711258466356\n",
      "[LOG 20191027-17:51:50] epoch: 608 train-loss: 0.0003341996566632588\n",
      "[LOG 20191027-17:51:51] epoch: 609 train-loss: 0.00033278711975981423\n",
      "[LOG 20191027-17:51:51] epoch: 610 train-loss: 0.0003313982147119532\n",
      "[LOG 20191027-17:51:51] epoch: 610 new best train-loss: 0.0003313982147119532 found\n",
      "[LOG 20191027-17:51:51] epoch: 611 train-loss: 0.00033003306839418656\n",
      "[LOG 20191027-17:51:51] epoch: 612 train-loss: 0.00032869346864572435\n",
      "[LOG 20191027-17:51:52] epoch: 613 train-loss: 0.00032737820561123954\n",
      "[LOG 20191027-17:51:52] epoch: 614 train-loss: 0.00032608687070023734\n",
      "[LOG 20191027-17:51:52] epoch: 615 train-loss: 0.00032482064170835656\n",
      "[LOG 20191027-17:51:52] epoch: 616 train-loss: 0.0003235793947169441\n",
      "[LOG 20191027-17:51:53] epoch: 617 train-loss: 0.00032236250035566627\n",
      "[LOG 20191027-17:51:53] epoch: 618 train-loss: 0.0003211705775356677\n",
      "[LOG 20191027-17:51:53] epoch: 619 train-loss: 0.00032000345504457073\n",
      "[LOG 20191027-17:51:53] epoch: 620 train-loss: 0.0003188609189237468\n",
      "[LOG 20191027-17:51:53] epoch: 620 new best train-loss: 0.0003188609189237468 found\n",
      "[LOG 20191027-17:51:54] epoch: 621 train-loss: 0.0003177428995968512\n",
      "[LOG 20191027-17:51:54] epoch: 622 train-loss: 0.00031665020469517913\n",
      "[LOG 20191027-17:51:54] epoch: 623 train-loss: 0.0003155818978939351\n",
      "[LOG 20191027-17:51:55] epoch: 624 train-loss: 0.0003145378402678034\n",
      "[LOG 20191027-17:51:55] epoch: 625 train-loss: 0.00031351903089671396\n",
      "[LOG 20191027-17:51:55] epoch: 626 train-loss: 0.0003125247433217737\n",
      "[LOG 20191027-17:51:55] epoch: 627 train-loss: 0.0003115545946457132\n",
      "[LOG 20191027-17:51:56] epoch: 628 train-loss: 0.0003106088811364316\n",
      "[LOG 20191027-17:51:56] epoch: 629 train-loss: 0.0003096880864177365\n",
      "[LOG 20191027-17:51:56] epoch: 630 train-loss: 0.0003087911811689992\n",
      "[LOG 20191027-17:51:56] epoch: 630 new best train-loss: 0.0003087911811689992 found\n",
      "[LOG 20191027-17:51:56] epoch: 631 train-loss: 0.0003079183049976564\n",
      "[LOG 20191027-17:51:57] epoch: 632 train-loss: 0.0003070695493079256\n",
      "[LOG 20191027-17:51:57] epoch: 633 train-loss: 0.00030624478699792235\n",
      "[LOG 20191027-17:51:57] epoch: 634 train-loss: 0.00030544371702490025\n",
      "[LOG 20191027-17:51:57] epoch: 635 train-loss: 0.00030466663019979023\n",
      "[LOG 20191027-17:51:58] epoch: 636 train-loss: 0.0003039133921447501\n",
      "[LOG 20191027-17:51:58] epoch: 637 train-loss: 0.00030318328708744957\n",
      "[LOG 20191027-17:51:58] epoch: 638 train-loss: 0.0003024763532266661\n",
      "[LOG 20191027-17:51:58] epoch: 639 train-loss: 0.0003017929489033122\n",
      "[LOG 20191027-17:51:59] epoch: 640 train-loss: 0.0003011327787589835\n",
      "[LOG 20191027-17:51:59] epoch: 640 new best train-loss: 0.0003011327787589835 found\n",
      "[LOG 20191027-17:51:59] epoch: 641 train-loss: 0.0003004952855008014\n",
      "[LOG 20191027-17:51:59] epoch: 642 train-loss: 0.00029988034611960757\n",
      "[LOG 20191027-17:51:59] epoch: 643 train-loss: 0.0002992883862589224\n",
      "[LOG 20191027-17:52:00] epoch: 644 train-loss: 0.0002987188622682879\n",
      "[LOG 20191027-17:52:00] epoch: 645 train-loss: 0.00029817157565048547\n",
      "[LOG 20191027-17:52:00] epoch: 646 train-loss: 0.0002976465470965195\n",
      "[LOG 20191027-17:52:01] epoch: 647 train-loss: 0.0002971437795622478\n",
      "[LOG 20191027-17:52:01] epoch: 648 train-loss: 0.00029666255204574554\n",
      "[LOG 20191027-17:52:01] epoch: 649 train-loss: 0.0002962028831916541\n",
      "[LOG 20191027-17:52:01] epoch: 650 train-loss: 0.0002957649055588263\n",
      "[LOG 20191027-17:52:01] epoch: 650 new best train-loss: 0.0002957649055588263 found\n",
      "[LOG 20191027-17:52:02] epoch: 651 train-loss: 0.0002953482671728125\n",
      "[LOG 20191027-17:52:02] epoch: 652 train-loss: 0.0002949526754036924\n",
      "[LOG 20191027-17:52:02] epoch: 653 train-loss: 0.0002945778667253762\n",
      "[LOG 20191027-17:52:02] epoch: 654 train-loss: 0.0002942239495951071\n",
      "[LOG 20191027-17:52:03] epoch: 655 train-loss: 0.00029389065866780584\n",
      "[LOG 20191027-17:52:03] epoch: 656 train-loss: 0.00029357768858062627\n",
      "[LOG 20191027-17:52:03] epoch: 657 train-loss: 0.00029328484720281267\n",
      "[LOG 20191027-17:52:03] epoch: 658 train-loss: 0.0002930120701876149\n",
      "[LOG 20191027-17:52:04] epoch: 659 train-loss: 0.00029275906717884936\n",
      "[LOG 20191027-17:52:04] epoch: 660 train-loss: 0.00029252559806991485\n",
      "[LOG 20191027-17:52:04] epoch: 660 new best train-loss: 0.00029252559806991485 found\n",
      "[LOG 20191027-17:52:04] epoch: 661 train-loss: 0.00029231141820673656\n",
      "[LOG 20191027-17:52:04] epoch: 662 train-loss: 0.0002921165080351784\n",
      "[LOG 20191027-17:52:05] epoch: 663 train-loss: 0.00029194053649916896\n",
      "[LOG 20191027-17:52:05] epoch: 664 train-loss: 0.00029178325848988607\n",
      "[LOG 20191027-17:52:05] epoch: 665 train-loss: 0.0002916444659604167\n",
      "[LOG 20191027-17:52:06] epoch: 666 train-loss: 0.0002915240531820018\n",
      "[LOG 20191027-17:52:06] epoch: 667 train-loss: 0.0002914217143370479\n",
      "[LOG 20191027-17:52:06] epoch: 668 train-loss: 0.00029133721432117454\n",
      "[LOG 20191027-17:52:06] epoch: 669 train-loss: 0.0002912703353104007\n",
      "[LOG 20191027-17:52:07] epoch: 670 train-loss: 0.0002912209010901279\n",
      "[LOG 20191027-17:52:07] epoch: 670 new best train-loss: 0.0002912209010901279 found\n",
      "[LOG 20191027-17:52:07] epoch: 671 train-loss: 0.0002911886467700242\n",
      "[LOG 20191027-17:52:07] epoch: 672 train-loss: 0.00029117334838701936\n",
      "[LOG 20191027-17:52:08] epoch: 673 train-loss: 0.0002911747451435076\n",
      "[LOG 20191027-17:52:08] epoch: 674 train-loss: 0.0002911926542310539\n",
      "[LOG 20191027-17:52:08] epoch: 675 train-loss: 0.0002912268046202371\n",
      "[LOG 20191027-17:52:09] epoch: 676 train-loss: 0.0002912769416525407\n",
      "[LOG 20191027-17:52:09] epoch: 677 train-loss: 0.0002913429486852692\n",
      "[LOG 20191027-17:52:09] epoch: 678 train-loss: 0.0002914244726071047\n",
      "[LOG 20191027-17:52:10] epoch: 679 train-loss: 0.0002915212126026745\n",
      "[LOG 20191027-17:52:10] epoch: 680 train-loss: 0.0002916330404332257\n",
      "[LOG 20191027-17:52:10] epoch: 681 train-loss: 0.00029175965892136446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:52:11] epoch: 682 train-loss: 0.00029190092868702777\n",
      "[LOG 20191027-17:52:11] epoch: 683 train-loss: 0.00029205644568719435\n",
      "[LOG 20191027-17:52:11] epoch: 684 train-loss: 0.00029222603825473925\n",
      "[LOG 20191027-17:52:12] epoch: 685 train-loss: 0.0002924094521858933\n",
      "[LOG 20191027-17:52:12] epoch: 686 train-loss: 0.00029260647261253325\n",
      "[LOG 20191027-17:52:12] epoch: 687 train-loss: 0.0002928167027675954\n",
      "[LOG 20191027-17:52:13] epoch: 688 train-loss: 0.00029303983774298104\n",
      "[LOG 20191027-17:52:13] epoch: 689 train-loss: 0.0002932759080067626\n",
      "[LOG 20191027-17:52:13] epoch: 690 train-loss: 0.00029352471392485313\n",
      "[LOG 20191027-17:52:14] epoch: 691 train-loss: 0.0002937857211691153\n",
      "[LOG 20191027-17:52:14] epoch: 692 train-loss: 0.0002940585127362283\n",
      "[LOG 20191027-17:52:14] epoch: 693 train-loss: 0.0002943430949926551\n",
      "[LOG 20191027-17:52:15] epoch: 694 train-loss: 0.00029463931809914357\n",
      "[LOG 20191027-17:52:15] epoch: 695 train-loss: 0.0002949469683244388\n",
      "[LOG 20191027-17:52:15] epoch: 696 train-loss: 0.0002952655368062551\n",
      "[LOG 20191027-17:52:15] epoch: 697 train-loss: 0.00029559490985775483\n",
      "[LOG 20191027-17:52:16] epoch: 698 train-loss: 0.00029593473004752013\n",
      "[LOG 20191027-17:52:16] epoch: 699 train-loss: 0.0002962846006084874\n",
      "[LOG 20191027-17:52:16] epoch: 700 train-loss: 0.0002966445008496521\n",
      "[LOG 20191027-17:52:17] epoch: 701 train-loss: 0.0002970138077671436\n",
      "[LOG 20191027-17:52:17] epoch: 702 train-loss: 0.00029739264073214144\n",
      "[LOG 20191027-17:52:17] epoch: 703 train-loss: 0.0002977808048854058\n",
      "[LOG 20191027-17:52:18] epoch: 704 train-loss: 0.0002981780594382144\n",
      "[LOG 20191027-17:52:18] epoch: 705 train-loss: 0.00029858387620151916\n",
      "[LOG 20191027-17:52:18] epoch: 706 train-loss: 0.0002989977085690043\n",
      "[LOG 20191027-17:52:19] epoch: 707 train-loss: 0.0002994198164287809\n",
      "[LOG 20191027-17:52:19] epoch: 708 train-loss: 0.00029984982347741607\n",
      "[LOG 20191027-17:52:19] epoch: 709 train-loss: 0.0003002875521360693\n",
      "[LOG 20191027-17:52:20] epoch: 710 train-loss: 0.0003007325335602218\n",
      "[LOG 20191027-17:52:20] epoch: 711 train-loss: 0.0003011843325566588\n",
      "[LOG 20191027-17:52:20] epoch: 712 train-loss: 0.00030164316035552474\n",
      "[LOG 20191027-17:52:21] epoch: 713 train-loss: 0.0003021088991772558\n",
      "[LOG 20191027-17:52:21] epoch: 714 train-loss: 0.0003025810995040956\n",
      "[LOG 20191027-17:52:21] epoch: 715 train-loss: 0.000303059057387145\n",
      "[LOG 20191027-17:52:21] epoch: 716 train-loss: 0.00030354268687915464\n",
      "[LOG 20191027-17:52:22] epoch: 717 train-loss: 0.00030403226378439285\n",
      "[LOG 20191027-17:52:22] epoch: 718 train-loss: 0.00030452747569142957\n",
      "[LOG 20191027-17:52:22] epoch: 719 train-loss: 0.0003050279772196518\n",
      "[LOG 20191027-17:52:23] epoch: 720 train-loss: 0.0003055331389987259\n",
      "[LOG 20191027-17:52:23] epoch: 721 train-loss: 0.00030604282733293076\n",
      "[LOG 20191027-17:52:23] epoch: 722 train-loss: 0.0003065573694129853\n",
      "[LOG 20191027-17:52:24] epoch: 723 train-loss: 0.00030707663768225757\n",
      "[LOG 20191027-17:52:24] epoch: 724 train-loss: 0.0003076001080444257\n",
      "[LOG 20191027-17:52:24] epoch: 725 train-loss: 0.0003081272141116642\n",
      "[LOG 20191027-17:52:24] epoch: 726 train-loss: 0.0003086577607973595\n",
      "[LOG 20191027-17:52:25] epoch: 727 train-loss: 0.0003091921271334286\n",
      "[LOG 20191027-17:52:25] epoch: 728 train-loss: 0.00030973011371315806\n",
      "[LOG 20191027-17:52:25] epoch: 729 train-loss: 0.00031027135855765664\n",
      "[LOG 20191027-17:52:26] epoch: 730 train-loss: 0.0003108153257471713\n",
      "[LOG 20191027-17:52:26] epoch: 731 train-loss: 0.00031136210145632504\n",
      "[LOG 20191027-17:52:26] epoch: 732 train-loss: 0.0003119117914138769\n",
      "[LOG 20191027-17:52:27] epoch: 733 train-loss: 0.00031246401522366796\n",
      "[LOG 20191027-17:52:27] epoch: 734 train-loss: 0.0003130186537418922\n",
      "[LOG 20191027-17:52:27] epoch: 735 train-loss: 0.000313575034169844\n",
      "[LOG 20191027-17:52:28] epoch: 736 train-loss: 0.0003141330739708792\n",
      "[LOG 20191027-17:52:28] epoch: 737 train-loss: 0.0003146930914681434\n",
      "[LOG 20191027-17:52:28] epoch: 738 train-loss: 0.00031525516783403873\n",
      "[LOG 20191027-17:52:29] epoch: 739 train-loss: 0.0003158191170768987\n",
      "[LOG 20191027-17:52:29] epoch: 740 train-loss: 0.0003163837964166305\n",
      "[LOG 20191027-17:52:29] epoch: 741 train-loss: 0.0003169493784298538\n",
      "[LOG 20191027-17:52:30] epoch: 742 train-loss: 0.0003175160416049039\n",
      "[LOG 20191027-17:52:30] epoch: 743 train-loss: 0.00031808350649953354\n",
      "[LOG 20191027-17:52:30] epoch: 744 train-loss: 0.00031865165874478407\n",
      "[LOG 20191027-17:52:31] epoch: 745 train-loss: 0.00031922071161716303\n",
      "[LOG 20191027-17:52:31] epoch: 746 train-loss: 0.0003197895459834399\n",
      "[LOG 20191027-17:52:31] epoch: 747 train-loss: 0.0003203584510629298\n",
      "[LOG 20191027-17:52:31] epoch: 748 train-loss: 0.0003209275480458018\n",
      "[LOG 20191027-17:52:32] epoch: 749 train-loss: 0.00032149688149729627\n",
      "[LOG 20191027-17:52:32] epoch: 750 train-loss: 0.0003220657447400299\n",
      "[LOG 20191027-17:52:32] epoch: 751 train-loss: 0.00032263407024402113\n",
      "[LOG 20191027-17:52:33] epoch: 752 train-loss: 0.00032320083096237795\n",
      "[LOG 20191027-17:52:33] epoch: 753 train-loss: 0.0003237668424844742\n",
      "[LOG 20191027-17:52:33] epoch: 754 train-loss: 0.00032433194905934215\n",
      "[LOG 20191027-17:52:34] epoch: 755 train-loss: 0.00032489629143128695\n",
      "[LOG 20191027-17:52:34] epoch: 756 train-loss: 0.0003254595474118105\n",
      "[LOG 20191027-17:52:34] epoch: 757 train-loss: 0.0003260206110553554\n",
      "[LOG 20191027-17:52:34] epoch: 758 train-loss: 0.00032657973815730656\n",
      "[LOG 20191027-17:52:35] epoch: 759 train-loss: 0.00032713752989366185\n",
      "[LOG 20191027-17:52:35] epoch: 760 train-loss: 0.00032769356607786904\n",
      "[LOG 20191027-17:52:35] epoch: 761 train-loss: 0.0003282472812315973\n",
      "[LOG 20191027-17:52:36] epoch: 762 train-loss: 0.00032879910759220365\n",
      "[LOG 20191027-17:52:36] epoch: 763 train-loss: 0.0003293476645467308\n",
      "[LOG 20191027-17:52:36] epoch: 764 train-loss: 0.0003298938572697807\n",
      "[LOG 20191027-17:52:36] epoch: 765 train-loss: 0.0003304377901258704\n",
      "[LOG 20191027-17:52:37] epoch: 766 train-loss: 0.00033097971277129545\n",
      "[LOG 20191027-17:52:37] epoch: 767 train-loss: 0.0003315189294426091\n",
      "[LOG 20191027-17:52:37] epoch: 768 train-loss: 0.00033205524937329756\n",
      "[LOG 20191027-17:52:37] epoch: 769 train-loss: 0.0003325884722471528\n",
      "[LOG 20191027-17:52:38] epoch: 770 train-loss: 0.000333118635808205\n",
      "[LOG 20191027-17:52:38] epoch: 771 train-loss: 0.0003336461734306795\n",
      "[LOG 20191027-17:52:38] epoch: 772 train-loss: 0.0003341714341331681\n",
      "[LOG 20191027-17:52:39] epoch: 773 train-loss: 0.000334693948389031\n",
      "[LOG 20191027-17:52:39] epoch: 774 train-loss: 0.00033521263071634166\n",
      "[LOG 20191027-17:52:39] epoch: 775 train-loss: 0.00033572794268366124\n",
      "[LOG 20191027-17:52:40] epoch: 776 train-loss: 0.0003362404074778169\n",
      "[LOG 20191027-17:52:40] epoch: 777 train-loss: 0.0003367499325577228\n",
      "[LOG 20191027-17:52:40] epoch: 778 train-loss: 0.00033725685034369235\n",
      "[LOG 20191027-17:52:40] epoch: 779 train-loss: 0.00033776050099731947\n",
      "[LOG 20191027-17:52:41] epoch: 780 train-loss: 0.00033826026060523873\n",
      "[LOG 20191027-17:52:41] epoch: 781 train-loss: 0.000338756393375661\n",
      "[LOG 20191027-17:52:41] epoch: 782 train-loss: 0.00033924916601790756\n",
      "[LOG 20191027-17:52:41] epoch: 783 train-loss: 0.00033973880681514856\n",
      "[LOG 20191027-17:52:42] epoch: 784 train-loss: 0.00034022463205474196\n",
      "[LOG 20191027-17:52:42] epoch: 785 train-loss: 0.0003407071460514999\n",
      "[LOG 20191027-17:52:42] epoch: 786 train-loss: 0.0003411854552268778\n",
      "[LOG 20191027-17:52:42] epoch: 787 train-loss: 0.0003416590584492951\n",
      "[LOG 20191027-17:52:43] epoch: 788 train-loss: 0.00034212890750495717\n",
      "[LOG 20191027-17:52:43] epoch: 789 train-loss: 0.00034259519543411443\n",
      "[LOG 20191027-17:52:43] epoch: 790 train-loss: 0.000343057087093257\n",
      "[LOG 20191027-17:52:43] epoch: 791 train-loss: 0.0003435148560129164\n",
      "[LOG 20191027-17:52:44] epoch: 792 train-loss: 0.0003439681893269153\n",
      "[LOG 20191027-17:52:44] epoch: 793 train-loss: 0.0003444153367127001\n",
      "[LOG 20191027-17:52:44] epoch: 794 train-loss: 0.00034485776313886163\n",
      "[LOG 20191027-17:52:45] epoch: 795 train-loss: 0.0003452953328633157\n",
      "[LOG 20191027-17:52:45] epoch: 796 train-loss: 0.0003457277209690801\n",
      "[LOG 20191027-17:52:45] epoch: 797 train-loss: 0.0003461546482412814\n",
      "[LOG 20191027-17:52:45] epoch: 798 train-loss: 0.00034657630362744385\n",
      "[LOG 20191027-17:52:46] epoch: 799 train-loss: 0.0003469911191587016\n",
      "[LOG 20191027-17:52:46] epoch: 800 train-loss: 0.00034740006753963826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:52:46] epoch: 801 train-loss: 0.0003478033245301049\n",
      "[LOG 20191027-17:52:46] epoch: 802 train-loss: 0.000348200170265045\n",
      "[LOG 20191027-17:52:47] epoch: 803 train-loss: 0.0003485908398488391\n",
      "[LOG 20191027-17:52:47] epoch: 804 train-loss: 0.0003489747703042667\n",
      "[LOG 20191027-17:52:47] epoch: 805 train-loss: 0.0003493509643703874\n",
      "[LOG 20191027-17:52:47] epoch: 806 train-loss: 0.0003497193411021726\n",
      "[LOG 20191027-17:52:48] epoch: 807 train-loss: 0.0003500807156342489\n",
      "[LOG 20191027-17:52:48] epoch: 808 train-loss: 0.0003504348096612375\n",
      "[LOG 20191027-17:52:48] epoch: 809 train-loss: 0.00035078187011094997\n",
      "[LOG 20191027-17:52:48] epoch: 810 train-loss: 0.00035112120895064436\n",
      "[LOG 20191027-17:52:49] epoch: 811 train-loss: 0.0003514526019898767\n",
      "[LOG 20191027-17:52:49] epoch: 812 train-loss: 0.0003517748264130205\n",
      "[LOG 20191027-17:52:49] epoch: 813 train-loss: 0.0003520891600601317\n",
      "[LOG 20191027-17:52:50] epoch: 814 train-loss: 0.00035239515636931174\n",
      "[LOG 20191027-17:52:50] epoch: 815 train-loss: 0.00035269262434667326\n",
      "[LOG 20191027-17:52:50] epoch: 816 train-loss: 0.000352981747710146\n",
      "[LOG 20191027-17:52:50] epoch: 817 train-loss: 0.00035326228817211813\n",
      "[LOG 20191027-17:52:51] epoch: 818 train-loss: 0.00035353322982700774\n",
      "[LOG 20191027-17:52:51] epoch: 819 train-loss: 0.00035379443079364137\n",
      "[LOG 20191027-17:52:51] epoch: 820 train-loss: 0.00035404703794483794\n",
      "[LOG 20191027-17:52:51] epoch: 821 train-loss: 0.0003542906156326353\n",
      "[LOG 20191027-17:52:52] epoch: 822 train-loss: 0.0003545253034644702\n",
      "[LOG 20191027-17:52:52] epoch: 823 train-loss: 0.0003547507840266917\n",
      "[LOG 20191027-17:52:52] epoch: 824 train-loss: 0.0003549667421793856\n",
      "[LOG 20191027-17:52:52] epoch: 825 train-loss: 0.0003551718464223086\n",
      "[LOG 20191027-17:52:53] epoch: 826 train-loss: 0.0003553671349436627\n",
      "[LOG 20191027-17:52:53] epoch: 827 train-loss: 0.00035555304339141003\n",
      "[LOG 20191027-17:52:53] epoch: 828 train-loss: 0.0003557295044629427\n",
      "[LOG 20191027-17:52:53] epoch: 829 train-loss: 0.00035589629578680615\n",
      "[LOG 20191027-17:52:54] epoch: 830 train-loss: 0.0003560534805728821\n",
      "[LOG 20191027-17:52:54] epoch: 831 train-loss: 0.0003562005299500015\n",
      "[LOG 20191027-17:52:54] epoch: 832 train-loss: 0.00035633621973829577\n",
      "[LOG 20191027-17:52:54] epoch: 833 train-loss: 0.00035646133892441867\n",
      "[LOG 20191027-17:52:55] epoch: 834 train-loss: 0.00035657638181874063\n",
      "[LOG 20191027-17:52:55] epoch: 835 train-loss: 0.0003566809896256018\n",
      "[LOG 20191027-17:52:55] epoch: 836 train-loss: 0.0003567751382433926\n",
      "[LOG 20191027-17:52:55] epoch: 837 train-loss: 0.00035685886859937455\n",
      "[LOG 20191027-17:52:56] epoch: 838 train-loss: 0.00035693172003448126\n",
      "[LOG 20191027-17:52:56] epoch: 839 train-loss: 0.00035699287263923907\n",
      "[LOG 20191027-17:52:56] epoch: 840 train-loss: 0.0003570423855308036\n",
      "[LOG 20191027-17:52:57] epoch: 841 train-loss: 0.00035708089262698195\n",
      "[LOG 20191027-17:52:57] epoch: 842 train-loss: 0.00035710857355297776\n",
      "[LOG 20191027-17:52:57] epoch: 843 train-loss: 0.00035712513363250764\n",
      "[LOG 20191027-17:52:57] epoch: 844 train-loss: 0.0003571303736862319\n",
      "[LOG 20191027-17:52:58] epoch: 845 train-loss: 0.0003571244637896598\n",
      "[LOG 20191027-17:52:58] epoch: 846 train-loss: 0.00035710713382286485\n",
      "[LOG 20191027-17:52:58] epoch: 847 train-loss: 0.0003570771327758848\n",
      "[LOG 20191027-17:52:58] epoch: 848 train-loss: 0.0003570352446331526\n",
      "[LOG 20191027-17:52:59] epoch: 849 train-loss: 0.00035698170358955394\n",
      "[LOG 20191027-17:52:59] epoch: 850 train-loss: 0.00035691706852958305\n",
      "[LOG 20191027-17:52:59] epoch: 851 train-loss: 0.0003568410543266509\n",
      "[LOG 20191027-17:52:59] epoch: 852 train-loss: 0.00035675375283972244\n",
      "[LOG 20191027-17:53:00] epoch: 853 train-loss: 0.00035665496352521586\n",
      "[LOG 20191027-17:53:00] epoch: 854 train-loss: 0.0003565450197129394\n",
      "[LOG 20191027-17:53:00] epoch: 855 train-loss: 0.0003564225394256937\n",
      "[LOG 20191027-17:53:00] epoch: 856 train-loss: 0.0003562889769455069\n",
      "[LOG 20191027-17:53:01] epoch: 857 train-loss: 0.00035614423768493\n",
      "[LOG 20191027-17:53:01] epoch: 858 train-loss: 0.00035598850718088215\n",
      "[LOG 20191027-17:53:01] epoch: 859 train-loss: 0.00035582206282924744\n",
      "[LOG 20191027-17:53:01] epoch: 860 train-loss: 0.00035564544532462605\n",
      "[LOG 20191027-17:53:02] epoch: 861 train-loss: 0.00035545831406125217\n",
      "[LOG 20191027-17:53:02] epoch: 862 train-loss: 0.0003552606872290198\n",
      "[LOG 20191027-17:53:02] epoch: 863 train-loss: 0.00035505306186678354\n",
      "[LOG 20191027-17:53:02] epoch: 864 train-loss: 0.0003548345871422498\n",
      "[LOG 20191027-17:53:03] epoch: 865 train-loss: 0.0003546063085195783\n",
      "[LOG 20191027-17:53:03] epoch: 866 train-loss: 0.00035436907683106256\n",
      "[LOG 20191027-17:53:03] epoch: 867 train-loss: 0.00035412304760029656\n",
      "[LOG 20191027-17:53:04] epoch: 868 train-loss: 0.00035386815579840913\n",
      "[LOG 20191027-17:53:04] epoch: 869 train-loss: 0.0003536051003720786\n",
      "[LOG 20191027-17:53:04] epoch: 870 train-loss: 0.0003533339058776619\n",
      "[LOG 20191027-17:53:04] epoch: 871 train-loss: 0.0003530546891852282\n",
      "[LOG 20191027-17:53:05] epoch: 872 train-loss: 0.0003527671715346514\n",
      "[LOG 20191027-17:53:05] epoch: 873 train-loss: 0.00035247230380264227\n",
      "[LOG 20191027-17:53:05] epoch: 874 train-loss: 0.0003521705530147301\n",
      "[LOG 20191027-17:53:06] epoch: 875 train-loss: 0.0003518620719660248\n",
      "[LOG 20191027-17:53:06] epoch: 876 train-loss: 0.0003515470293677936\n",
      "[LOG 20191027-17:53:06] epoch: 877 train-loss: 0.000351225388385501\n",
      "[LOG 20191027-17:53:06] epoch: 878 train-loss: 0.0003508972920371889\n",
      "[LOG 20191027-17:53:07] epoch: 879 train-loss: 0.0003505630079416733\n",
      "[LOG 20191027-17:53:07] epoch: 880 train-loss: 0.00035022324459532683\n",
      "[LOG 20191027-17:53:07] epoch: 881 train-loss: 0.0003498786786622077\n",
      "[LOG 20191027-17:53:08] epoch: 882 train-loss: 0.00034952895316564536\n",
      "[LOG 20191027-17:53:08] epoch: 883 train-loss: 0.00034917431844405655\n",
      "[LOG 20191027-17:53:08] epoch: 884 train-loss: 0.00034881462738667324\n",
      "[LOG 20191027-17:53:09] epoch: 885 train-loss: 0.0003484504875359562\n",
      "[LOG 20191027-17:53:09] epoch: 886 train-loss: 0.00034808240343409125\n",
      "[LOG 20191027-17:53:09] epoch: 887 train-loss: 0.00034771051991810964\n",
      "[LOG 20191027-17:53:09] epoch: 888 train-loss: 0.0003473352587661793\n",
      "[LOG 20191027-17:53:10] epoch: 889 train-loss: 0.00034695697149800253\n",
      "[LOG 20191027-17:53:10] epoch: 890 train-loss: 0.0003465752743068151\n",
      "[LOG 20191027-17:53:10] epoch: 891 train-loss: 0.00034619075813679956\n",
      "[LOG 20191027-17:53:11] epoch: 892 train-loss: 0.0003458029575540422\n",
      "[LOG 20191027-17:53:11] epoch: 893 train-loss: 0.00034541281365818577\n",
      "[LOG 20191027-17:53:11] epoch: 894 train-loss: 0.00034502053517826425\n",
      "[LOG 20191027-17:53:11] epoch: 895 train-loss: 0.00034462633630027995\n",
      "[LOG 20191027-17:53:12] epoch: 896 train-loss: 0.0003442304821419384\n",
      "[LOG 20191027-17:53:12] epoch: 897 train-loss: 0.00034383337538201886\n",
      "[LOG 20191027-17:53:12] epoch: 898 train-loss: 0.00034343469883424405\n",
      "[LOG 20191027-17:53:13] epoch: 899 train-loss: 0.0003430347289850033\n",
      "[LOG 20191027-17:53:13] epoch: 900 train-loss: 0.00034263409565937764\n",
      "[LOG 20191027-17:53:13] epoch: 901 train-loss: 0.00034223288730572676\n",
      "[LOG 20191027-17:53:13] epoch: 902 train-loss: 0.000341831112109503\n",
      "[LOG 20191027-17:53:14] epoch: 903 train-loss: 0.00034142868048547825\n",
      "[LOG 20191027-17:53:14] epoch: 904 train-loss: 0.0003410255471862911\n",
      "[LOG 20191027-17:53:14] epoch: 905 train-loss: 0.0003406223379442963\n",
      "[LOG 20191027-17:53:15] epoch: 906 train-loss: 0.00034021922124338744\n",
      "[LOG 20191027-17:53:15] epoch: 907 train-loss: 0.00033981616206801846\n",
      "[LOG 20191027-17:53:15] epoch: 908 train-loss: 0.0003394134248537739\n",
      "[LOG 20191027-17:53:16] epoch: 909 train-loss: 0.0003390112212855456\n",
      "[LOG 20191027-17:53:16] epoch: 910 train-loss: 0.00033860941675811773\n",
      "[LOG 20191027-17:53:16] epoch: 911 train-loss: 0.0003382086017609254\n",
      "[LOG 20191027-17:53:17] epoch: 912 train-loss: 0.0003378084443284024\n",
      "[LOG 20191027-17:53:17] epoch: 913 train-loss: 0.00033740915887392475\n",
      "[LOG 20191027-17:53:17] epoch: 914 train-loss: 0.000337010933435522\n",
      "[LOG 20191027-17:53:18] epoch: 915 train-loss: 0.00033661394536466105\n",
      "[LOG 20191027-17:53:18] epoch: 916 train-loss: 0.0003362183415447362\n",
      "[LOG 20191027-17:53:18] epoch: 917 train-loss: 0.00033582402511456166\n",
      "[LOG 20191027-17:53:18] epoch: 918 train-loss: 0.0003354310413214989\n",
      "[LOG 20191027-17:53:19] epoch: 919 train-loss: 0.00033503947088320274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:53:19] epoch: 920 train-loss: 0.0003346494813740719\n",
      "[LOG 20191027-17:53:19] epoch: 921 train-loss: 0.000334261320404039\n",
      "[LOG 20191027-17:53:20] epoch: 922 train-loss: 0.000333874806528911\n",
      "[LOG 20191027-17:53:20] epoch: 923 train-loss: 0.0003334901471134799\n",
      "[LOG 20191027-17:53:20] epoch: 924 train-loss: 0.0003331072630317067\n",
      "[LOG 20191027-17:53:21] epoch: 925 train-loss: 0.00033272687687713187\n",
      "[LOG 20191027-17:53:21] epoch: 926 train-loss: 0.00033234869647458254\n",
      "[LOG 20191027-17:53:21] epoch: 927 train-loss: 0.00033197293510056625\n",
      "[LOG 20191027-17:53:21] epoch: 928 train-loss: 0.0003315993390060612\n",
      "[LOG 20191027-17:53:22] epoch: 929 train-loss: 0.0003312281926355354\n",
      "[LOG 20191027-17:53:22] epoch: 930 train-loss: 0.00033085945460697985\n",
      "[LOG 20191027-17:53:22] epoch: 931 train-loss: 0.0003304933277377131\n",
      "[LOG 20191027-17:53:23] epoch: 932 train-loss: 0.00033012992253134144\n",
      "[LOG 20191027-17:53:23] epoch: 933 train-loss: 0.00032976941270135285\n",
      "[LOG 20191027-17:53:23] epoch: 934 train-loss: 0.0003294122625447926\n",
      "[LOG 20191027-17:53:24] epoch: 935 train-loss: 0.0003290583990747109\n",
      "[LOG 20191027-17:53:24] epoch: 936 train-loss: 0.0003287078272933286\n",
      "[LOG 20191027-17:53:24] epoch: 937 train-loss: 0.000328360132925809\n",
      "[LOG 20191027-17:53:24] epoch: 938 train-loss: 0.00032801578367980255\n",
      "[LOG 20191027-17:53:25] epoch: 939 train-loss: 0.0003276749323504191\n",
      "[LOG 20191027-17:53:25] epoch: 940 train-loss: 0.0003273377863024507\n",
      "[LOG 20191027-17:53:25] epoch: 941 train-loss: 0.0003270041049745487\n",
      "[LOG 20191027-17:53:26] epoch: 942 train-loss: 0.00032667370533090434\n",
      "[LOG 20191027-17:53:26] epoch: 943 train-loss: 0.0003263469211560732\n",
      "[LOG 20191027-17:53:26] epoch: 944 train-loss: 0.000326023709249057\n",
      "[LOG 20191027-17:53:27] epoch: 945 train-loss: 0.0003257043088069622\n",
      "[LOG 20191027-17:53:27] epoch: 946 train-loss: 0.00032538882874177943\n",
      "[LOG 20191027-17:53:27] epoch: 947 train-loss: 0.00032507746436749585\n",
      "[LOG 20191027-17:53:28] epoch: 948 train-loss: 0.00032476992510055425\n",
      "[LOG 20191027-17:53:28] epoch: 949 train-loss: 0.00032446637828797975\n",
      "[LOG 20191027-17:53:28] epoch: 950 train-loss: 0.0003241668418922927\n",
      "[LOG 20191027-17:53:29] epoch: 951 train-loss: 0.00032387113833465264\n",
      "[LOG 20191027-17:53:29] epoch: 952 train-loss: 0.0003235791759834683\n",
      "[LOG 20191027-17:53:29] epoch: 953 train-loss: 0.0003232914179989166\n",
      "[LOG 20191027-17:53:30] epoch: 954 train-loss: 0.0003230079337299685\n",
      "[LOG 20191027-17:53:30] epoch: 955 train-loss: 0.00032272872954308696\n",
      "[LOG 20191027-17:53:30] epoch: 956 train-loss: 0.0003224537172172859\n",
      "[LOG 20191027-17:53:31] epoch: 957 train-loss: 0.0003221827566903812\n",
      "[LOG 20191027-17:53:31] epoch: 958 train-loss: 0.0003219157595140132\n",
      "[LOG 20191027-17:53:31] epoch: 959 train-loss: 0.00032165286415875016\n",
      "[LOG 20191027-17:53:31] epoch: 960 train-loss: 0.0003213939896795637\n",
      "[LOG 20191027-17:53:32] epoch: 961 train-loss: 0.00032113926363308565\n",
      "[LOG 20191027-17:53:32] epoch: 962 train-loss: 0.0003208890368568973\n",
      "[LOG 20191027-17:53:32] epoch: 963 train-loss: 0.00032064307265500247\n",
      "[LOG 20191027-17:53:33] epoch: 964 train-loss: 0.0003204008994543983\n",
      "[LOG 20191027-17:53:33] epoch: 965 train-loss: 0.00032016261980061245\n",
      "[LOG 20191027-17:53:33] epoch: 966 train-loss: 0.000319928375120071\n",
      "[LOG 20191027-17:53:34] epoch: 967 train-loss: 0.00031969831366041035\n",
      "[LOG 20191027-17:53:34] epoch: 968 train-loss: 0.00031947259412845597\n",
      "[LOG 20191027-17:53:34] epoch: 969 train-loss: 0.0003192507726907934\n",
      "[LOG 20191027-17:53:34] epoch: 970 train-loss: 0.0003190329259723512\n",
      "[LOG 20191027-17:53:35] epoch: 971 train-loss: 0.00031881910899755894\n",
      "[LOG 20191027-17:53:35] epoch: 972 train-loss: 0.00031860921626503114\n",
      "[LOG 20191027-17:53:35] epoch: 973 train-loss: 0.0003184035563208454\n",
      "[LOG 20191027-17:53:36] epoch: 974 train-loss: 0.00031820178128327825\n",
      "[LOG 20191027-17:53:36] epoch: 975 train-loss: 0.00031800397914594214\n",
      "[LOG 20191027-17:53:36] epoch: 976 train-loss: 0.0003178100159857422\n",
      "[LOG 20191027-17:53:37] epoch: 977 train-loss: 0.00031761980403643975\n",
      "[LOG 20191027-17:53:37] epoch: 978 train-loss: 0.0003174338444296154\n",
      "[LOG 20191027-17:53:37] epoch: 979 train-loss: 0.00031725181474939745\n",
      "[LOG 20191027-17:53:37] epoch: 980 train-loss: 0.00031707338985142997\n",
      "[LOG 20191027-17:53:38] epoch: 981 train-loss: 0.00031689880415797234\n",
      "[LOG 20191027-17:53:38] epoch: 982 train-loss: 0.00031672812838223763\n",
      "[LOG 20191027-17:53:38] epoch: 983 train-loss: 0.0003165614627960167\n",
      "[LOG 20191027-17:53:38] epoch: 984 train-loss: 0.00031639855774301395\n",
      "[LOG 20191027-17:53:39] epoch: 985 train-loss: 0.0003162392108606582\n",
      "[LOG 20191027-17:53:39] epoch: 986 train-loss: 0.000316083737743611\n",
      "[LOG 20191027-17:53:39] epoch: 987 train-loss: 0.00031593239896210434\n",
      "[LOG 20191027-17:53:39] epoch: 988 train-loss: 0.0003157845906116563\n",
      "[LOG 20191027-17:53:40] epoch: 989 train-loss: 0.00031564035202791274\n",
      "[LOG 20191027-17:53:40] epoch: 990 train-loss: 0.00031549980144518486\n",
      "[LOG 20191027-17:53:40] epoch: 991 train-loss: 0.0003153633310830628\n",
      "[LOG 20191027-17:53:41] epoch: 992 train-loss: 0.0003152305084768159\n",
      "[LOG 20191027-17:53:41] epoch: 993 train-loss: 0.0003151012435864686\n",
      "[LOG 20191027-17:53:41] epoch: 994 train-loss: 0.0003149757294522715\n",
      "[LOG 20191027-17:53:41] epoch: 995 train-loss: 0.0003148539767607872\n",
      "[LOG 20191027-17:53:42] epoch: 996 train-loss: 0.00031473571971218917\n",
      "[LOG 20191027-17:53:42] epoch: 997 train-loss: 0.00031462099127566034\n",
      "[LOG 20191027-17:53:42] epoch: 998 train-loss: 0.00031450983033209923\n",
      "[LOG 20191027-17:53:43] epoch: 999 train-loss: 0.00031440264206139545\n",
      "[LOG 20191027-17:53:43] epoch: 1000 train-loss: 0.00031429909677171963\n",
      "[LOG 20191027-17:53:43] epoch: 1001 train-loss: 0.00031419893434758706\n",
      "[LOG 20191027-17:53:43] epoch: 1002 train-loss: 0.0003141022523323045\n",
      "[LOG 20191027-17:53:44] epoch: 1003 train-loss: 0.0003140091714612936\n",
      "[LOG 20191027-17:53:44] epoch: 1004 train-loss: 0.0003139195164294506\n",
      "[LOG 20191027-17:53:44] epoch: 1005 train-loss: 0.00031383333202938957\n",
      "[LOG 20191027-17:53:44] epoch: 1006 train-loss: 0.0003137507255814853\n",
      "[LOG 20191027-17:53:45] epoch: 1007 train-loss: 0.0003136715772598109\n",
      "[LOG 20191027-17:53:45] epoch: 1008 train-loss: 0.00031359579179479624\n",
      "[LOG 20191027-17:53:45] epoch: 1009 train-loss: 0.0003135234489946015\n",
      "[LOG 20191027-17:53:45] epoch: 1010 train-loss: 0.00031345463935394946\n",
      "[LOG 20191027-17:53:46] epoch: 1011 train-loss: 0.0003133890502340364\n",
      "[LOG 20191027-17:53:46] epoch: 1012 train-loss: 0.0003133266679924418\n",
      "[LOG 20191027-17:53:46] epoch: 1013 train-loss: 0.0003132677022676944\n",
      "[LOG 20191027-17:53:46] epoch: 1014 train-loss: 0.00031321208689405466\n",
      "[LOG 20191027-17:53:47] epoch: 1015 train-loss: 0.0003131595772174478\n",
      "[LOG 20191027-17:53:47] epoch: 1016 train-loss: 0.000313110152092122\n",
      "[LOG 20191027-17:53:47] epoch: 1017 train-loss: 0.0003130639113351208\n",
      "[LOG 20191027-17:53:47] epoch: 1018 train-loss: 0.0003130207630874793\n",
      "[LOG 20191027-17:53:48] epoch: 1019 train-loss: 0.00031298057160711323\n",
      "[LOG 20191027-17:53:48] epoch: 1020 train-loss: 0.00031294356449507177\n",
      "[LOG 20191027-17:53:48] epoch: 1021 train-loss: 0.00031290967444874696\n",
      "[LOG 20191027-17:53:48] epoch: 1022 train-loss: 0.0003128785931494349\n",
      "[LOG 20191027-17:53:49] epoch: 1023 train-loss: 0.00031285033378480875\n",
      "[LOG 20191027-17:53:49] epoch: 1024 train-loss: 0.0003128250812096667\n",
      "[LOG 20191027-17:53:49] epoch: 1025 train-loss: 0.0003128025684873137\n",
      "[LOG 20191027-17:53:50] epoch: 1026 train-loss: 0.0003127827715161402\n",
      "[LOG 20191027-17:53:50] epoch: 1027 train-loss: 0.00031276579557015793\n",
      "[LOG 20191027-17:53:50] epoch: 1028 train-loss: 0.0003127514396510378\n",
      "[LOG 20191027-17:53:50] epoch: 1029 train-loss: 0.00031273965532818693\n",
      "[LOG 20191027-17:53:51] epoch: 1030 train-loss: 0.0003127306213173142\n",
      "[LOG 20191027-17:53:51] epoch: 1031 train-loss: 0.0003127241282072646\n",
      "[LOG 20191027-17:53:51] epoch: 1032 train-loss: 0.00031272006822291587\n",
      "[LOG 20191027-17:53:51] epoch: 1033 train-loss: 0.00031271856119019503\n",
      "[LOG 20191027-17:53:52] epoch: 1034 train-loss: 0.0003127194113403675\n",
      "[LOG 20191027-17:53:52] epoch: 1035 train-loss: 0.00031272256092051975\n",
      "[LOG 20191027-17:53:52] epoch: 1036 train-loss: 0.0003127280806438648\n",
      "[LOG 20191027-17:53:52] epoch: 1037 train-loss: 0.00031273590070668433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:53:53] epoch: 1038 train-loss: 0.0003127458303424646\n",
      "[LOG 20191027-17:53:53] epoch: 1039 train-loss: 0.0003127579773263278\n",
      "[LOG 20191027-17:53:53] epoch: 1040 train-loss: 0.0003127723746274569\n",
      "[LOG 20191027-17:53:53] epoch: 1041 train-loss: 0.0003127887168830057\n",
      "[LOG 20191027-17:53:54] epoch: 1042 train-loss: 0.0003128070525235671\n",
      "[LOG 20191027-17:53:54] epoch: 1043 train-loss: 0.00031282754343919805\n",
      "[LOG 20191027-17:53:54] epoch: 1044 train-loss: 0.0003128499315607769\n",
      "[LOG 20191027-17:53:55] epoch: 1045 train-loss: 0.0003128741327600437\n",
      "[LOG 20191027-17:53:55] epoch: 1046 train-loss: 0.000312900237076974\n",
      "[LOG 20191027-17:53:55] epoch: 1047 train-loss: 0.00031292813082473003\n",
      "[LOG 20191027-17:53:55] epoch: 1048 train-loss: 0.0003129578076368489\n",
      "[LOG 20191027-17:53:56] epoch: 1049 train-loss: 0.0003129892393189948\n",
      "[LOG 20191027-17:53:56] epoch: 1050 train-loss: 0.00031302232559937693\n",
      "[LOG 20191027-17:53:56] epoch: 1051 train-loss: 0.00031305697871175653\n",
      "[LOG 20191027-17:53:56] epoch: 1052 train-loss: 0.0003130932318526902\n",
      "[LOG 20191027-17:53:57] epoch: 1053 train-loss: 0.0003131310545541055\n",
      "[LOG 20191027-17:53:57] epoch: 1054 train-loss: 0.0003131702678729198\n",
      "[LOG 20191027-17:53:57] epoch: 1055 train-loss: 0.0003132109570742614\n",
      "[LOG 20191027-17:53:57] epoch: 1056 train-loss: 0.0003132531430765084\n",
      "[LOG 20191027-17:53:58] epoch: 1057 train-loss: 0.00031329659964285383\n",
      "[LOG 20191027-17:53:58] epoch: 1058 train-loss: 0.0003133413508749072\n",
      "[LOG 20191027-17:53:58] epoch: 1059 train-loss: 0.0003133874411105353\n",
      "[LOG 20191027-17:53:58] epoch: 1060 train-loss: 0.0003134347189188702\n",
      "[LOG 20191027-17:53:59] epoch: 1061 train-loss: 0.00031348312063528283\n",
      "[LOG 20191027-17:53:59] epoch: 1062 train-loss: 0.0003135327349355066\n",
      "[LOG 20191027-17:53:59] epoch: 1063 train-loss: 0.00031358346859633457\n",
      "[LOG 20191027-17:53:59] epoch: 1064 train-loss: 0.0003136352343062754\n",
      "[LOG 20191027-17:54:00] epoch: 1065 train-loss: 0.00031368806526188564\n",
      "[LOG 20191027-17:54:00] epoch: 1066 train-loss: 0.0003137419173526723\n",
      "[LOG 20191027-17:54:00] epoch: 1067 train-loss: 0.00031379666347675084\n",
      "[LOG 20191027-17:54:00] epoch: 1068 train-loss: 0.0003138523047709896\n",
      "[LOG 20191027-17:54:01] epoch: 1069 train-loss: 0.00031390888079840806\n",
      "[LOG 20191027-17:54:01] epoch: 1070 train-loss: 0.0003139662535431853\n",
      "[LOG 20191027-17:54:01] epoch: 1071 train-loss: 0.0003140243700272549\n",
      "[LOG 20191027-17:54:01] epoch: 1072 train-loss: 0.00031408330687554553\n",
      "[LOG 20191027-17:54:02] epoch: 1073 train-loss: 0.00031414294289788813\n",
      "[LOG 20191027-17:54:02] epoch: 1074 train-loss: 0.00031420324557984713\n",
      "[LOG 20191027-17:54:02] epoch: 1075 train-loss: 0.00031426422651748\n",
      "[LOG 20191027-17:54:03] epoch: 1076 train-loss: 0.00031432592481905886\n",
      "[LOG 20191027-17:54:03] epoch: 1077 train-loss: 0.0003143882067888626\n",
      "[LOG 20191027-17:54:03] epoch: 1078 train-loss: 0.00031445102945326653\n",
      "[LOG 20191027-17:54:03] epoch: 1079 train-loss: 0.0003145144519294263\n",
      "[LOG 20191027-17:54:04] epoch: 1080 train-loss: 0.000314578365532725\n",
      "[LOG 20191027-17:54:04] epoch: 1081 train-loss: 0.00031464271205550176\n",
      "[LOG 20191027-17:54:04] epoch: 1082 train-loss: 0.00031470750650441914\n",
      "[LOG 20191027-17:54:04] epoch: 1083 train-loss: 0.000314772771389471\n",
      "[LOG 20191027-17:54:05] epoch: 1084 train-loss: 0.00031483840962209797\n",
      "[LOG 20191027-17:54:05] epoch: 1085 train-loss: 0.00031490437527281756\n",
      "[LOG 20191027-17:54:05] epoch: 1086 train-loss: 0.0003149707292777748\n",
      "[LOG 20191027-17:54:05] epoch: 1087 train-loss: 0.0003150374384404131\n",
      "[LOG 20191027-17:54:06] epoch: 1088 train-loss: 0.00031510442568105645\n",
      "[LOG 20191027-17:54:06] epoch: 1089 train-loss: 0.0003151716953198047\n",
      "[LOG 20191027-17:54:06] epoch: 1090 train-loss: 0.00031523926895715704\n",
      "[LOG 20191027-17:54:06] epoch: 1091 train-loss: 0.0003153070485950593\n",
      "[LOG 20191027-17:54:07] epoch: 1092 train-loss: 0.0003153750133151334\n",
      "[LOG 20191027-17:54:07] epoch: 1093 train-loss: 0.0003154431499297061\n",
      "[LOG 20191027-17:54:07] epoch: 1094 train-loss: 0.00031551148936159734\n",
      "[LOG 20191027-17:54:07] epoch: 1095 train-loss: 0.0003155800015974819\n",
      "[LOG 20191027-17:54:08] epoch: 1096 train-loss: 0.0003156486161515204\n",
      "[LOG 20191027-17:54:08] epoch: 1097 train-loss: 0.0003157173216550291\n",
      "[LOG 20191027-17:54:08] epoch: 1098 train-loss: 0.00031578615494254336\n",
      "[LOG 20191027-17:54:08] epoch: 1099 train-loss: 0.00031585505098519207\n",
      "[LOG 20191027-17:54:09] epoch: 1100 train-loss: 0.0003159239715841977\n",
      "[LOG 20191027-17:54:09] epoch: 1101 train-loss: 0.0003159928726290673\n",
      "[LOG 20191027-17:54:09] epoch: 1102 train-loss: 0.00031606181232746167\n",
      "[LOG 20191027-17:54:09] epoch: 1103 train-loss: 0.0003161307265600044\n",
      "[LOG 20191027-17:54:10] epoch: 1104 train-loss: 0.0003161995819027652\n",
      "[LOG 20191027-17:54:10] epoch: 1105 train-loss: 0.0003162683576647396\n",
      "[LOG 20191027-17:54:10] epoch: 1106 train-loss: 0.0003163371009122784\n",
      "[LOG 20191027-17:54:10] epoch: 1107 train-loss: 0.0003164057268350007\n",
      "[LOG 20191027-17:54:11] epoch: 1108 train-loss: 0.0003164742208809912\n",
      "[LOG 20191027-17:54:11] epoch: 1109 train-loss: 0.0003165425050610793\n",
      "[LOG 20191027-17:54:11] epoch: 1110 train-loss: 0.000316610641675652\n",
      "[LOG 20191027-17:54:12] epoch: 1111 train-loss: 0.0003166786241308728\n",
      "[LOG 20191027-17:54:12] epoch: 1112 train-loss: 0.0003167464135458431\n",
      "[LOG 20191027-17:54:12] epoch: 1113 train-loss: 0.00031681395921623334\n",
      "[LOG 20191027-17:54:12] epoch: 1114 train-loss: 0.0003168812463627546\n",
      "[LOG 20191027-17:54:13] epoch: 1115 train-loss: 0.0003169483270539786\n",
      "[LOG 20191027-17:54:13] epoch: 1116 train-loss: 0.00031701514080850757\n",
      "[LOG 20191027-17:54:13] epoch: 1117 train-loss: 0.0003170816469264537\n",
      "[LOG 20191027-17:54:13] epoch: 1118 train-loss: 0.00031714782380731776\n",
      "[LOG 20191027-17:54:14] epoch: 1119 train-loss: 0.0003172136093780864\n",
      "[LOG 20191027-17:54:14] epoch: 1120 train-loss: 0.0003172790900407563\n",
      "[LOG 20191027-17:54:14] epoch: 1121 train-loss: 0.00031734419576423534\n",
      "[LOG 20191027-17:54:14] epoch: 1122 train-loss: 0.0003174089060848928\n",
      "[LOG 20191027-17:54:15] epoch: 1123 train-loss: 0.00031747319644637173\n",
      "[LOG 20191027-17:54:15] epoch: 1124 train-loss: 0.00031753706548443006\n",
      "[LOG 20191027-17:54:15] epoch: 1125 train-loss: 0.0003176005129716941\n",
      "[LOG 20191027-17:54:15] epoch: 1126 train-loss: 0.0003176634870669659\n",
      "[LOG 20191027-17:54:16] epoch: 1127 train-loss: 0.00031772601050761295\n",
      "[LOG 20191027-17:54:16] epoch: 1128 train-loss: 0.0003177880100793118\n",
      "[LOG 20191027-17:54:16] epoch: 1129 train-loss: 0.00031784949555913045\n",
      "[LOG 20191027-17:54:17] epoch: 1130 train-loss: 0.00031791052242624573\n",
      "[LOG 20191027-17:54:17] epoch: 1131 train-loss: 0.0003179710404310754\n",
      "[LOG 20191027-17:54:17] epoch: 1132 train-loss: 0.00031803102478988876\n",
      "[LOG 20191027-17:54:17] epoch: 1133 train-loss: 0.00031809050642550574\n",
      "[LOG 20191027-17:54:18] epoch: 1134 train-loss: 0.00031814946737540595\n",
      "[LOG 20191027-17:54:18] epoch: 1135 train-loss: 0.0003182079010457528\n",
      "[LOG 20191027-17:54:18] epoch: 1136 train-loss: 0.00031826582358007727\n",
      "[LOG 20191027-17:54:18] epoch: 1137 train-loss: 0.00031832321064939606\n",
      "[LOG 20191027-17:54:19] epoch: 1138 train-loss: 0.0003183800777151191\n",
      "[LOG 20191027-17:54:19] epoch: 1139 train-loss: 0.0003184364331900724\n",
      "[LOG 20191027-17:54:19] epoch: 1140 train-loss: 0.00031849228003011376\n",
      "[LOG 20191027-17:54:19] epoch: 1141 train-loss: 0.0003185476325597847\n",
      "[LOG 20191027-17:54:20] epoch: 1142 train-loss: 0.00031860250533100043\n",
      "[LOG 20191027-17:54:20] epoch: 1143 train-loss: 0.0003186568660566991\n",
      "[LOG 20191027-17:54:20] epoch: 1144 train-loss: 0.00031871072997091687\n",
      "[LOG 20191027-17:54:20] epoch: 1145 train-loss: 0.0003187640991200169\n",
      "[LOG 20191027-17:54:21] epoch: 1146 train-loss: 0.000318816942808553\n",
      "[LOG 20191027-17:54:21] epoch: 1147 train-loss: 0.00031886925808066735\n",
      "[LOG 20191027-17:54:21] epoch: 1148 train-loss: 0.00031892106085251726\n",
      "[LOG 20191027-17:54:22] epoch: 1149 train-loss: 0.0003189723950072221\n",
      "[LOG 20191027-17:54:22] epoch: 1150 train-loss: 0.0003190232475844823\n",
      "[LOG 20191027-17:54:22] epoch: 1151 train-loss: 0.00031907359925753553\n",
      "[LOG 20191027-17:54:22] epoch: 1152 train-loss: 0.000319123472991123\n",
      "[LOG 20191027-17:54:23] epoch: 1153 train-loss: 0.0003191728842466546\n",
      "[LOG 20191027-17:54:23] epoch: 1154 train-loss: 0.0003192218616732134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:54:23] epoch: 1155 train-loss: 0.0003192704045886785\n",
      "[LOG 20191027-17:54:23] epoch: 1156 train-loss: 0.00031931853777678043\n",
      "[LOG 20191027-17:54:24] epoch: 1157 train-loss: 0.0003193662130343\n",
      "[LOG 20191027-17:54:24] epoch: 1158 train-loss: 0.00031941344172992103\n",
      "[LOG 20191027-17:54:24] epoch: 1159 train-loss: 0.0003194601711129508\n",
      "[LOG 20191027-17:54:24] epoch: 1160 train-loss: 0.00031950643369782483\n",
      "[LOG 20191027-17:54:25] epoch: 1161 train-loss: 0.00031955227359503624\n",
      "[LOG 20191027-17:54:25] epoch: 1162 train-loss: 0.00031959768216438533\n",
      "[LOG 20191027-17:54:25] epoch: 1163 train-loss: 0.00031964262484507344\n",
      "[LOG 20191027-17:54:25] epoch: 1164 train-loss: 0.0003196871150521474\n",
      "[LOG 20191027-17:54:26] epoch: 1165 train-loss: 0.00031973117256711703\n",
      "[LOG 20191027-17:54:26] epoch: 1166 train-loss: 0.00031977478079170396\n",
      "[LOG 20191027-17:54:26] epoch: 1167 train-loss: 0.0003198179169885407\n",
      "[LOG 20191027-17:54:27] epoch: 1168 train-loss: 0.0003198605866145954\n",
      "[LOG 20191027-17:54:27] epoch: 1169 train-loss: 0.00031990278876037337\n",
      "[LOG 20191027-17:54:27] epoch: 1170 train-loss: 0.00031994453183870064\n",
      "[LOG 20191027-17:54:27] epoch: 1171 train-loss: 0.00031998581107473\n",
      "[LOG 20191027-17:54:28] epoch: 1172 train-loss: 0.00032002664352148713\n",
      "[LOG 20191027-17:54:28] epoch: 1173 train-loss: 0.0003200670164460462\n",
      "[LOG 20191027-17:54:28] epoch: 1174 train-loss: 0.0003201069553142588\n",
      "[LOG 20191027-17:54:28] epoch: 1175 train-loss: 0.0003201464314770419\n",
      "[LOG 20191027-17:54:29] epoch: 1176 train-loss: 0.00032018543333833804\n",
      "[LOG 20191027-17:54:29] epoch: 1177 train-loss: 0.000320223937933406\n",
      "[LOG 20191027-17:54:29] epoch: 1178 train-loss: 0.0003202619748208235\n",
      "[LOG 20191027-17:54:29] epoch: 1179 train-loss: 0.0003202995121682761\n",
      "[LOG 20191027-17:54:30] epoch: 1180 train-loss: 0.0003203365467925323\n",
      "[LOG 20191027-17:54:30] epoch: 1181 train-loss: 0.0003203731002940913\n",
      "[LOG 20191027-17:54:30] epoch: 1182 train-loss: 0.0003204091744919424\n",
      "[LOG 20191027-17:54:31] epoch: 1183 train-loss: 0.0003204447393727605\n",
      "[LOG 20191027-17:54:31] epoch: 1184 train-loss: 0.00032047979516391933\n",
      "[LOG 20191027-17:54:31] epoch: 1185 train-loss: 0.00032051438074631733\n",
      "[LOG 20191027-17:54:31] epoch: 1186 train-loss: 0.0003205484877071285\n",
      "[LOG 20191027-17:54:32] epoch: 1187 train-loss: 0.0003205821114988794\n",
      "[LOG 20191027-17:54:32] epoch: 1188 train-loss: 0.0003206152246093552\n",
      "[LOG 20191027-17:54:32] epoch: 1189 train-loss: 0.00032064783772511873\n",
      "[LOG 20191027-17:54:33] epoch: 1190 train-loss: 0.00032067996835394297\n",
      "[LOG 20191027-17:54:33] epoch: 1191 train-loss: 0.00032071159284896567\n",
      "[LOG 20191027-17:54:33] epoch: 1192 train-loss: 0.00032074270711746067\n",
      "[LOG 20191027-17:54:33] epoch: 1193 train-loss: 0.00032077332252811175\n",
      "[LOG 20191027-17:54:34] epoch: 1194 train-loss: 0.00032080345340546046\n",
      "[LOG 20191027-17:54:34] epoch: 1195 train-loss: 0.00032083309815789107\n",
      "[LOG 20191027-17:54:34] epoch: 1196 train-loss: 0.00032086225178318273\n",
      "[LOG 20191027-17:54:35] epoch: 1197 train-loss: 0.0003208908997294202\n",
      "[LOG 20191027-17:54:35] epoch: 1198 train-loss: 0.00032091903312903014\n",
      "[LOG 20191027-17:54:35] epoch: 1199 train-loss: 0.00032094666676130146\n",
      "[LOG 20191027-17:54:35] epoch: 1200 train-loss: 0.00032097381154017057\n",
      "[LOG 20191027-17:54:36] epoch: 1201 train-loss: 0.00032100045496008534\n",
      "[LOG 20191027-17:54:36] epoch: 1202 train-loss: 0.00032102660611599276\n",
      "[LOG 20191027-17:54:36] epoch: 1203 train-loss: 0.0003210522665995086\n",
      "[LOG 20191027-17:54:37] epoch: 1204 train-loss: 0.0003210774352737644\n",
      "[LOG 20191027-17:54:37] epoch: 1205 train-loss: 0.00032110210327118693\n",
      "[LOG 20191027-17:54:37] epoch: 1206 train-loss: 0.00032112627491187595\n",
      "[LOG 20191027-17:54:37] epoch: 1207 train-loss: 0.0003211499322333111\n",
      "[LOG 20191027-17:54:38] epoch: 1208 train-loss: 0.0003211730718248873\n",
      "[LOG 20191027-17:54:38] epoch: 1209 train-loss: 0.0003211956984614517\n",
      "[LOG 20191027-17:54:38] epoch: 1210 train-loss: 0.00032121781418936735\n",
      "[LOG 20191027-17:54:39] epoch: 1211 train-loss: 0.00032123941832651326\n",
      "[LOG 20191027-17:54:39] epoch: 1212 train-loss: 0.00032126051974046277\n",
      "[LOG 20191027-17:54:39] epoch: 1213 train-loss: 0.0003212810911463748\n",
      "[LOG 20191027-17:54:39] epoch: 1214 train-loss: 0.0003213011548268696\n",
      "[LOG 20191027-17:54:40] epoch: 1215 train-loss: 0.00032132072624335706\n",
      "[LOG 20191027-17:54:40] epoch: 1216 train-loss: 0.00032133982767845737\n",
      "[LOG 20191027-17:54:40] epoch: 1217 train-loss: 0.0003213584548120707\n",
      "[LOG 20191027-17:54:40] epoch: 1218 train-loss: 0.0003213765962755133\n",
      "[LOG 20191027-17:54:41] epoch: 1219 train-loss: 0.00032139422887667024\n",
      "[LOG 20191027-17:54:41] epoch: 1220 train-loss: 0.00032141137580765644\n",
      "[LOG 20191027-17:54:41] epoch: 1221 train-loss: 0.00032142802638190915\n",
      "[LOG 20191027-17:54:42] epoch: 1222 train-loss: 0.0003214441821910441\n",
      "[LOG 20191027-17:54:42] epoch: 1223 train-loss: 0.00032145984982889786\n",
      "[LOG 20191027-17:54:42] epoch: 1224 train-loss: 0.0003214749981452769\n",
      "[LOG 20191027-17:54:42] epoch: 1225 train-loss: 0.0003214896507870435\n",
      "[LOG 20191027-17:54:43] epoch: 1226 train-loss: 0.0003215038091184397\n",
      "[LOG 20191027-17:54:43] epoch: 1227 train-loss: 0.0003215174938304699\n",
      "[LOG 20191027-17:54:43] epoch: 1228 train-loss: 0.0003215306953734398\n",
      "[LOG 20191027-17:54:43] epoch: 1229 train-loss: 0.00032154341420209676\n",
      "[LOG 20191027-17:54:44] epoch: 1230 train-loss: 0.0003215556691884558\n",
      "[LOG 20191027-17:54:44] epoch: 1231 train-loss: 0.0003215674391867651\n",
      "[LOG 20191027-17:54:44] epoch: 1232 train-loss: 0.00032157873602045584\n",
      "[LOG 20191027-17:54:44] epoch: 1233 train-loss: 0.00032158955696104385\n",
      "[LOG 20191027-17:54:45] epoch: 1234 train-loss: 0.0003215999165604444\n",
      "[LOG 20191027-17:54:45] epoch: 1235 train-loss: 0.00032160981140805234\n",
      "[LOG 20191027-17:54:45] epoch: 1236 train-loss: 0.00032161924445972545\n",
      "[LOG 20191027-17:54:45] epoch: 1237 train-loss: 0.0003216282011635485\n",
      "[LOG 20191027-17:54:46] epoch: 1238 train-loss: 0.0003216366947071947\n",
      "[LOG 20191027-17:54:46] epoch: 1239 train-loss: 0.0003216447307750059\n",
      "[LOG 20191027-17:54:46] epoch: 1240 train-loss: 0.00032165230049940874\n",
      "[LOG 20191027-17:54:47] epoch: 1241 train-loss: 0.0003216594116111082\n",
      "[LOG 20191027-17:54:47] epoch: 1242 train-loss: 0.00032166607434191974\n",
      "[LOG 20191027-17:54:47] epoch: 1243 train-loss: 0.00032167228255275404\n",
      "[LOG 20191027-17:54:48] epoch: 1244 train-loss: 0.0003216780617094628\n",
      "[LOG 20191027-17:54:48] epoch: 1245 train-loss: 0.0003216834113572986\n",
      "[LOG 20191027-17:54:48] epoch: 1246 train-loss: 0.0003216883235381829\n",
      "[LOG 20191027-17:54:48] epoch: 1247 train-loss: 0.00032169283667826676\n",
      "[LOG 20191027-17:54:49] epoch: 1248 train-loss: 0.0003216969425920979\n",
      "[LOG 20191027-17:54:49] epoch: 1249 train-loss: 0.0003217006342310924\n",
      "[LOG 20191027-17:54:49] epoch: 1250 train-loss: 0.0003217039165974711\n",
      "[LOG 20191027-17:54:50] epoch: 1251 train-loss: 0.00032170680970011745\n",
      "[LOG 20191027-17:54:50] epoch: 1252 train-loss: 0.00032170931444852613\n",
      "[LOG 20191027-17:54:50] epoch: 1253 train-loss: 0.0003217114776816743\n",
      "[LOG 20191027-17:54:50] epoch: 1254 train-loss: 0.0003217132534700795\n",
      "[LOG 20191027-17:54:51] epoch: 1255 train-loss: 0.00032171463271879475\n",
      "[LOG 20191027-17:54:51] epoch: 1256 train-loss: 0.00032171564203054004\n",
      "[LOG 20191027-17:54:51] epoch: 1257 train-loss: 0.000321716289363394\n",
      "[LOG 20191027-17:54:52] epoch: 1258 train-loss: 0.0003217165851765458\n",
      "[LOG 20191027-17:54:52] epoch: 1259 train-loss: 0.0003217165228761587\n",
      "[LOG 20191027-17:54:52] epoch: 1260 train-loss: 0.0003217161051907169\n",
      "[LOG 20191027-17:54:52] epoch: 1261 train-loss: 0.00032171531802305253\n",
      "[LOG 20191027-17:54:53] epoch: 1262 train-loss: 0.00032171420843951637\n",
      "[LOG 20191027-17:54:53] epoch: 1263 train-loss: 0.0003217127673451614\n",
      "[LOG 20191027-17:54:53] epoch: 1264 train-loss: 0.0003217110061086714\n",
      "[LOG 20191027-17:54:53] epoch: 1265 train-loss: 0.0003217089370082249\n",
      "[LOG 20191027-17:54:54] epoch: 1266 train-loss: 0.0003217065325316071\n",
      "[LOG 20191027-17:54:54] epoch: 1267 train-loss: 0.00032170380768548057\n",
      "[LOG 20191027-17:54:54] epoch: 1268 train-loss: 0.0003217007702005503\n",
      "[LOG 20191027-17:54:55] epoch: 1269 train-loss: 0.00032169741803045326\n",
      "[LOG 20191027-17:54:55] epoch: 1270 train-loss: 0.0003216937518573104\n",
      "[LOG 20191027-17:54:55] epoch: 1271 train-loss: 0.0003216897819129372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:54:55] epoch: 1272 train-loss: 0.00032168551729228057\n",
      "[LOG 20191027-17:54:56] epoch: 1273 train-loss: 0.0003216809698187717\n",
      "[LOG 20191027-17:54:56] epoch: 1274 train-loss: 0.00032167615381695214\n",
      "[LOG 20191027-17:54:56] epoch: 1275 train-loss: 0.0003216710449578386\n",
      "[LOG 20191027-17:54:57] epoch: 1276 train-loss: 0.00032166566302294086\n",
      "[LOG 20191027-17:54:57] epoch: 1277 train-loss: 0.0003216600027826644\n",
      "[LOG 20191027-17:54:57] epoch: 1278 train-loss: 0.00032165407446882455\n",
      "[LOG 20191027-17:54:57] epoch: 1279 train-loss: 0.00032164789672606275\n",
      "[LOG 20191027-17:54:58] epoch: 1280 train-loss: 0.00032164145818569523\n",
      "[LOG 20191027-17:54:58] epoch: 1281 train-loss: 0.0003216347688521637\n",
      "[LOG 20191027-17:54:58] epoch: 1282 train-loss: 0.00032162780712496897\n",
      "[LOG 20191027-17:54:58] epoch: 1283 train-loss: 0.00032162058437279484\n",
      "[LOG 20191027-17:54:59] epoch: 1284 train-loss: 0.00032161313811229775\n",
      "[LOG 20191027-17:54:59] epoch: 1285 train-loss: 0.0003216054360564158\n",
      "[LOG 20191027-17:54:59] epoch: 1286 train-loss: 0.0003215974913928221\n",
      "[LOG 20191027-17:55:00] epoch: 1287 train-loss: 0.0003215893082142429\n",
      "[LOG 20191027-17:55:00] epoch: 1288 train-loss: 0.00032158089311451477\n",
      "[LOG 20191027-17:55:00] epoch: 1289 train-loss: 0.00032157223859030637\n",
      "[LOG 20191027-17:55:01] epoch: 1290 train-loss: 0.00032156333691091277\n",
      "[LOG 20191027-17:55:01] epoch: 1291 train-loss: 0.00032155420558410697\n",
      "[LOG 20191027-17:55:01] epoch: 1292 train-loss: 0.000321544845974131\n",
      "[LOG 20191027-17:55:02] epoch: 1293 train-loss: 0.00032153526080946904\n",
      "[LOG 20191027-17:55:02] epoch: 1294 train-loss: 0.00032152545509234187\n",
      "[LOG 20191027-17:55:02] epoch: 1295 train-loss: 0.00032151544633052254\n",
      "[LOG 20191027-17:55:02] epoch: 1296 train-loss: 0.00032150523247764795\n",
      "[LOG 20191027-17:55:03] epoch: 1297 train-loss: 0.0003214948173990706\n",
      "[LOG 20191027-17:55:03] epoch: 1298 train-loss: 0.0003214841945009539\n",
      "[LOG 20191027-17:55:03] epoch: 1299 train-loss: 0.0003214733628738031\n",
      "[LOG 20191027-17:55:03] epoch: 1300 train-loss: 0.0003214623272924655\n",
      "[LOG 20191027-17:55:04] epoch: 1301 train-loss: 0.000321451097534009\n",
      "[LOG 20191027-17:55:04] epoch: 1302 train-loss: 0.00032143965290742926\n",
      "[LOG 20191027-17:55:04] epoch: 1303 train-loss: 0.00032142800705514674\n",
      "[LOG 20191027-17:55:05] epoch: 1304 train-loss: 0.00032141616998160316\n",
      "[LOG 20191027-17:55:05] epoch: 1305 train-loss: 0.00032140414919012983\n",
      "[LOG 20191027-17:55:05] epoch: 1306 train-loss: 0.00032139196491698385\n",
      "[LOG 20191027-17:55:05] epoch: 1307 train-loss: 0.0003213796212548914\n",
      "[LOG 20191027-17:55:06] epoch: 1308 train-loss: 0.00032136710319718986\n",
      "[LOG 20191027-17:55:06] epoch: 1309 train-loss: 0.00032135440983438457\n",
      "[LOG 20191027-17:55:06] epoch: 1310 train-loss: 0.0003213415486698068\n",
      "[LOG 20191027-17:55:06] epoch: 1311 train-loss: 0.00032132852243194066\n",
      "[LOG 20191027-17:55:07] epoch: 1312 train-loss: 0.0003213153584056272\n",
      "[LOG 20191027-17:55:07] epoch: 1313 train-loss: 0.0003213020529528876\n",
      "[LOG 20191027-17:55:07] epoch: 1314 train-loss: 0.00032128860198099574\n",
      "[LOG 20191027-17:55:08] epoch: 1315 train-loss: 0.00032127500662681996\n",
      "[LOG 20191027-17:55:08] epoch: 1316 train-loss: 0.0003212612696188444\n",
      "[LOG 20191027-17:55:08] epoch: 1317 train-loss: 0.0003212473945950478\n",
      "[LOG 20191027-17:55:08] epoch: 1318 train-loss: 0.0003212333533610945\n",
      "[LOG 20191027-17:55:09] epoch: 1319 train-loss: 0.0003212191793409147\n",
      "[LOG 20191027-17:55:09] epoch: 1320 train-loss: 0.00032120488754117105\n",
      "[LOG 20191027-17:55:09] epoch: 1321 train-loss: 0.00032119047114065324\n",
      "[LOG 20191027-17:55:10] epoch: 1322 train-loss: 0.00032117594173541875\n",
      "[LOG 20191027-17:55:10] epoch: 1323 train-loss: 0.00032116128272718925\n",
      "[LOG 20191027-17:55:10] epoch: 1324 train-loss: 0.0003211465195818164\n",
      "[LOG 20191027-17:55:10] epoch: 1325 train-loss: 0.0003211316670785891\n",
      "[LOG 20191027-17:55:11] epoch: 1326 train-loss: 0.00032111671202983416\n",
      "[LOG 20191027-17:55:11] epoch: 1327 train-loss: 0.0003211016630757513\n",
      "[LOG 20191027-17:55:11] epoch: 1328 train-loss: 0.0003210865215805825\n",
      "[LOG 20191027-17:55:11] epoch: 1329 train-loss: 0.0003210712959571538\n",
      "[LOG 20191027-17:55:12] epoch: 1330 train-loss: 0.00032105598347698105\n",
      "[LOG 20191027-17:55:12] epoch: 1331 train-loss: 0.0003210405257050297\n",
      "[LOG 20191027-17:55:12] epoch: 1332 train-loss: 0.0003210249797120923\n",
      "[LOG 20191027-17:55:13] epoch: 1333 train-loss: 0.00032100936937240476\n",
      "[LOG 20191027-17:55:13] epoch: 1334 train-loss: 0.00032099369377647236\n",
      "[LOG 20191027-17:55:13] epoch: 1335 train-loss: 0.0003209779536064161\n",
      "[LOG 20191027-17:55:13] epoch: 1336 train-loss: 0.00032096214613375196\n",
      "[LOG 20191027-17:55:14] epoch: 1337 train-loss: 0.0003209462786344375\n",
      "[LOG 20191027-17:55:14] epoch: 1338 train-loss: 0.0003209303527000884\n",
      "[LOG 20191027-17:55:14] epoch: 1339 train-loss: 0.0003209143856111041\n",
      "[LOG 20191027-17:55:15] epoch: 1340 train-loss: 0.0003208983744116267\n",
      "[LOG 20191027-17:55:15] epoch: 1341 train-loss: 0.00032088234183902387\n",
      "[LOG 20191027-17:55:15] epoch: 1342 train-loss: 0.0003208662003544305\n",
      "[LOG 20191027-17:55:15] epoch: 1343 train-loss: 0.0003208499940683396\n",
      "[LOG 20191027-17:55:16] epoch: 1344 train-loss: 0.00032083376595437585\n",
      "[LOG 20191027-17:55:16] epoch: 1345 train-loss: 0.00032081751919577073\n",
      "[LOG 20191027-17:55:16] epoch: 1346 train-loss: 0.0003208012512914138\n",
      "[LOG 20191027-17:55:16] epoch: 1347 train-loss: 0.0003207849729278678\n",
      "[LOG 20191027-17:55:17] epoch: 1348 train-loss: 0.000320768691608464\n",
      "[LOG 20191027-17:55:17] epoch: 1349 train-loss: 0.0003207524268873385\n",
      "[LOG 20191027-17:55:17] epoch: 1350 train-loss: 0.0003207361719432811\n",
      "[LOG 20191027-17:55:18] epoch: 1351 train-loss: 0.00032071994860416453\n",
      "[LOG 20191027-17:55:18] epoch: 1352 train-loss: 0.0003207037425454473\n",
      "[LOG 20191027-17:55:18] epoch: 1353 train-loss: 0.00032068747123048524\n",
      "[LOG 20191027-17:55:18] epoch: 1354 train-loss: 0.0003206711912753235\n",
      "[LOG 20191027-17:55:19] epoch: 1355 train-loss: 0.00032065493951449753\n",
      "[LOG 20191027-17:55:19] epoch: 1356 train-loss: 0.0003206387143563916\n",
      "[LOG 20191027-17:55:19] epoch: 1357 train-loss: 0.0003206225228495896\n",
      "[LOG 20191027-17:55:20] epoch: 1358 train-loss: 0.00032060636340247584\n",
      "[LOG 20191027-17:55:20] epoch: 1359 train-loss: 0.0003205902426088869\n",
      "[LOG 20191027-17:55:20] epoch: 1360 train-loss: 0.0003205741556939756\n",
      "[LOG 20191027-17:55:20] epoch: 1361 train-loss: 0.0003205581090242049\n",
      "[LOG 20191027-17:55:21] epoch: 1362 train-loss: 0.0003205420923677593\n",
      "[LOG 20191027-17:55:21] epoch: 1363 train-loss: 0.00032052604046839406\n",
      "[LOG 20191027-17:55:21] epoch: 1364 train-loss: 0.0003205100301784114\n",
      "[LOG 20191027-17:55:21] epoch: 1365 train-loss: 0.0003204940710475057\n",
      "[LOG 20191027-17:55:22] epoch: 1366 train-loss: 0.0003204781742169871\n",
      "[LOG 20191027-17:55:22] epoch: 1367 train-loss: 0.00032046231831373007\n",
      "[LOG 20191027-17:55:22] epoch: 1368 train-loss: 0.0003204465137969237\n",
      "[LOG 20191027-17:55:22] epoch: 1369 train-loss: 0.000320430771125757\n",
      "[LOG 20191027-17:55:23] epoch: 1370 train-loss: 0.0003204150864348776\n",
      "[LOG 20191027-17:55:23] epoch: 1371 train-loss: 0.0003203994644991326\n",
      "[LOG 20191027-17:55:23] epoch: 1372 train-loss: 0.00032038382050814107\n",
      "[LOG 20191027-17:55:23] epoch: 1373 train-loss: 0.00032036822199188464\n",
      "[LOG 20191027-17:55:24] epoch: 1374 train-loss: 0.00032035268668551\n",
      "[LOG 20191027-17:55:24] epoch: 1375 train-loss: 0.00032033721504376444\n",
      "[LOG 20191027-17:55:24] epoch: 1376 train-loss: 0.00032032181047725317\n",
      "[LOG 20191027-17:55:24] epoch: 1377 train-loss: 0.00032030647753344965\n",
      "[LOG 20191027-17:55:25] epoch: 1378 train-loss: 0.00032029122235144314\n",
      "[LOG 20191027-17:55:25] epoch: 1379 train-loss: 0.00032027598626882536\n",
      "[LOG 20191027-17:55:25] epoch: 1380 train-loss: 0.000320260803846395\n",
      "[LOG 20191027-17:55:25] epoch: 1381 train-loss: 0.0003202457126008085\n",
      "[LOG 20191027-17:55:26] epoch: 1382 train-loss: 0.0003202307100309554\n",
      "[LOG 20191027-17:55:26] epoch: 1383 train-loss: 0.0003202157490704849\n",
      "[LOG 20191027-17:55:26] epoch: 1384 train-loss: 0.00032020084427131223\n",
      "[LOG 20191027-17:55:27] epoch: 1385 train-loss: 0.0003201860224635311\n",
      "[LOG 20191027-17:55:27] epoch: 1386 train-loss: 0.0003201712904683518\n",
      "[LOG 20191027-17:55:27] epoch: 1387 train-loss: 0.00032015660872275475\n",
      "[LOG 20191027-17:55:27] epoch: 1388 train-loss: 0.00032014200792218617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:55:28] epoch: 1389 train-loss: 0.0003201275178525975\n",
      "[LOG 20191027-17:55:28] epoch: 1390 train-loss: 0.00032011311577662127\n",
      "[LOG 20191027-17:55:28] epoch: 1391 train-loss: 0.0003200987666787114\n",
      "[LOG 20191027-17:55:28] epoch: 1392 train-loss: 0.0003200845046649192\n",
      "[LOG 20191027-17:55:29] epoch: 1393 train-loss: 0.0003200703465608967\n",
      "[LOG 20191027-17:55:29] epoch: 1394 train-loss: 0.0003200562841811916\n",
      "[LOG 20191027-17:55:29] epoch: 1395 train-loss: 0.00032004227659854223\n",
      "[LOG 20191027-17:55:30] epoch: 1396 train-loss: 0.0003200283649675839\n",
      "[LOG 20191027-17:55:30] epoch: 1397 train-loss: 0.00032001455019781133\n",
      "[LOG 20191027-17:55:30] epoch: 1398 train-loss: 0.00032000084638639237\n",
      "[LOG 20191027-17:55:30] epoch: 1399 train-loss: 0.0003199872037384921\n",
      "[LOG 20191027-17:55:31] epoch: 1400 train-loss: 0.000319973643627236\n",
      "[LOG 20191027-17:55:31] epoch: 1401 train-loss: 0.00031996018810787064\n",
      "[LOG 20191027-17:55:31] epoch: 1402 train-loss: 0.0003199468367256486\n",
      "[LOG 20191027-17:55:32] epoch: 1403 train-loss: 0.0003199335537829029\n",
      "[LOG 20191027-17:55:32] epoch: 1404 train-loss: 0.0003199203686108376\n",
      "[LOG 20191027-17:55:32] epoch: 1405 train-loss: 0.00031990730735742545\n",
      "[LOG 20191027-17:55:32] epoch: 1406 train-loss: 0.00031989435751711426\n",
      "[LOG 20191027-17:55:33] epoch: 1407 train-loss: 0.0003198814822553686\n",
      "[LOG 20191027-17:55:33] epoch: 1408 train-loss: 0.00031986873568712326\n",
      "[LOG 20191027-17:55:33] epoch: 1409 train-loss: 0.00031985611008167325\n",
      "[LOG 20191027-17:55:33] epoch: 1410 train-loss: 0.000319843565193878\n",
      "[LOG 20191027-17:55:34] epoch: 1411 train-loss: 0.000319831146271099\n",
      "[LOG 20191027-17:55:34] epoch: 1412 train-loss: 0.00031981887104848283\n",
      "[LOG 20191027-17:55:34] epoch: 1413 train-loss: 0.00031980674225451367\n",
      "[LOG 20191027-17:55:34] epoch: 1414 train-loss: 0.0003197947096396092\n",
      "[LOG 20191027-17:55:35] epoch: 1415 train-loss: 0.0003197828173142625\n",
      "[LOG 20191027-17:55:35] epoch: 1416 train-loss: 0.00031977105868463696\n",
      "[LOG 20191027-17:55:35] epoch: 1417 train-loss: 0.0003197594176072016\n",
      "[LOG 20191027-17:55:36] epoch: 1418 train-loss: 0.0003197478501988371\n",
      "[LOG 20191027-17:55:36] epoch: 1419 train-loss: 0.0003197364274001302\n",
      "[LOG 20191027-17:55:36] epoch: 1420 train-loss: 0.00031972516035239096\n",
      "[LOG 20191027-17:55:36] epoch: 1421 train-loss: 0.00031971396879271197\n",
      "[LOG 20191027-17:55:37] epoch: 1422 train-loss: 0.0003197029295733955\n",
      "[LOG 20191027-17:55:37] epoch: 1423 train-loss: 0.00031969203337212093\n",
      "[LOG 20191027-17:55:37] epoch: 1424 train-loss: 0.00031968125563253125\n",
      "[LOG 20191027-17:55:37] epoch: 1425 train-loss: 0.00031967060749593657\n",
      "[LOG 20191027-17:55:38] epoch: 1426 train-loss: 0.0003196601089712203\n",
      "[LOG 20191027-17:55:38] epoch: 1427 train-loss: 0.0003196497484623251\n",
      "[LOG 20191027-17:55:38] epoch: 1428 train-loss: 0.00031963947026270034\n",
      "[LOG 20191027-17:55:38] epoch: 1429 train-loss: 0.0003196293419023277\n",
      "[LOG 20191027-17:55:39] epoch: 1430 train-loss: 0.0003196193647454493\n",
      "[LOG 20191027-17:55:39] epoch: 1431 train-loss: 0.0003196095062776294\n",
      "[LOG 20191027-17:55:39] epoch: 1432 train-loss: 0.00031959981629370304\n",
      "[LOG 20191027-17:55:39] epoch: 1433 train-loss: 0.0003195902827428654\n",
      "[LOG 20191027-17:55:40] epoch: 1434 train-loss: 0.000319580851282808\n",
      "[LOG 20191027-17:55:40] epoch: 1435 train-loss: 0.0003195715623860451\n",
      "[LOG 20191027-17:55:40] epoch: 1436 train-loss: 0.0003195624619820592\n",
      "[LOG 20191027-17:55:41] epoch: 1437 train-loss: 0.00031955352051227237\n",
      "[LOG 20191027-17:55:41] epoch: 1438 train-loss: 0.0003195447038706334\n",
      "[LOG 20191027-17:55:41] epoch: 1439 train-loss: 0.00031953606821844005\n",
      "[LOG 20191027-17:55:41] epoch: 1440 train-loss: 0.0003195275855887303\n",
      "[LOG 20191027-17:55:42] epoch: 1441 train-loss: 0.00031951921391737415\n",
      "[LOG 20191027-17:55:42] epoch: 1442 train-loss: 0.0003195110070919327\n",
      "[LOG 20191027-17:55:42] epoch: 1443 train-loss: 0.00031950298648553144\n",
      "[LOG 20191027-17:55:42] epoch: 1444 train-loss: 0.00031949508365869406\n",
      "[LOG 20191027-17:55:43] epoch: 1445 train-loss: 0.0003194873604570603\n",
      "[LOG 20191027-17:55:43] epoch: 1446 train-loss: 0.00031947981256053026\n",
      "[LOG 20191027-17:55:43] epoch: 1447 train-loss: 0.00031947238335305883\n",
      "[LOG 20191027-17:55:43] epoch: 1448 train-loss: 0.0003194651394551329\n",
      "[LOG 20191027-17:55:44] epoch: 1449 train-loss: 0.00031945808814271004\n",
      "[LOG 20191027-17:55:44] epoch: 1450 train-loss: 0.0003194511209585471\n",
      "[LOG 20191027-17:55:44] epoch: 1451 train-loss: 0.00031944432430464076\n",
      "[LOG 20191027-17:55:44] epoch: 1452 train-loss: 0.0003194377204636112\n",
      "[LOG 20191027-17:55:45] epoch: 1453 train-loss: 0.00031943123758537695\n",
      "[LOG 20191027-17:55:45] epoch: 1454 train-loss: 0.00031942497366799216\n",
      "[LOG 20191027-17:55:45] epoch: 1455 train-loss: 0.000319418915978531\n",
      "[LOG 20191027-17:55:45] epoch: 1456 train-loss: 0.00031941298334459134\n",
      "[LOG 20191027-17:55:46] epoch: 1457 train-loss: 0.0003194072355654498\n",
      "[LOG 20191027-17:55:46] epoch: 1458 train-loss: 0.0003194016858287796\n",
      "[LOG 20191027-17:55:46] epoch: 1459 train-loss: 0.00031939623340804246\n",
      "[LOG 20191027-17:55:46] epoch: 1460 train-loss: 0.0003193909940364392\n",
      "[LOG 20191027-17:55:47] epoch: 1461 train-loss: 0.0003193859497514495\n",
      "[LOG 20191027-17:55:47] epoch: 1462 train-loss: 0.0003193810305219813\n",
      "[LOG 20191027-17:55:47] epoch: 1463 train-loss: 0.00031937631160872115\n",
      "[LOG 20191027-17:55:48] epoch: 1464 train-loss: 0.00031937170069795684\n",
      "[LOG 20191027-17:55:48] epoch: 1465 train-loss: 0.00031936724144543405\n",
      "[LOG 20191027-17:55:48] epoch: 1466 train-loss: 0.0003193630077475973\n",
      "[LOG 20191027-17:55:49] epoch: 1467 train-loss: 0.0003193589116108342\n",
      "[LOG 20191027-17:55:49] epoch: 1468 train-loss: 0.00031935500646795845\n",
      "[LOG 20191027-17:55:49] epoch: 1469 train-loss: 0.00031935131028149044\n",
      "[LOG 20191027-17:55:49] epoch: 1470 train-loss: 0.00031934773528519145\n",
      "[LOG 20191027-17:55:50] epoch: 1471 train-loss: 0.00031934436697156343\n",
      "[LOG 20191027-17:55:50] epoch: 1472 train-loss: 0.00031934120647747477\n",
      "[LOG 20191027-17:55:50] epoch: 1473 train-loss: 0.0003193381869550649\n",
      "[LOG 20191027-17:55:51] epoch: 1474 train-loss: 0.00031933538753037283\n",
      "[LOG 20191027-17:55:51] epoch: 1475 train-loss: 0.00031933272157402826\n",
      "[LOG 20191027-17:55:51] epoch: 1476 train-loss: 0.0003193302072759252\n",
      "[LOG 20191027-17:55:51] epoch: 1477 train-loss: 0.00031932790238897724\n",
      "[LOG 20191027-17:55:52] epoch: 1478 train-loss: 0.00031932573824633437\n",
      "[LOG 20191027-17:55:52] epoch: 1479 train-loss: 0.00031932377169141546\n",
      "[LOG 20191027-17:55:52] epoch: 1480 train-loss: 0.0003193219943113945\n",
      "[LOG 20191027-17:55:53] epoch: 1481 train-loss: 0.0003193203806404199\n",
      "[LOG 20191027-17:55:53] epoch: 1482 train-loss: 0.00031931901708048827\n",
      "[LOG 20191027-17:55:53] epoch: 1483 train-loss: 0.00031931782814353937\n",
      "[LOG 20191027-17:55:54] epoch: 1484 train-loss: 0.0003193168472535035\n",
      "[LOG 20191027-17:55:54] epoch: 1485 train-loss: 0.00031931605712998135\n",
      "[LOG 20191027-17:55:54] epoch: 1486 train-loss: 0.0003193153897882439\n",
      "[LOG 20191027-17:55:55] epoch: 1487 train-loss: 0.000319314933676651\n",
      "[LOG 20191027-17:55:55] epoch: 1488 train-loss: 0.00031931463013279426\n",
      "[LOG 20191027-17:55:56] epoch: 1489 train-loss: 0.00031931454986988683\n",
      "[LOG 20191027-17:55:56] epoch: 1490 train-loss: 0.0003193146806097502\n",
      "[LOG 20191027-17:55:56] epoch: 1491 train-loss: 0.00031931494800119253\n",
      "[LOG 20191027-17:55:56] epoch: 1492 train-loss: 0.00031931543753671576\n",
      "[LOG 20191027-17:55:57] epoch: 1493 train-loss: 0.00031931605508361827\n",
      "[LOG 20191027-17:55:57] epoch: 1494 train-loss: 0.0003193168622601661\n",
      "[LOG 20191027-17:55:57] epoch: 1495 train-loss: 0.000319317862022217\n",
      "[LOG 20191027-17:55:57] epoch: 1496 train-loss: 0.0003193190038928151\n",
      "[LOG 20191027-17:55:58] epoch: 1497 train-loss: 0.00031932039291859837\n",
      "[LOG 20191027-17:55:58] epoch: 1498 train-loss: 0.00031932195406625397\n",
      "[LOG 20191027-17:55:58] epoch: 1499 train-loss: 0.0003193237462255638\n",
      "[LOG 20191027-17:55:58] epoch: 1500 train-loss: 0.0003193257075508882\n",
      "[LOG 20191027-17:55:59] epoch: 1501 train-loss: 0.00031932786782817857\n",
      "[LOG 20191027-17:55:59] epoch: 1502 train-loss: 0.0003193302691215649\n",
      "[LOG 20191027-17:55:59] epoch: 1503 train-loss: 0.0003193328329871292\n",
      "[LOG 20191027-17:55:59] epoch: 1504 train-loss: 0.0003193356089923327\n",
      "[LOG 20191027-17:56:00] epoch: 1505 train-loss: 0.0003193385359736567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:56:00] epoch: 1506 train-loss: 0.0003193417192051129\n",
      "[LOG 20191027-17:56:00] epoch: 1507 train-loss: 0.0003193450850176305\n",
      "[LOG 20191027-17:56:01] epoch: 1508 train-loss: 0.0003193486734289763\n",
      "[LOG 20191027-17:56:01] epoch: 1509 train-loss: 0.0003193524421476468\n",
      "[LOG 20191027-17:56:01] epoch: 1510 train-loss: 0.0003193564143657568\n",
      "[LOG 20191027-17:56:01] epoch: 1511 train-loss: 0.0003193606282820838\n",
      "[LOG 20191027-17:56:02] epoch: 1512 train-loss: 0.00031936499794937845\n",
      "[LOG 20191027-17:56:02] epoch: 1513 train-loss: 0.00031936960704115336\n",
      "[LOG 20191027-17:56:02] epoch: 1514 train-loss: 0.00031937434982864943\n",
      "[LOG 20191027-17:56:02] epoch: 1515 train-loss: 0.00031937933454173617\n",
      "[LOG 20191027-17:56:03] epoch: 1516 train-loss: 0.00031938450956658926\n",
      "[LOG 20191027-17:56:03] epoch: 1517 train-loss: 0.00031938992992763815\n",
      "[LOG 20191027-17:56:03] epoch: 1518 train-loss: 0.00031939552422954876\n",
      "[LOG 20191027-17:56:03] epoch: 1519 train-loss: 0.00031940134567776113\n",
      "[LOG 20191027-17:56:04] epoch: 1520 train-loss: 0.00031940736198521336\n",
      "[LOG 20191027-17:56:04] epoch: 1521 train-loss: 0.0003194136293132033\n",
      "[LOG 20191027-17:56:04] epoch: 1522 train-loss: 0.0003194200492089294\n",
      "[LOG 20191027-17:56:05] epoch: 1523 train-loss: 0.0003194266807895474\n",
      "[LOG 20191027-17:56:05] epoch: 1524 train-loss: 0.0003194335306488938\n",
      "[LOG 20191027-17:56:05] epoch: 1525 train-loss: 0.0003194405885551532\n",
      "[LOG 20191027-17:56:05] epoch: 1526 train-loss: 0.00031944783063408977\n",
      "[LOG 20191027-17:56:06] epoch: 1527 train-loss: 0.0003194552755303448\n",
      "[LOG 20191027-17:56:06] epoch: 1528 train-loss: 0.0003194629173322028\n",
      "[LOG 20191027-17:56:06] epoch: 1529 train-loss: 0.00031947078878147295\n",
      "[LOG 20191027-17:56:06] epoch: 1530 train-loss: 0.00031947887282512966\n",
      "[LOG 20191027-17:56:07] epoch: 1531 train-loss: 0.00031948716173246794\n",
      "[LOG 20191027-17:56:07] epoch: 1532 train-loss: 0.0003194956473180355\n",
      "[LOG 20191027-17:56:07] epoch: 1533 train-loss: 0.00031950435482031025\n",
      "[LOG 20191027-17:56:07] epoch: 1534 train-loss: 0.00031951327127899276\n",
      "[LOG 20191027-17:56:08] epoch: 1535 train-loss: 0.00031952236827237357\n",
      "[LOG 20191027-17:56:08] epoch: 1536 train-loss: 0.0003195316262463166\n",
      "[LOG 20191027-17:56:08] epoch: 1537 train-loss: 0.0003195411195520137\n",
      "[LOG 20191027-17:56:09] epoch: 1538 train-loss: 0.0003195507936197828\n",
      "[LOG 20191027-17:56:09] epoch: 1539 train-loss: 0.00031956069847183244\n",
      "[LOG 20191027-17:56:09] epoch: 1540 train-loss: 0.0003195707281520299\n",
      "[LOG 20191027-17:56:09] epoch: 1541 train-loss: 0.0003195809069893585\n",
      "[LOG 20191027-17:56:10] epoch: 1542 train-loss: 0.00031959123862179695\n",
      "[LOG 20191027-17:56:10] epoch: 1543 train-loss: 0.000319601809451342\n",
      "[LOG 20191027-17:56:10] epoch: 1544 train-loss: 0.0003196125505837699\n",
      "[LOG 20191027-17:56:10] epoch: 1545 train-loss: 0.0003196235013547266\n",
      "[LOG 20191027-17:56:11] epoch: 1546 train-loss: 0.0003196346540335071\n",
      "[LOG 20191027-17:56:11] epoch: 1547 train-loss: 0.0003196459908849647\n",
      "[LOG 20191027-17:56:11] epoch: 1548 train-loss: 0.00031965752532414626\n",
      "[LOG 20191027-17:56:12] epoch: 1549 train-loss: 0.00031966921710591123\n",
      "[LOG 20191027-17:56:12] epoch: 1550 train-loss: 0.0003196811494490248\n",
      "[LOG 20191027-17:56:12] epoch: 1551 train-loss: 0.0003196932671016839\n",
      "[LOG 20191027-17:56:12] epoch: 1552 train-loss: 0.0003197055991677189\n",
      "[LOG 20191027-17:56:13] epoch: 1553 train-loss: 0.00031971812904885155\n",
      "[LOG 20191027-17:56:13] epoch: 1554 train-loss: 0.00031973084355740866\n",
      "[LOG 20191027-17:56:13] epoch: 1555 train-loss: 0.000319743805903272\n",
      "[LOG 20191027-17:56:14] epoch: 1556 train-loss: 0.00031975695151231776\n",
      "[LOG 20191027-17:56:14] epoch: 1557 train-loss: 0.00031977029766494525\n",
      "[LOG 20191027-17:56:14] epoch: 1558 train-loss: 0.00031978385868569603\n",
      "[LOG 20191027-17:56:14] epoch: 1559 train-loss: 0.0003197975643161044\n",
      "[LOG 20191027-17:56:15] epoch: 1560 train-loss: 0.0003198114920905937\n",
      "[LOG 20191027-17:56:15] epoch: 1561 train-loss: 0.00031982559494281304\n",
      "[LOG 20191027-17:56:15] epoch: 1562 train-loss: 0.0003198399062966928\n",
      "[LOG 20191027-17:56:16] epoch: 1563 train-loss: 0.00031985443524717994\n",
      "[LOG 20191027-17:56:16] epoch: 1564 train-loss: 0.0003198690901626833\n",
      "[LOG 20191027-17:56:16] epoch: 1565 train-loss: 0.00031988389923753857\n",
      "[LOG 20191027-17:56:16] epoch: 1566 train-loss: 0.0003198989431894006\n",
      "[LOG 20191027-17:56:17] epoch: 1567 train-loss: 0.00031991414994081424\n",
      "[LOG 20191027-17:56:17] epoch: 1568 train-loss: 0.0003199295870217611\n",
      "[LOG 20191027-17:56:17] epoch: 1569 train-loss: 0.000319945188721249\n",
      "[LOG 20191027-17:56:17] epoch: 1570 train-loss: 0.0003199609652710933\n",
      "[LOG 20191027-17:56:18] epoch: 1571 train-loss: 0.000319976933496946\n",
      "[LOG 20191027-17:56:18] epoch: 1572 train-loss: 0.0003199930970367859\n",
      "[LOG 20191027-17:56:18] epoch: 1573 train-loss: 0.00032000945111576584\n",
      "[LOG 20191027-17:56:18] epoch: 1574 train-loss: 0.0003200260125595378\n",
      "[LOG 20191027-17:56:19] epoch: 1575 train-loss: 0.00032004279296415916\n",
      "[LOG 20191027-17:56:19] epoch: 1576 train-loss: 0.00032005975936044706\n",
      "[LOG 20191027-17:56:19] epoch: 1577 train-loss: 0.00032007690833779634\n",
      "[LOG 20191027-17:56:19] epoch: 1578 train-loss: 0.00032009423739509657\n",
      "[LOG 20191027-17:56:20] epoch: 1579 train-loss: 0.0003201117310709378\n",
      "[LOG 20191027-17:56:20] epoch: 1580 train-loss: 0.0003201293550318951\n",
      "[LOG 20191027-17:56:20] epoch: 1581 train-loss: 0.0003201471342890727\n",
      "[LOG 20191027-17:56:20] epoch: 1582 train-loss: 0.00032016510226640094\n",
      "[LOG 20191027-17:56:21] epoch: 1583 train-loss: 0.0003201832234935864\n",
      "[LOG 20191027-17:56:21] epoch: 1584 train-loss: 0.00032020150524658675\n",
      "[LOG 20191027-17:56:21] epoch: 1585 train-loss: 0.0003202199889074109\n",
      "[LOG 20191027-17:56:22] epoch: 1586 train-loss: 0.00032023862763708166\n",
      "[LOG 20191027-17:56:22] epoch: 1587 train-loss: 0.00032025741893448867\n",
      "[LOG 20191027-17:56:22] epoch: 1588 train-loss: 0.00032027625684349914\n",
      "[LOG 20191027-17:56:23] epoch: 1589 train-loss: 0.00032029520548348955\n",
      "[LOG 20191027-17:56:23] epoch: 1590 train-loss: 0.00032031431555878953\n",
      "[LOG 20191027-17:56:23] epoch: 1591 train-loss: 0.0003203335986654565\n",
      "[LOG 20191027-17:56:24] epoch: 1592 train-loss: 0.00032035294543675263\n",
      "[LOG 20191027-17:56:24] epoch: 1593 train-loss: 0.0003203724002105446\n",
      "[LOG 20191027-17:56:24] epoch: 1594 train-loss: 0.00032039196389632707\n",
      "[LOG 20191027-17:56:24] epoch: 1595 train-loss: 0.00032041168424257194\n",
      "[LOG 20191027-17:56:25] epoch: 1596 train-loss: 0.0003204315112270706\n",
      "[LOG 20191027-17:56:25] epoch: 1597 train-loss: 0.0003204514664503222\n",
      "[LOG 20191027-17:56:25] epoch: 1598 train-loss: 0.00032047156628323137\n",
      "[LOG 20191027-17:56:25] epoch: 1599 train-loss: 0.0003204918064056983\n",
      "[LOG 20191027-17:56:26] epoch: 1600 train-loss: 0.00032051218795459135\n",
      "[LOG 20191027-17:56:26] epoch: 1601 train-loss: 0.0003205327077466791\n",
      "[LOG 20191027-17:56:26] epoch: 1602 train-loss: 0.000320553344408836\n",
      "[LOG 20191027-17:56:27] epoch: 1603 train-loss: 0.0003205740920293465\n",
      "[LOG 20191027-17:56:27] epoch: 1604 train-loss: 0.0003205949158200383\n",
      "[LOG 20191027-17:56:27] epoch: 1605 train-loss: 0.0003206158319244423\n",
      "[LOG 20191027-17:56:27] epoch: 1606 train-loss: 0.0003206368614883104\n",
      "[LOG 20191027-17:56:28] epoch: 1607 train-loss: 0.00032065800678537926\n",
      "[LOG 20191027-17:56:28] epoch: 1608 train-loss: 0.0003206792634955491\n",
      "[LOG 20191027-17:56:28] epoch: 1609 train-loss: 0.00032070063844003016\n",
      "[LOG 20191027-17:56:28] epoch: 1610 train-loss: 0.0003207221202501387\n",
      "[LOG 20191027-17:56:29] epoch: 1611 train-loss: 0.00032074369528345414\n",
      "[LOG 20191027-17:56:29] epoch: 1612 train-loss: 0.00032076535603664524\n",
      "[LOG 20191027-17:56:29] epoch: 1613 train-loss: 0.0003207870799997181\n",
      "[LOG 20191027-17:56:29] epoch: 1614 train-loss: 0.0003208088735391357\n",
      "[LOG 20191027-17:56:30] epoch: 1615 train-loss: 0.0003208307330169191\n",
      "[LOG 20191027-17:56:30] epoch: 1616 train-loss: 0.0003208526613889262\n",
      "[LOG 20191027-17:56:30] epoch: 1617 train-loss: 0.0003208746861673717\n",
      "[LOG 20191027-17:56:31] epoch: 1618 train-loss: 0.00032089676938085177\n",
      "[LOG 20191027-17:56:31] epoch: 1619 train-loss: 0.0003209188953405828\n",
      "[LOG 20191027-17:56:31] epoch: 1620 train-loss: 0.0003209410820090852\n",
      "[LOG 20191027-17:56:31] epoch: 1621 train-loss: 0.0003209633218830277\n",
      "[LOG 20191027-17:56:32] epoch: 1622 train-loss: 0.00032098557630888536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:56:32] epoch: 1623 train-loss: 0.0003210078864412935\n",
      "[LOG 20191027-17:56:32] epoch: 1624 train-loss: 0.000321030247505405\n",
      "[LOG 20191027-17:56:32] epoch: 1625 train-loss: 0.0003210526360817312\n",
      "[LOG 20191027-17:56:33] epoch: 1626 train-loss: 0.00032107503602674115\n",
      "[LOG 20191027-17:56:33] epoch: 1627 train-loss: 0.000321097433243267\n",
      "[LOG 20191027-17:56:33] epoch: 1628 train-loss: 0.0003211198709323071\n",
      "[LOG 20191027-17:56:33] epoch: 1629 train-loss: 0.00032114231066771026\n",
      "[LOG 20191027-17:56:34] epoch: 1630 train-loss: 0.00032116476768351276\n",
      "[LOG 20191027-17:56:34] epoch: 1631 train-loss: 0.0003211872422070883\n",
      "[LOG 20191027-17:56:34] epoch: 1632 train-loss: 0.0003212097099094535\n",
      "[LOG 20191027-17:56:35] epoch: 1633 train-loss: 0.00032123220171342837\n",
      "[LOG 20191027-17:56:35] epoch: 1634 train-loss: 0.0003212546889699297\n",
      "[LOG 20191027-17:56:35] epoch: 1635 train-loss: 0.0003212771709968365\n",
      "[LOG 20191027-17:56:35] epoch: 1636 train-loss: 0.00032129960732163454\n",
      "[LOG 20191027-17:56:36] epoch: 1637 train-loss: 0.00032132197543432994\n",
      "[LOG 20191027-17:56:36] epoch: 1638 train-loss: 0.0003213443105778424\n",
      "[LOG 20191027-17:56:36] epoch: 1639 train-loss: 0.0003213666157080297\n",
      "[LOG 20191027-17:56:37] epoch: 1640 train-loss: 0.00032138890287569666\n",
      "[LOG 20191027-17:56:37] epoch: 1641 train-loss: 0.0003214111604847858\n",
      "[LOG 20191027-17:56:37] epoch: 1642 train-loss: 0.0003214334128642804\n",
      "[LOG 20191027-17:56:37] epoch: 1643 train-loss: 0.0003214556040802563\n",
      "[LOG 20191027-17:56:38] epoch: 1644 train-loss: 0.00032147770707524614\n",
      "[LOG 20191027-17:56:38] epoch: 1645 train-loss: 0.00032149974913409096\n",
      "[LOG 20191027-17:56:38] epoch: 1646 train-loss: 0.0003215217222987121\n",
      "[LOG 20191027-17:56:39] epoch: 1647 train-loss: 0.00032154359837477386\n",
      "[LOG 20191027-17:56:39] epoch: 1648 train-loss: 0.0003215654053292383\n",
      "[LOG 20191027-17:56:39] epoch: 1649 train-loss: 0.00032158709882423864\n",
      "[LOG 20191027-17:56:39] epoch: 1650 train-loss: 0.00032160870864572644\n",
      "[LOG 20191027-17:56:40] epoch: 1651 train-loss: 0.00032163021455744456\n",
      "[LOG 20191027-17:56:40] epoch: 1652 train-loss: 0.00032165161633201933\n",
      "[LOG 20191027-17:56:40] epoch: 1653 train-loss: 0.0003216729153336928\n",
      "[LOG 20191027-17:56:41] epoch: 1654 train-loss: 0.00032169406813409296\n",
      "[LOG 20191027-17:56:41] epoch: 1655 train-loss: 0.0003217150349428266\n",
      "[LOG 20191027-17:56:41] epoch: 1656 train-loss: 0.00032173585555028694\n",
      "[LOG 20191027-17:56:41] epoch: 1657 train-loss: 0.000321756531320716\n",
      "[LOG 20191027-17:56:42] epoch: 1658 train-loss: 0.00032177700290958455\n",
      "[LOG 20191027-17:56:42] epoch: 1659 train-loss: 0.00032179724871639337\n",
      "[LOG 20191027-17:56:42] epoch: 1660 train-loss: 0.00032181735218728136\n",
      "[LOG 20191027-17:56:42] epoch: 1661 train-loss: 0.0003218373030904331\n",
      "[LOG 20191027-17:56:43] epoch: 1662 train-loss: 0.00032185710233534337\n",
      "[LOG 20191027-17:56:43] epoch: 1663 train-loss: 0.0003218767005819245\n",
      "[LOG 20191027-17:56:43] epoch: 1664 train-loss: 0.00032189612579713867\n",
      "[LOG 20191027-17:56:43] epoch: 1665 train-loss: 0.0003219153604732128\n",
      "[LOG 20191027-17:56:44] epoch: 1666 train-loss: 0.00032193438937611063\n",
      "[LOG 20191027-17:56:44] epoch: 1667 train-loss: 0.00032195317794503353\n",
      "[LOG 20191027-17:56:44] epoch: 1668 train-loss: 0.00032197170889958215\n",
      "[LOG 20191027-17:56:44] epoch: 1669 train-loss: 0.00032198999565480335\n",
      "[LOG 20191027-17:56:45] epoch: 1670 train-loss: 0.00032200808800553204\n",
      "[LOG 20191027-17:56:45] epoch: 1671 train-loss: 0.00032202592296926014\n",
      "[LOG 20191027-17:56:45] epoch: 1672 train-loss: 0.00032204350395659276\n",
      "[LOG 20191027-17:56:46] epoch: 1673 train-loss: 0.00032206085279540275\n",
      "[LOG 20191027-17:56:46] epoch: 1674 train-loss: 0.00032207793037741794\n",
      "[LOG 20191027-17:56:46] epoch: 1675 train-loss: 0.0003220947210138547\n",
      "[LOG 20191027-17:56:46] epoch: 1676 train-loss: 0.00032211121038017154\n",
      "[LOG 20191027-17:56:47] epoch: 1677 train-loss: 0.00032212742871706723\n",
      "[LOG 20191027-17:56:47] epoch: 1678 train-loss: 0.0003221432834834559\n",
      "[LOG 20191027-17:56:47] epoch: 1679 train-loss: 0.00032215883629760356\n",
      "[LOG 20191027-17:56:47] epoch: 1680 train-loss: 0.0003221740330445755\n",
      "[LOG 20191027-17:56:48] epoch: 1681 train-loss: 0.00032218884621215693\n",
      "[LOG 20191027-17:56:48] epoch: 1682 train-loss: 0.00032220334196608746\n",
      "[LOG 20191027-17:56:48] epoch: 1683 train-loss: 0.00032221748915617354\n",
      "[LOG 20191027-17:56:48] epoch: 1684 train-loss: 0.00032223124526353786\n",
      "[LOG 20191027-17:56:49] epoch: 1685 train-loss: 0.00032224465735453123\n",
      "[LOG 20191027-17:56:49] epoch: 1686 train-loss: 0.00032225764880422503\n",
      "[LOG 20191027-17:56:49] epoch: 1687 train-loss: 0.0003222702175662562\n",
      "[LOG 20191027-17:56:49] epoch: 1688 train-loss: 0.0003222823781925399\n",
      "[LOG 20191027-17:56:50] epoch: 1689 train-loss: 0.0003222940763407678\n",
      "[LOG 20191027-17:56:50] epoch: 1690 train-loss: 0.00032230536635324825\n",
      "[LOG 20191027-17:56:50] epoch: 1691 train-loss: 0.00032231617615252617\n",
      "[LOG 20191027-17:56:50] epoch: 1692 train-loss: 0.00032232649368779676\n",
      "[LOG 20191027-17:56:51] epoch: 1693 train-loss: 0.00032233631895906\n",
      "[LOG 20191027-17:56:51] epoch: 1694 train-loss: 0.00032234560831057024\n",
      "[LOG 20191027-17:56:51] epoch: 1695 train-loss: 0.0003223543781132321\n",
      "[LOG 20191027-17:56:51] epoch: 1696 train-loss: 0.00032236255879070086\n",
      "[LOG 20191027-17:56:52] epoch: 1697 train-loss: 0.00032237022469416843\n",
      "[LOG 20191027-17:56:52] epoch: 1698 train-loss: 0.00032237734853879374\n",
      "[LOG 20191027-17:56:52] epoch: 1699 train-loss: 0.0003223839376005344\n",
      "[LOG 20191027-17:56:52] epoch: 1700 train-loss: 0.00032238995913758117\n",
      "[LOG 20191027-17:56:53] epoch: 1701 train-loss: 0.0003223953706310567\n",
      "[LOG 20191027-17:56:53] epoch: 1702 train-loss: 0.00032240011842077365\n",
      "[LOG 20191027-17:56:53] epoch: 1703 train-loss: 0.0003224041495286656\n",
      "[LOG 20191027-17:56:53] epoch: 1704 train-loss: 0.00032240752193501976\n",
      "[LOG 20191027-17:56:54] epoch: 1705 train-loss: 0.0003224102481453883\n",
      "[LOG 20191027-17:56:54] epoch: 1706 train-loss: 0.00032241230064755655\n",
      "[LOG 20191027-17:56:54] epoch: 1707 train-loss: 0.00032241365624940954\n",
      "[LOG 20191027-17:56:54] epoch: 1708 train-loss: 0.0003224142778890382\n",
      "[LOG 20191027-17:56:55] epoch: 1709 train-loss: 0.00032241418125522614\n",
      "[LOG 20191027-17:56:55] epoch: 1710 train-loss: 0.0003224133067760704\n",
      "[LOG 20191027-17:56:55] epoch: 1711 train-loss: 0.00032241160511148337\n",
      "[LOG 20191027-17:56:55] epoch: 1712 train-loss: 0.00032240906716651807\n",
      "[LOG 20191027-17:56:56] epoch: 1713 train-loss: 0.00032240570271824254\n",
      "[LOG 20191027-17:56:56] epoch: 1714 train-loss: 0.0003224014512852591\n",
      "[LOG 20191027-17:56:56] epoch: 1715 train-loss: 0.00032239634992947686\n",
      "[LOG 20191027-17:56:56] epoch: 1716 train-loss: 0.00032239036022474465\n",
      "[LOG 20191027-17:56:57] epoch: 1717 train-loss: 0.0003223834949039883\n",
      "[LOG 20191027-17:56:57] epoch: 1718 train-loss: 0.0003223756730221794\n",
      "[LOG 20191027-17:56:57] epoch: 1719 train-loss: 0.0003223668770715449\n",
      "[LOG 20191027-17:56:57] epoch: 1720 train-loss: 0.0003223571141006687\n",
      "[LOG 20191027-17:56:58] epoch: 1721 train-loss: 0.0003223464104848972\n",
      "[LOG 20191027-17:56:58] epoch: 1722 train-loss: 0.00032233471756626386\n",
      "[LOG 20191027-17:56:58] epoch: 1723 train-loss: 0.0003223219164283364\n",
      "[LOG 20191027-17:56:59] epoch: 1724 train-loss: 0.00032230800434263074\n",
      "[LOG 20191027-17:56:59] epoch: 1725 train-loss: 0.00032229294811259024\n",
      "[LOG 20191027-17:56:59] epoch: 1726 train-loss: 0.00032227674728346756\n",
      "[LOG 20191027-17:56:59] epoch: 1727 train-loss: 0.00032225940299213107\n",
      "[LOG 20191027-17:57:00] epoch: 1728 train-loss: 0.0003222408936380816\n",
      "[LOG 20191027-17:57:00] epoch: 1729 train-loss: 0.0003222211644242634\n",
      "[LOG 20191027-17:57:00] epoch: 1730 train-loss: 0.0003222002055736084\n",
      "[LOG 20191027-17:57:00] epoch: 1731 train-loss: 0.00032217797570410767\n",
      "[LOG 20191027-17:57:01] epoch: 1732 train-loss: 0.00032215439773608523\n",
      "[LOG 20191027-17:57:01] epoch: 1733 train-loss: 0.00032212948417509324\n",
      "[LOG 20191027-17:57:01] epoch: 1734 train-loss: 0.0003221031379325723\n",
      "[LOG 20191027-17:57:01] epoch: 1735 train-loss: 0.0003220753787900321\n",
      "[LOG 20191027-17:57:02] epoch: 1736 train-loss: 0.0003220461926503049\n",
      "[LOG 20191027-17:57:02] epoch: 1737 train-loss: 0.00032201552198785066\n",
      "[LOG 20191027-17:57:02] epoch: 1738 train-loss: 0.0003219833267849026\n",
      "[LOG 20191027-17:57:02] epoch: 1739 train-loss: 0.000321949580211367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:57:03] epoch: 1740 train-loss: 0.00032191425520977646\n",
      "[LOG 20191027-17:57:03] epoch: 1741 train-loss: 0.0003218773963453714\n",
      "[LOG 20191027-17:57:03] epoch: 1742 train-loss: 0.00032183891198656056\n",
      "[LOG 20191027-17:57:03] epoch: 1743 train-loss: 0.00032179870618165296\n",
      "[LOG 20191027-17:57:04] epoch: 1744 train-loss: 0.00032175672254197707\n",
      "[LOG 20191027-17:57:04] epoch: 1745 train-loss: 0.00032171296970773255\n",
      "[LOG 20191027-17:57:04] epoch: 1746 train-loss: 0.0003216673876522691\n",
      "[LOG 20191027-17:57:04] epoch: 1747 train-loss: 0.0003216199872895231\n",
      "[LOG 20191027-17:57:05] epoch: 1748 train-loss: 0.0003215707001800183\n",
      "[LOG 20191027-17:57:05] epoch: 1749 train-loss: 0.0003215194838048774\n",
      "[LOG 20191027-17:57:05] epoch: 1750 train-loss: 0.00032146626745088724\n",
      "[LOG 20191027-17:57:05] epoch: 1751 train-loss: 0.0003214109967757395\n",
      "[LOG 20191027-17:57:06] epoch: 1752 train-loss: 0.000321353644494593\n",
      "[LOG 20191027-17:57:06] epoch: 1753 train-loss: 0.00032129417195392307\n",
      "[LOG 20191027-17:57:06] epoch: 1754 train-loss: 0.0003212325484582834\n",
      "[LOG 20191027-17:57:06] epoch: 1755 train-loss: 0.00032116873899212806\n",
      "[LOG 20191027-17:57:07] epoch: 1756 train-loss: 0.0003211027117231424\n",
      "[LOG 20191027-17:57:07] epoch: 1757 train-loss: 0.0003210344475519378\n",
      "[LOG 20191027-17:57:07] epoch: 1758 train-loss: 0.00032096387121782755\n",
      "[LOG 20191027-17:57:08] epoch: 1759 train-loss: 0.0003208909849945485\n",
      "[LOG 20191027-17:57:08] epoch: 1760 train-loss: 0.00032081572840070294\n",
      "[LOG 20191027-17:57:08] epoch: 1761 train-loss: 0.00032073806937660265\n",
      "[LOG 20191027-17:57:08] epoch: 1762 train-loss: 0.00032065794925983937\n",
      "[LOG 20191027-17:57:09] epoch: 1763 train-loss: 0.00032057534917839803\n",
      "[LOG 20191027-17:57:09] epoch: 1764 train-loss: 0.0003204901945537131\n",
      "[LOG 20191027-17:57:09] epoch: 1765 train-loss: 0.0003204024669685168\n",
      "[LOG 20191027-17:57:09] epoch: 1766 train-loss: 0.0003203121241313056\n",
      "[LOG 20191027-17:57:10] epoch: 1767 train-loss: 0.0003202190894171508\n",
      "[LOG 20191027-17:57:10] epoch: 1768 train-loss: 0.0003201234151219978\n",
      "[LOG 20191027-17:57:10] epoch: 1769 train-loss: 0.0003200250107511238\n",
      "[LOG 20191027-17:57:10] epoch: 1770 train-loss: 0.0003199238651632186\n",
      "[LOG 20191027-17:57:11] epoch: 1771 train-loss: 0.0003198198851350753\n",
      "[LOG 20191027-17:57:11] epoch: 1772 train-loss: 0.0003197130872649723\n",
      "[LOG 20191027-17:57:11] epoch: 1773 train-loss: 0.0003196033853782865\n",
      "[LOG 20191027-17:57:11] epoch: 1774 train-loss: 0.00031949077992976527\n",
      "[LOG 20191027-17:57:12] epoch: 1775 train-loss: 0.0003193751854269067\n",
      "[LOG 20191027-17:57:12] epoch: 1776 train-loss: 0.0003192565648078016\n",
      "[LOG 20191027-17:57:12] epoch: 1777 train-loss: 0.00031913488692225656\n",
      "[LOG 20191027-17:57:12] epoch: 1778 train-loss: 0.0003190100608208013\n",
      "[LOG 20191027-17:57:13] epoch: 1779 train-loss: 0.00031888199691820773\n",
      "[LOG 20191027-17:57:13] epoch: 1780 train-loss: 0.0003187506847552868\n",
      "[LOG 20191027-17:57:13] epoch: 1781 train-loss: 0.0003186160729455878\n",
      "[LOG 20191027-17:57:13] epoch: 1782 train-loss: 0.0003184780391620734\n",
      "[LOG 20191027-17:57:14] epoch: 1783 train-loss: 0.000318336576810907\n",
      "[LOG 20191027-17:57:14] epoch: 1784 train-loss: 0.0003181916515586636\n",
      "[LOG 20191027-17:57:14] epoch: 1785 train-loss: 0.0003180432379394915\n",
      "[LOG 20191027-17:57:14] epoch: 1786 train-loss: 0.000317891237727963\n",
      "[LOG 20191027-17:57:15] epoch: 1787 train-loss: 0.00031773556975167594\n",
      "[LOG 20191027-17:57:15] epoch: 1788 train-loss: 0.00031757612714500283\n",
      "[LOG 20191027-17:57:15] epoch: 1789 train-loss: 0.0003174128617047245\n",
      "[LOG 20191027-17:57:15] epoch: 1790 train-loss: 0.0003172457634263992\n",
      "[LOG 20191027-17:57:16] epoch: 1791 train-loss: 0.00031707477251075034\n",
      "[LOG 20191027-17:57:16] epoch: 1792 train-loss: 0.0003168998312048643\n",
      "[LOG 20191027-17:57:16] epoch: 1793 train-loss: 0.00031672083855482924\n",
      "[LOG 20191027-17:57:16] epoch: 1794 train-loss: 0.00031653775181439414\n",
      "[LOG 20191027-17:57:17] epoch: 1795 train-loss: 0.0003163505054999405\n",
      "[LOG 20191027-17:57:17] epoch: 1796 train-loss: 0.0003161590250329027\n",
      "[LOG 20191027-17:57:17] epoch: 1797 train-loss: 0.0003159632888127817\n",
      "[LOG 20191027-17:57:17] epoch: 1798 train-loss: 0.00031576328888149874\n",
      "[LOG 20191027-17:57:18] epoch: 1799 train-loss: 0.00031555894952361996\n",
      "[LOG 20191027-17:57:18] epoch: 1800 train-loss: 0.00031535019456896407\n",
      "[LOG 20191027-17:57:18] epoch: 1801 train-loss: 0.0003151370663090347\n",
      "[LOG 20191027-17:57:18] epoch: 1802 train-loss: 0.00031491957793150505\n",
      "[LOG 20191027-17:57:19] epoch: 1803 train-loss: 0.0003146977078358759\n",
      "[LOG 20191027-17:57:19] epoch: 1804 train-loss: 0.00031447140008822316\n",
      "[LOG 20191027-17:57:19] epoch: 1805 train-loss: 0.0003142406230836059\n",
      "[LOG 20191027-17:57:19] epoch: 1806 train-loss: 0.00031400533589476254\n",
      "[LOG 20191027-17:57:20] epoch: 1807 train-loss: 0.000313765587634407\n",
      "[LOG 20191027-17:57:20] epoch: 1808 train-loss: 0.00031352145128948905\n",
      "[LOG 20191027-17:57:20] epoch: 1809 train-loss: 0.00031327301007877395\n",
      "[LOG 20191027-17:57:20] epoch: 1810 train-loss: 0.00031302028196478204\n",
      "[LOG 20191027-17:57:21] epoch: 1811 train-loss: 0.00031276332583729527\n",
      "[LOG 20191027-17:57:21] epoch: 1812 train-loss: 0.00031250224606083066\n",
      "[LOG 20191027-17:57:21] epoch: 1813 train-loss: 0.00031223718406181433\n",
      "[LOG 20191027-17:57:21] epoch: 1814 train-loss: 0.00031196826694213087\n",
      "[LOG 20191027-17:57:22] epoch: 1815 train-loss: 0.0003116956554549688\n",
      "[LOG 20191027-17:57:22] epoch: 1816 train-loss: 0.00031141954650593107\n",
      "[LOG 20191027-17:57:22] epoch: 1817 train-loss: 0.00031114022090150684\n",
      "[LOG 20191027-17:57:23] epoch: 1818 train-loss: 0.00031085783007256396\n",
      "[LOG 20191027-17:57:23] epoch: 1819 train-loss: 0.0003105725641034951\n",
      "[LOG 20191027-17:57:23] epoch: 1820 train-loss: 0.00031028469607008446\n",
      "[LOG 20191027-17:57:23] epoch: 1821 train-loss: 0.00030999456885183463\n",
      "[LOG 20191027-17:57:24] epoch: 1822 train-loss: 0.0003097023925420217\n",
      "[LOG 20191027-17:57:24] epoch: 1823 train-loss: 0.0003094084495387506\n",
      "[LOG 20191027-17:57:24] epoch: 1824 train-loss: 0.0003091130886332394\n",
      "[LOG 20191027-17:57:24] epoch: 1825 train-loss: 0.00030881671909810393\n",
      "[LOG 20191027-17:57:25] epoch: 1826 train-loss: 0.0003085196440224536\n",
      "[LOG 20191027-17:57:25] epoch: 1827 train-loss: 0.00030822220378468046\n",
      "[LOG 20191027-17:57:25] epoch: 1828 train-loss: 0.0003079248251651734\n",
      "[LOG 20191027-17:57:25] epoch: 1829 train-loss: 0.0003076278644584818\n",
      "[LOG 20191027-17:57:26] epoch: 1830 train-loss: 0.00030733169364793866\n",
      "[LOG 20191027-17:57:26] epoch: 1831 train-loss: 0.00030703670540788153\n",
      "[LOG 20191027-17:57:26] epoch: 1832 train-loss: 0.0003067432837724482\n",
      "[LOG 20191027-17:57:26] epoch: 1833 train-loss: 0.0003064517600250838\n",
      "[LOG 20191027-17:57:27] epoch: 1834 train-loss: 0.000306162497963669\n",
      "[LOG 20191027-17:57:27] epoch: 1835 train-loss: 0.0003058759270970768\n",
      "[LOG 20191027-17:57:27] epoch: 1836 train-loss: 0.0003055923373267433\n",
      "[LOG 20191027-17:57:27] epoch: 1837 train-loss: 0.00030531202742167807\n",
      "[LOG 20191027-17:57:28] epoch: 1838 train-loss: 0.0003050353298021946\n",
      "[LOG 20191027-17:57:28] epoch: 1839 train-loss: 0.00030476260440082115\n",
      "[LOG 20191027-17:57:28] epoch: 1840 train-loss: 0.0003044940804102225\n",
      "[LOG 20191027-17:57:28] epoch: 1841 train-loss: 0.00030423005023294536\n",
      "[LOG 20191027-17:57:29] epoch: 1842 train-loss: 0.000303970739196302\n",
      "[LOG 20191027-17:57:29] epoch: 1843 train-loss: 0.000303716411053756\n",
      "[LOG 20191027-17:57:29] epoch: 1844 train-loss: 0.0003034672943158512\n",
      "[LOG 20191027-17:57:29] epoch: 1845 train-loss: 0.00030322354882628133\n",
      "[LOG 20191027-17:57:30] epoch: 1846 train-loss: 0.000302985368989539\n",
      "[LOG 20191027-17:57:30] epoch: 1847 train-loss: 0.00030275292760961747\n",
      "[LOG 20191027-17:57:30] epoch: 1848 train-loss: 0.0003025263274594181\n",
      "[LOG 20191027-17:57:30] epoch: 1849 train-loss: 0.0003023056819984049\n",
      "[LOG 20191027-17:57:31] epoch: 1850 train-loss: 0.00030209113333512505\n",
      "[LOG 20191027-17:57:31] epoch: 1851 train-loss: 0.00030188276014087023\n",
      "[LOG 20191027-17:57:31] epoch: 1852 train-loss: 0.0003016805951574497\n",
      "[LOG 20191027-17:57:32] epoch: 1853 train-loss: 0.00030148471682878153\n",
      "[LOG 20191027-17:57:32] epoch: 1854 train-loss: 0.00030129519655019976\n",
      "[LOG 20191027-17:57:32] epoch: 1855 train-loss: 0.00030111205524008255\n",
      "[LOG 20191027-17:57:32] epoch: 1856 train-loss: 0.0003009352633398521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:57:33] epoch: 1857 train-loss: 0.00030076486109464895\n",
      "[LOG 20191027-17:57:33] epoch: 1858 train-loss: 0.0003006008714692143\n",
      "[LOG 20191027-17:57:33] epoch: 1859 train-loss: 0.00030044324148548185\n",
      "[LOG 20191027-17:57:33] epoch: 1860 train-loss: 0.00030029194113012636\n",
      "[LOG 20191027-17:57:34] epoch: 1861 train-loss: 0.0003001469431183068\n",
      "[LOG 20191027-17:57:34] epoch: 1862 train-loss: 0.0003000082137987192\n",
      "[LOG 20191027-17:57:34] epoch: 1863 train-loss: 0.0002998757124714757\n",
      "[LOG 20191027-17:57:34] epoch: 1864 train-loss: 0.0002997493672864948\n",
      "[LOG 20191027-17:57:35] epoch: 1865 train-loss: 0.0002996291279941943\n",
      "[LOG 20191027-17:57:35] epoch: 1866 train-loss: 0.00029951493843327626\n",
      "[LOG 20191027-17:57:35] epoch: 1867 train-loss: 0.0002994067297095171\n",
      "[LOG 20191027-17:57:35] epoch: 1868 train-loss: 0.0002993044258801092\n",
      "[LOG 20191027-17:57:36] epoch: 1869 train-loss: 0.000299207925763767\n",
      "[LOG 20191027-17:57:36] epoch: 1870 train-loss: 0.0002991171613757615\n",
      "[LOG 20191027-17:57:36] epoch: 1871 train-loss: 0.0002990320463140961\n",
      "[LOG 20191027-17:57:36] epoch: 1872 train-loss: 0.00029895250327172107\n",
      "[LOG 20191027-17:57:37] epoch: 1873 train-loss: 0.0002988784394801769\n",
      "[LOG 20191027-17:57:37] epoch: 1874 train-loss: 0.0002988097471643414\n",
      "[LOG 20191027-17:57:37] epoch: 1875 train-loss: 0.0002987463353747444\n",
      "[LOG 20191027-17:57:37] epoch: 1876 train-loss: 0.0002986881313518097\n",
      "[LOG 20191027-17:57:38] epoch: 1877 train-loss: 0.00029863502618354687\n",
      "[LOG 20191027-17:57:38] epoch: 1878 train-loss: 0.000298586903454634\n",
      "[LOG 20191027-17:57:38] epoch: 1879 train-loss: 0.0002985436776725692\n",
      "[LOG 20191027-17:57:38] epoch: 1880 train-loss: 0.00029850524629182473\n",
      "[LOG 20191027-17:57:39] epoch: 1881 train-loss: 0.00029847150881323614\n",
      "[LOG 20191027-17:57:39] epoch: 1882 train-loss: 0.00029844236928511236\n",
      "[LOG 20191027-17:57:39] epoch: 1883 train-loss: 0.00029841772720828885\n",
      "[LOG 20191027-17:57:39] epoch: 1884 train-loss: 0.0002983974770813802\n",
      "[LOG 20191027-17:57:40] epoch: 1885 train-loss: 0.00029838151385774836\n",
      "[LOG 20191027-17:57:40] epoch: 1886 train-loss: 0.0002983697147556086\n",
      "[LOG 20191027-17:57:40] epoch: 1887 train-loss: 0.0002983619806400384\n",
      "[LOG 20191027-17:57:40] epoch: 1888 train-loss: 0.0002983582294291409\n",
      "[LOG 20191027-17:57:41] epoch: 1889 train-loss: 0.00029835834675395745\n",
      "[LOG 20191027-17:57:41] epoch: 1890 train-loss: 0.00029836219391654595\n",
      "[LOG 20191027-17:57:41] epoch: 1891 train-loss: 0.0002983696886076359\n",
      "[LOG 20191027-17:57:41] epoch: 1892 train-loss: 0.0002983807414693729\n",
      "[LOG 20191027-17:57:42] epoch: 1893 train-loss: 0.0002983952745125862\n",
      "[LOG 20191027-17:57:42] epoch: 1894 train-loss: 0.00029841314290024457\n",
      "[LOG 20191027-17:57:42] epoch: 1895 train-loss: 0.00029843428910680814\n",
      "[LOG 20191027-17:57:42] epoch: 1896 train-loss: 0.0002984585739795875\n",
      "[LOG 20191027-17:57:43] epoch: 1897 train-loss: 0.000298485875418919\n",
      "[LOG 20191027-17:57:43] epoch: 1898 train-loss: 0.00029851604972463974\n",
      "[LOG 20191027-17:57:43] epoch: 1899 train-loss: 0.00029854906301807205\n",
      "[LOG 20191027-17:57:43] epoch: 1900 train-loss: 0.00029858484390388185\n",
      "[LOG 20191027-17:57:44] epoch: 1901 train-loss: 0.00029862325141039037\n",
      "[LOG 20191027-17:57:44] epoch: 1902 train-loss: 0.00029866416275581287\n",
      "[LOG 20191027-17:57:44] epoch: 1903 train-loss: 0.0002987075311011722\n",
      "[LOG 20191027-17:57:45] epoch: 1904 train-loss: 0.0002987532368479151\n",
      "[LOG 20191027-17:57:45] epoch: 1905 train-loss: 0.00029880115948799357\n",
      "[LOG 20191027-17:57:45] epoch: 1906 train-loss: 0.00029885122739869985\n",
      "[LOG 20191027-17:57:45] epoch: 1907 train-loss: 0.0002989033537232899\n",
      "[LOG 20191027-17:57:46] epoch: 1908 train-loss: 0.0002989574657021876\n",
      "[LOG 20191027-17:57:46] epoch: 1909 train-loss: 0.0002990134619267337\n",
      "[LOG 20191027-17:57:46] epoch: 1910 train-loss: 0.0002990712271184748\n",
      "[LOG 20191027-17:57:46] epoch: 1911 train-loss: 0.0002991306398598681\n",
      "[LOG 20191027-17:57:47] epoch: 1912 train-loss: 0.0002991916478549683\n",
      "[LOG 20191027-17:57:47] epoch: 1913 train-loss: 0.00029925415401521605\n",
      "[LOG 20191027-17:57:47] epoch: 1914 train-loss: 0.00029931802532701113\n",
      "[LOG 20191027-17:57:47] epoch: 1915 train-loss: 0.0002993831747062359\n",
      "[LOG 20191027-17:57:48] epoch: 1916 train-loss: 0.00029944958782834874\n",
      "[LOG 20191027-17:57:48] epoch: 1917 train-loss: 0.00029951713759146514\n",
      "[LOG 20191027-17:57:48] epoch: 1918 train-loss: 0.00029958570007693197\n",
      "[LOG 20191027-17:57:48] epoch: 1919 train-loss: 0.00029965521116537275\n",
      "[LOG 20191027-17:57:49] epoch: 1920 train-loss: 0.0002997255771788332\n",
      "[LOG 20191027-17:57:49] epoch: 1921 train-loss: 0.0002997967173996585\n",
      "[LOG 20191027-17:57:49] epoch: 1922 train-loss: 0.00029986855679453583\n",
      "[LOG 20191027-17:57:49] epoch: 1923 train-loss: 0.0002999409734911751\n",
      "[LOG 20191027-17:57:50] epoch: 1924 train-loss: 0.0003000138613060699\n",
      "[LOG 20191027-17:57:50] epoch: 1925 train-loss: 0.0003000871129188454\n",
      "[LOG 20191027-17:57:50] epoch: 1926 train-loss: 0.00030016069149496616\n",
      "[LOG 20191027-17:57:50] epoch: 1927 train-loss: 0.0003002345085860725\n",
      "[LOG 20191027-17:57:51] epoch: 1928 train-loss: 0.0003003084616466367\n",
      "[LOG 20191027-17:57:51] epoch: 1929 train-loss: 0.00030038238310226006\n",
      "[LOG 20191027-17:57:51] epoch: 1930 train-loss: 0.0003004561788202409\n",
      "[LOG 20191027-17:57:51] epoch: 1931 train-loss: 0.00030052977058403485\n",
      "[LOG 20191027-17:57:52] epoch: 1932 train-loss: 0.0003006030681262928\n",
      "[LOG 20191027-17:57:52] epoch: 1933 train-loss: 0.0003006759873187548\n",
      "[LOG 20191027-17:57:52] epoch: 1934 train-loss: 0.0003007484342560929\n",
      "[LOG 20191027-17:57:52] epoch: 1935 train-loss: 0.0003008202663750126\n",
      "[LOG 20191027-17:57:53] epoch: 1936 train-loss: 0.00030089142182987416\n",
      "[LOG 20191027-17:57:53] epoch: 1937 train-loss: 0.0003009617498719308\n",
      "[LOG 20191027-17:57:53] epoch: 1938 train-loss: 0.0003010311679645383\n",
      "[LOG 20191027-17:57:53] epoch: 1939 train-loss: 0.000301099640182656\n",
      "[LOG 20191027-17:57:54] epoch: 1940 train-loss: 0.0003011670573869196\n",
      "[LOG 20191027-17:57:54] epoch: 1941 train-loss: 0.00030123320993880043\n",
      "[LOG 20191027-17:57:54] epoch: 1942 train-loss: 0.0003012980944276933\n",
      "[LOG 20191027-17:57:54] epoch: 1943 train-loss: 0.0003013615662439406\n",
      "[LOG 20191027-17:57:55] epoch: 1944 train-loss: 0.0003014235505816032\n",
      "[LOG 20191027-17:57:55] epoch: 1945 train-loss: 0.0003014838598573988\n",
      "[LOG 20191027-17:57:55] epoch: 1946 train-loss: 0.00030154242494973005\n",
      "[LOG 20191027-17:57:55] epoch: 1947 train-loss: 0.000301599090335003\n",
      "[LOG 20191027-17:57:56] epoch: 1948 train-loss: 0.0003016538096289878\n",
      "[LOG 20191027-17:57:56] epoch: 1949 train-loss: 0.0003017065484982595\n",
      "[LOG 20191027-17:57:56] epoch: 1950 train-loss: 0.0003017571887085069\n",
      "[LOG 20191027-17:57:56] epoch: 1951 train-loss: 0.0003018055397205899\n",
      "[LOG 20191027-17:57:57] epoch: 1952 train-loss: 0.0003018515506028052\n",
      "[LOG 20191027-17:57:57] epoch: 1953 train-loss: 0.00030189505696398555\n",
      "[LOG 20191027-17:57:57] epoch: 1954 train-loss: 0.0003019359835434443\n",
      "[LOG 20191027-17:57:57] epoch: 1955 train-loss: 0.0003019741986918234\n",
      "[LOG 20191027-17:57:58] epoch: 1956 train-loss: 0.0003020095880401641\n",
      "[LOG 20191027-17:57:58] epoch: 1957 train-loss: 0.0003020419974291144\n",
      "[LOG 20191027-17:57:58] epoch: 1958 train-loss: 0.00030207128384063253\n",
      "[LOG 20191027-17:57:59] epoch: 1959 train-loss: 0.00030209735450625885\n",
      "[LOG 20191027-17:57:59] epoch: 1960 train-loss: 0.00030212002548068995\n",
      "[LOG 20191027-17:57:59] epoch: 1961 train-loss: 0.0003021390764388343\n",
      "[LOG 20191027-17:57:59] epoch: 1962 train-loss: 0.0003021544557668676\n",
      "[LOG 20191027-17:58:00] epoch: 1963 train-loss: 0.00030216597542676027\n",
      "[LOG 20191027-17:58:00] epoch: 1964 train-loss: 0.00030217349694794393\n",
      "[LOG 20191027-17:58:00] epoch: 1965 train-loss: 0.00030217681637623173\n",
      "[LOG 20191027-17:58:00] epoch: 1966 train-loss: 0.0003021757315764262\n",
      "[LOG 20191027-17:58:01] epoch: 1967 train-loss: 0.00030217010976230085\n",
      "[LOG 20191027-17:58:01] epoch: 1968 train-loss: 0.00030215975266401074\n",
      "[LOG 20191027-17:58:01] epoch: 1969 train-loss: 0.00030214449543564115\n",
      "[LOG 20191027-17:58:01] epoch: 1970 train-loss: 0.0003021241172973532\n",
      "[LOG 20191027-17:58:02] epoch: 1971 train-loss: 0.0003020983788246667\n",
      "[LOG 20191027-17:58:02] epoch: 1972 train-loss: 0.00030206714404812374\n",
      "[LOG 20191027-17:58:02] epoch: 1973 train-loss: 0.00030203015830920776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191027-17:58:03] epoch: 1974 train-loss: 0.00030198725744412513\n",
      "[LOG 20191027-17:58:03] epoch: 1975 train-loss: 0.0003019382113507163\n",
      "[LOG 20191027-17:58:03] epoch: 1976 train-loss: 0.0003018828588210454\n",
      "[LOG 20191027-17:58:03] epoch: 1977 train-loss: 0.0003018210504706076\n",
      "[LOG 20191027-17:58:04] epoch: 1978 train-loss: 0.0003017526059920783\n",
      "[LOG 20191027-17:58:04] epoch: 1979 train-loss: 0.00030167731142682896\n",
      "[LOG 20191027-17:58:04] epoch: 1980 train-loss: 0.00030159499988258176\n",
      "[LOG 20191027-17:58:04] epoch: 1981 train-loss: 0.00030150548582241754\n",
      "[LOG 20191027-17:58:05] epoch: 1982 train-loss: 0.0003014085866652749\n",
      "[LOG 20191027-17:58:05] epoch: 1983 train-loss: 0.00030130406321404735\n",
      "[LOG 20191027-17:58:05] epoch: 1984 train-loss: 0.0003011919156961085\n",
      "[LOG 20191027-17:58:06] epoch: 1985 train-loss: 0.00030107214297458995\n",
      "[LOG 20191027-17:58:06] epoch: 1986 train-loss: 0.0003009446509167901\n",
      "[LOG 20191027-17:58:06] epoch: 1987 train-loss: 0.0003008094172400888\n",
      "[LOG 20191027-17:58:07] epoch: 1988 train-loss: 0.0003006664796885161\n",
      "[LOG 20191027-17:58:07] epoch: 1989 train-loss: 0.00030051585713408713\n",
      "[LOG 20191027-17:58:07] epoch: 1990 train-loss: 0.0003003576607625291\n",
      "[LOG 20191027-17:58:07] epoch: 1991 train-loss: 0.00030019199471098545\n",
      "[LOG 20191027-17:58:08] epoch: 1992 train-loss: 0.0003000190313287021\n",
      "[LOG 20191027-17:58:08] epoch: 1993 train-loss: 0.0002998390170887433\n",
      "[LOG 20191027-17:58:08] epoch: 1994 train-loss: 0.0002996522596276918\n",
      "[LOG 20191027-17:58:08] epoch: 1995 train-loss: 0.0002994590240632533\n",
      "[LOG 20191027-17:58:09] epoch: 1996 train-loss: 0.00029925963826826774\n",
      "[LOG 20191027-17:58:09] epoch: 1997 train-loss: 0.0002990544521708216\n",
      "[LOG 20191027-17:58:09] epoch: 1998 train-loss: 0.0002988439910041052\n",
      "[LOG 20191027-17:58:09] epoch: 1999 train-loss: 0.00029862869041608064\n"
     ]
    }
   ],
   "source": [
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set the model in training mode\n",
    "lstm_model.train()\n",
    "\n",
    "# init the best loss\n",
    "best_loss = 100.00\n",
    "\n",
    "# iterate over epochs\n",
    "for epoch in range(0, num_epochs):\n",
    "\n",
    "    # init collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "            \n",
    "    # iterate over mini-batches\n",
    "    for sequence_batch, target_batch in dataloader:\n",
    "\n",
    "        # predict sequence output\n",
    "        prediction_batch = lstm_model(sequence_batch)\n",
    "\n",
    "        # calculate batch loss\n",
    "        batch_loss = loss_function(prediction_batch, target_batch)\n",
    "\n",
    "        # run backward gradient calculation\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # update network parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # collect mini-batch loss\n",
    "        train_mini_batch_losses.append(batch_loss.data.item())\n",
    "            \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "    \n",
    "    # print epoch loss\n",
    "    now = dt.datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] epoch: {} train-loss: {}'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "    \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "        \n",
    "    # print epoch and save models\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        \n",
    "        # case: new best model trained\n",
    "        if train_epoch_loss < best_loss:\n",
    "                        \n",
    "            # store new best model\n",
    "            model_name = 'awsome_lstm_model_{}.pth'.format(str(epoch))\n",
    "            torch.save(lstm_model.state_dict(), os.path.join(\"./models\", model_name))\n",
    "            \n",
    "            # update best loss\n",
    "            best_loss = train_epoch_loss\n",
    "            \n",
    "            # print epoch loss\n",
    "            now = dt.datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "            print('[LOG {}] epoch: {} new best train-loss: {} found'.format(str(now), str(epoch), str(train_epoch_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successful training let's visualize and inspect the training loss per epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' prediction error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
    "ax.set_ylabel(\"[Prediction Error $\\mathcal{L}^{MSE}$]\", fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Epochs $e_i$ vs. Prediction Error $L^{MSE}$', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, fantastic. The training error is nicely going down. We could train the network a couple more epochs until the error converges. But let's stay with the 2'000 training epochs for now and continue with evaluating our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation of the Trained Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will conduct a visual comparison of the predicted daily returns to the actual ('true') daily returns. The comparison will encompass the daily returns of the in-sample time period as well as the returns of the out-of-sample time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. In-Sample Evaluation of the Trained Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluating our model, let's load the best performing model or an already pre-trained model (as done below). Remember, that we stored a snapshot of the model after each training epoch to our local model directory. We will now load one of the (hopefully well-performing) snapshots saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/awsome_lstm_model_12250.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-13dd905f628e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'awsome_lstm_model_12250.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/awsome_lstm_model_12250.pth'"
     ]
    }
   ],
   "source": [
    "model_name = 'awsome_lstm_model_12250.pth'\n",
    "lstm_model.load_state_dict(torch.load(os.path.join(\"./models\", model_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect if the model was loaded successfully: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model in evaluation mode\n",
    "lstm_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine predictions of the in-sample population of sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't calculate gradients\n",
    "with torch.no_grad():\n",
    "\n",
    "    # predict sequence output\n",
    "    predictions = lstm_model(train_sequences_input)\n",
    "\n",
    "    # collect prediction batch results\n",
    "    predictions_list = predictions.detach().numpy()[:, -1].tolist()\n",
    "\n",
    "    # collect target batch results\n",
    "    targets_list = train_sequences_target.numpy()[:, -1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the lstm neural network in-sample predictions vs. the actual \"ground-truth\" adjusted daily returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction results\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(pd.to_datetime(stock_data.index).values[0:train_sequences.shape[0],], targets_list, color='C1', label='groundtruth (green)')\n",
    "ax.plot(pd.to_datetime(stock_data.index).values[0:train_sequences.shape[0],], predictions_list, color='C0', label='predictions (blue)')\n",
    "\n",
    "# set y-axis limits\n",
    "ax.set_xlim(pd.to_datetime(stock_data.index).values[0], pd.to_datetime(stock_data.index).values[train_sequences.shape[0]])\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"lower right\", numpoints=1, fancybox=True)\n",
    "\n",
    "plt.title('LSTM NN In-Sample Prediction vs. Ground-Truth Market Prices', fontsize=10)\n",
    "plt.xlabel('[time]', fontsize=8)\n",
    "plt.ylabel('[market price]', fontsize=8)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Out-of-Sample Evaluation of the Trained Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine predictions of the out-of-sample population of sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't calculate gradients\n",
    "with torch.no_grad():\n",
    "\n",
    "    # predict sequence output\n",
    "    predictions = lstm_model(valid_sequences_input)\n",
    "\n",
    "    # collect prediction batch results\n",
    "    predictions_list = predictions.detach().numpy()[:, -1].tolist()\n",
    "\n",
    "    # collect target batch results\n",
    "    targets_list = valid_sequences_target.numpy()[:, -1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the lstm neural network out-of-sample predictions vs. the actual \"ground-truth\" adjusted daily returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction results\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],], targets_list, color='C1', label='groundtruth (green)')\n",
    "ax.plot(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],], predictions_list, color='C0', label='predictions (blue)')\n",
    "\n",
    "# set y-axis limits\n",
    "ax.set_xlim(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]], pd.to_datetime(stock_data.index).values[train_sequences.shape[0]+valid_sequences.shape[0]])\n",
    "#ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"lower right\", numpoints=1, fancybox=True)\n",
    "\n",
    "plt.title('LSTM NN Out-of-Sample Prediction vs. Ground-Truth Market Prices', fontsize=10)\n",
    "plt.xlabel('[time]', fontsize=8)\n",
    "plt.ylabel('[market price]', fontsize=8)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Backtest of the Trained Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will backtest the python `bt` library. Python `bt` is a flexible, backtest framework that can be used to test quantitative trading strategies. In general, backtesting is the process of testing a strategy over a given data set (more details about the `bt` library can be found via: https://pmorissette.github.io/bt/). \n",
    "\n",
    "In order to test the predictions derived from the LSTM model we will view its predictions as trade signals. Thereby, we will interpret any positive future return prediction $r_{t+1} > 0.0$ of a sequence $s$ as a \"long\" (buy) signal. Likewise, we will interpret any negative future return prediction $r_{t+1} < 0.0$ of a sequence $s$ as a \"short\" (sell) signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. LSTM Trading Signal Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by converting the out-of-sample model predictions into a trading signal as described above. Therefore, we first convert the obtained predictions into a Pandas data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_daily_predictions = pd.DataFrame(predictions_list, columns=['PREDICTIONS'])\n",
    "stock_daily_predictions = stock_daily_predictions.set_index(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's briefly ensure the successful conversion by inspecting the top 5 rows of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_daily_predictions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's derive a trading signal from the converted predictions. As already described, we will generate the trading signal $\\phi$ according to the following function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "$\n",
    "\\\\\n",
    "\\phi(\\hat{r}_{t+1})=\n",
    "\\begin{cases}\n",
    "1.0, & for & \\hat{r}_{t+1} > 0.0\\\\\n",
    "-1.0, & for & \\hat{r}_{t+1} < 0.0\\\\\n",
    "\\end{cases}\n",
    "$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{r}_{t+1}$ denotes a by the model predicted future return at time $t+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = pd.DataFrame(np.where(stock_daily_predictions['PREDICTIONS'] > 0.0, 1.0, -1.0), columns=['SIGNAL'])\n",
    "signal_data = signal_data.set_index(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the top 5 rows of the prepared trading signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Stock Market Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare the daily adjusted closing prices so that they can be utilized in the backtest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_market_data = pd.DataFrame(stock_data['Adj. Close'])\n",
    "stock_market_data = stock_market_data.rename(columns={'Adj. Close': 'PRICE'})\n",
    "stock_market_data = stock_market_data.set_index(pd.to_datetime(stock_data.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the top 5 rows of the prepared adjusted closing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_market_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-sample the prepared daily adjusted closing prices to the out-of-sample time period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_market_data = stock_market_data[stock_market_data.index >= stock_daily_predictions.index[0]]\n",
    "stock_market_data = stock_market_data[stock_market_data.index <= stock_daily_predictions.index[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the out-of-sample daily adjusted closing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(stock_market_data['PRICE'], color='#9b59b6')\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "    \n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "#ax.set_xlim(pd.to_datetime(stock_market_data.index).values[train_sequences.shape[0]], pd.to_datetime(stock_data.index).values[train_sequences.shape[0]+valid_sequences.shape[0]])\n",
    "ax.set_ylabel('[equity %]', fontsize=10)\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim([stock_market_data.index[0], stock_market_data.index[-1]])\n",
    "ax.set_ylabel('[adj. closing price]', fontsize=10)\n",
    "\n",
    "# set plot title\n",
    "plt.title('International Business Machines Corporation (IBM) - Daily Historical Stock Closing Prices', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate our potential gained return by the application of a simple \"buy and hold\" strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(stock_market_data.iloc[0]['PRICE'] - stock_market_data.iloc[-1]['PRICE']) / stock_market_data.iloc[0]['PRICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Backtest Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the \"Moving Average\" trading strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMStrategy(bt.Algo):\n",
    "    \n",
    "    def __init__(self, signals):\n",
    "        \n",
    "        # set class signals\n",
    "        self.signals = signals\n",
    "        \n",
    "    def __call__(self, target):\n",
    "        \n",
    "        if target.now in self.signals.index[1:]:\n",
    "            \n",
    "            # get actual signal\n",
    "            signal = self.signals.ix[target.now]\n",
    "            \n",
    "            # set target weights according to signal\n",
    "            target.temp['weights'] = dict(PRICE=signal)\n",
    "            \n",
    "        # return True since we want to move on to the next timestep\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our LSTM based trading strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_strategy = bt.Strategy('lstm', [bt.algos.SelectAll(), LSTMStrategy(signal_data['SIGNAL']), bt.algos.Rebalance()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the backtest of our LSTM based trading strategy using the strategy and prepared market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_lstm = bt.Backtest(strategy=lstm_strategy, data=stock_market_data, name='stock_lstm_backtest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's also prepare a backtest of a \"baseline\" buy-and-hold trading strategy for comparison purposes. Our buy-and-hold strategy sends a \"long\" (+1.0) signal at each time step of the out-of-sample time frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data_base = signal_data.copy(deep=True) \n",
    "signal_data_base['SIGNAL'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init the buy-and-hold (\"base\") strategy as well as the corresponding backtest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_strategy = bt.Strategy('base', [bt.algos.SelectAll(), LSTMStrategy(signal_data_base['SIGNAL']), bt.algos.Rebalance()])\n",
    "backtest_base = bt.Backtest(strategy=base_strategy, data=stock_market_data, name='stock_base_backtest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4. Running the Backtest and Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the backtest for both trading strategies: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_results = bt.run(backtest_lstm, backtest_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the individual backtest results and performance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_results.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the LSTM trading strategy backtest details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_lstm_details = backtest_lstm.strategy.data\n",
    "backtest_lstm_details.columns = ['% EQUITY', 'EQUITY', 'CASH', 'FEES']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the LSTM trading strategy backtest details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_lstm_details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the \"buy-and-hold\" trading strategy backtest details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_base_details = backtest_base.strategy.data\n",
    "backtest_base_details.columns = ['% EQUITY', 'EQUITY', 'CASH', 'FEES']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the \"buy-and-hold\" trading strategy backtest details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_base_details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the equity progression of both strategies over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(backtest_lstm_details['% EQUITY'], color='C1',lw=1.0, label='lstm strategy (green)')\n",
    "ax.plot(backtest_base_details['% EQUITY'], color='C2',lw=1.0, label='base strategy (red)')\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "    \n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]], pd.to_datetime(stock_data.index).values[train_sequences.shape[0]+valid_sequences.shape[0]])\n",
    "ax.set_ylabel('[equity %]', fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# set plot title\n",
    "plt.title('International Business Machines Corporation (IBM) - Backtest % Equity Progression', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following exercises as part of the lab:\n",
    "\n",
    "**1. Evaluation of Shallow vs. Deep RNN Models.**\n",
    "\n",
    "> Download the daily adjusted closing prices of the IBM stock within the time frame starting from 01/01/1990 until 05/31/2019. In addition to the architecture of the lab notebook, evaluate further (more shallow as well as more deep) RNN architectures by (1) either re-moving/adding layers of LSTM cells and/or (2) increasing/decreasing the dimensionality of the LSTM cells hidden state. Train your model (using architectures you selected) for at least 20'000 training epochs but keep the following parameters unchanged (a) sequence length: 5 time-steps (days) and (b) train vs. test fraction: 0.9.\n",
    "\n",
    "> Analyze the prediction performance of the trained models in terms of training time and prediction accuracy. Furthermore, backtest the out-of-sample signals predicted by each of your models and evaluate them in terms of total return and equity progression. Which of your architecture results in the best performing model, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Training and Evaluation of Models Learned from Additional Stocks.**\n",
    "\n",
    "> Download the daily adjusted closing prices of at least two additional stocks (e.g., Alphabet, Deutsche Bank) within the time frame starting from 01/01/1990 until 05/31/2017. Pls. select two stocks that you are interested in to investigate ( e.g. stocks that you may occasionally trade yourself). Learn an ’optimal’ RNN model of both stocks and backtest their corresponding trade signals by following the approach outlined in the lab notebook regarding the IBM stock. Pls. keep the train vs. test dataset fraction fixed to 0.9, all other parameters of the data preparation and model training can be changed.\n",
    "\n",
    "> Analyse the performance of the learned models in terms of their prediction accuracy as well as their out-of-sample backtest performance (e. g. the total return and equity progression). What architectures and corresponding training parameters result in the best performing models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Training and Evaluation of Models Learned from Augmented Data.**\n",
    "\n",
    "> In the prior exercises, we used the historical daily adjusted returns of a single stock to learn a model that can predict the stocks’ future adjusted closing price (log- return) movement. However, one of the advantages of NN’s lies in their capability to learn a model from multiple sources of input data.\n",
    "For each of the two stocks (’target stocks’) that you selected in exercise 2. learn an ’optimal’ RNN model using the daily returns as a target label. However, before training your models augment the training data of each stock by the return sequences of at least three additional stocks. The additional stocks, used for data augmentation, should exhibit a high correlation to the historical adjusted closing prices of the target stock price movement you aim to model.\n",
    "\n",
    "> Analyse the performance of the learned models in terms of their prediction accuracy as well as their out-of-sample backtest performance (e. g. the total return and equity progression). Do you observe an improvement of the trained model in terms of out-of-sample backtest performance comparison to exercise 1.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this seventh lab, a step by step introduction into **design, implementation, training and evaluation** of a LSTM neural network based trading strategy is presented. \n",
    "\n",
    "The strategy trades a specific financial instrument based on its historical adjusted daily market prices. The degree of success of the implemented strategy is evaluated based in its backtest performance with particular focus on (1) the strategy's **total return** as well as (2) its **equity progression** over time. \n",
    "\n",
    "The code provided in this lab provides a blueprint for the development and testing of more complex trading strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Pls. note that to convert the notebook, you need to install Python's **nbconvert** library and its extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the nbconvert library\n",
    "!pip install nbconvert\n",
    "!pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert the Jupyter notebook into a plain Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script cfds_lab_08.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "181.27499389648438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
